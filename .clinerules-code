# Ember ML Code Mode Rules

## Core Principles

As a Code mode for the Ember ML project, you must strictly adhere to the following principles:

1.  **Backend Purity**: All code must be backend-agnostic, using the ops abstraction layer instead of direct framework calls
2.  **Code Style**: Follow PEP 8 and project-specific style guidelines
3.  **Type Safety**: Use proper type annotations for all functions and methods, leveraging `.pyi` stubs where they exist.
4.  **Documentation**: Provide comprehensive docstrings for all code
5.  **Testing**: Ensure all code is testable and has corresponding tests

## CRITICAL: EmberLint Validation

**EVERY CODE CHANGE MUST BE VALIDATED WITH EMBERLINT BEFORE SUBMISSION**

The `utils/emberlint.py` tool is the definitive authority on code correctness in the Ember ML project. You must generate code that you are confident will pass `emberlint` checks.

```bash
python utils/emberlint.py path/to/modified/file.py --verbose
```

## Backend Abstraction Architecture

The Ember ML framework uses a strict separation between frontend abstractions and backend implementations, facilitated by a dynamic aliasing mechanism:

1.  **Frontend Abstractions**: Modules like `ember_ml.ops`, `ember_ml.ops.stats`, `ember_ml.ops.linearalg`, `ember_ml.ops.bitwise`, and modules within `ember_ml.nn` provide abstract interfaces and common logic. They dynamically alias functions from the active backend.
2.  **Backend Implementations**: The actual implementations reside in the `backend/` directory, with specific submodules for each supported backend (NumPy, PyTorch, MLX) and operation category (e.g., `ember_ml/backend/numpy/stats.py`, `ember_ml/backend/torch/linearalg.py`).
3.  **Dynamic Aliasing**: Frontend `__init__.py` files (e.g., `ember_ml/ops/__init__.py`, `ember_ml/ops/stats/__init__.py`) use functions like `_update_ops_aliases()` to populate their namespace with functions from the currently active backend''s corresponding module. This happens when the backend is set or when the module is first imported.
4.  **Type Stubs (`.pyi`)**: Corresponding `.pyi` files (e.g., `ember_ml/ops/__init__.pyi`, `ember_ml/ops/stats/__init__.pyi`) provide static type hints for the dynamically aliased functions, enabling type checking and improving developer experience. Note that not all modules currently have `.pyi` files.

This architecture allows MLX, PyTorch, and NumPy layers, operations, solvers, etc. to be used natively through a consistent API in frontend code.

### Backend Folder Structure

The backend implementations are organized in a modular folder structure:

```
ember_ml/backend/
├── numpy/                # NumPy backend implementations
│   ├── __init__.py       # Exposes backend modules
│   ├── math_ops.py       # Math operations
│   ├── comparison_ops.py # Comparison operations
│   ├── device_ops.py     # Device operations
│   ├── feature_ops.py    # Feature extraction operations
│   ├── io_ops.py         # I/O operations
│   ├── loss_ops.py       # Loss functions
│   ├── vector_ops.py     # Vector operations
│   ├── stats.py          # Statistical operations
│   ├── linearalg.py      # Linear algebra operations
│   ├── bitwise.py        # Bitwise operations
│   ├── tensor/           # Tensor implementations
│   │   ├── __init__.py   # Exposes tensor class and ops
│   │   ├── tensor.py     # Tensor class
│   │   ├── dtype.py      # Data type class
│   │   └── ops/          # Tensor operations implementations
│   │       ├── __init__.py # Exposes tensor ops
│   │       └── ...
│   └── ...
├── torch/                # PyTorch backend implementations
│   ├── __init__.py
│   ├── math_ops.py
│   ├── ...
│   └── tensor/
│       ├── __init__.py
│       └── ...
├── mlx/                  # MLX backend implementations
│   ├── __init__.py
│   ├── math_ops.py
│   ├── ...
│   └── tensor/
│       ├── __init__.py
│       └── ...
└── ...
```

Each backend folder contains implementation files that correspond to the operation categories defined in the frontend interfaces. The `tensor` subfolder contains the backend-specific tensor and data type implementations. This modular structure makes it easier to maintain and extend the backend implementations.

### Interface Definition and Implementation

For each operation category, there is:

1.  **Interface Definition**: (Less common now, but historically in `ops/interfaces/`) Abstract classes defining the API.
2.  **Frontend Exposure**: Functions in frontend `__init__.py` files (e.g., `ember_ml/ops/__init__.py`, `ember_ml/ops/stats/__init__.py`) that dynamically alias backend functions.
3.  **Backend Implementation**: Concrete implementations in each backend submodule (e.g., `ember_ml/backend/numpy/stats.py`).

## Code Structure and Organization

### Module Organization

Follow the established module structure:

```
ember_ml/
├── backend/       # Backend abstraction system implementations
├── ops/           # Operations (math, comparison, device, IO, loss, vector, stats, linearalg, bitwise)
├── nn/            # Neural network components
│   ├── tensor/    # Backend-agnostic tensor operations and abstractions
│   ├── activations/ # Activation functions (Modules and functional aliases)
│   ├── container/ # Container modules (Sequential, Linear, BatchNorm, Dropout)
│   ├── modules/   # Core neural network modules (BaseModule, Module, Parameter, NCP, AutoNCP)
│   ├── wirings/   # Neuron Maps (connectivity patterns: NeuronMap, NCPMap, FullyConnectedMap, RandomMap, EnhancedNeuronMap, EnhancedNCPMap)
│   └── ...
├── features/      # Feature extraction
├── models/        # Model implementations (RBM, Liquid, Attention)
├── attention/     # Attention mechanisms (might be integrated into nn.modules or models)
├── core/          # Core functionality (e.g., backend selection logic)
├── utils/         # Utility functions (backend_utils, fraction, performance, visualization)
└── ...            # Other modules
```

### File Organization

Each file should be organized as follows:

1.  Module docstring
2.  Imports (grouped by standard library, third-party, and internal)
3.  Constants and global variables
4.  Classes
5.  Functions
6.  Main execution block (if applicable)

## Documentation Standards

### Docstrings

Use Google-style docstrings for all functions, classes, and modules:

```python
def function_name(param1: Type1, param2: Type2) -> ReturnType:
    """Short description of the function.

    Longer description explaining the function's purpose and behavior.

    Args:
        param1: Description of param1
        param2: Description of param2

    Returns:
        Description of return value

    Raises:
        ExceptionType: Description of when this exception is raised
    """
```

### Type Annotations

Use type annotations for all function parameters and return values, leveraging `.pyi` files for dynamically aliased functions:

```python
def process_data(data: Union[List[float], tensor.EmberTensor]) -> tensor.EmberTensor:
    """Process the input data."""
    # Implementation
```

## Testing Requirements

### Unit Tests

Every component must have corresponding unit tests that:

1.  Test the component in isolation
2.  Cover all code paths and edge cases
3.  Test with different backends (NumPy, PyTorch, MLX)

### Integration Tests

Components that interact with other parts of the system must have integration tests that:

1.  Test the component in the context of the larger system
2.  Verify correct behavior across component boundaries

## Common Pitfalls to Avoid

### 1. Backend Leakage

❌ **FORBIDDEN**:
- Importing NumPy directly (`import numpy` or `from numpy import ...`) in frontend code.
- Using NumPy functions or methods (`np.array()`, `np.sin()`, etc.) in frontend code.
- Converting tensors to NumPy arrays (`.numpy()`, `np.array(tensor)`) in frontend code, except where explicitly allowed (e.g., for visualization libraries *after* thorough testing confirms necessity).

✅ **REQUIRED**:
- Import ops from ember_ml (`from ember_ml import ops`)
- Import tensor from ember_ml.nn (`from ember_ml.nn import tensor`)
- Use ops functions for all mathematical, comparison, device, I/O, loss, and vector operations (`ops.sin()`, `ops.matmul()`, `ops.equal()`, `ops.to_device()`, etc.) in frontend code.
- Use tensor functions for tensor creation, manipulation, and random operations (`tensor.convert_to_tensor()`, `tensor.reshape()`, `tensor.random_normal()`, etc.) in frontend code.

**EXCEPTION**: Limited NumPy usage is permitted within backend implementations (`ember_ml/backend/numpy/`) and potentially for visualization/plotting libraries that specifically require it, and ONLY after thorough testing to confirm that it is required. Even in these cases, the code should be isolated and clearly documented.

### 2. NO PRECISION-REDUCING CASTS

❌ **FORBIDDEN**:
- Using `float()` or `int()` casts that may reduce precision in frontend code.
- Hardcoding data types that may not be compatible with all backends in frontend code.

✅ **REQUIRED**:
- Use `nn.tensor.cast()` with appropriate dtype constants from `ember_ml.nn.tensor` (e.g., `tensor.int32`, `tensor.float32`).

### 3. NO DIRECT PYTHON OPERATORS ON TENSORS

❌ **FORBIDDEN**:
- Using Python operators directly on tensors (`+`, `-`, `*`, `/`, etc.) in frontend code.
- Using Python comparison operators on tensors (`<`, `>`, `==`, etc.) in frontend code.

✅ **REQUIRED**:
- Use ops functions for all operations on tensors (`ops.add()`, `ops.subtract()`, `ops.equal()`, `ops.greater()`, etc.) in frontend code.

### 4. NO DIRECT BACKEND ACCESS IN FRONTEND CODE

❌ **FORBIDDEN**:
- Bypassing frontend abstractions (`ops`, `nn`) to use backends directly in frontend code.
- Implementing backend-specific code outside the `backend/` directory.
- Using backend-specific features (e.g., PyTorch `cuda()` method, MLX `to()` function) in frontend code.

✅ **REQUIRED**:
- Always use the `ops` and `nn` abstraction layers for all interactions with tensors and neural network components in frontend code.
- Keep all backend implementations strictly within the `backend/` directory.
- Use backend-agnostic code in all frontend components.

## Strong Typing and Tensor Creation

-   **Frontend Tensor Creation**: The primary way to create tensors on the frontend is by using `from ember_ml.nn import tensor` and then calling `tensor.convert_to_tensor(data)`, where `data` can be a Python list, NumPy array, etc.
-   **Backend Tensor Creation**: Within backend implementations, get the backend-specific tensor operations object (e.g., `MLXTensor()`) and use its `convert_to_tensor(data)` method to create the native backend tensor.
-   **Type Aliases**: Type aliases like `TensorLike` are used in both frontend and backend type hints to indicate that a function can accept a variety of input types that can be converted to a tensor.
-   **Type Stubs (`.pyi`)**: `.pyi` files provide static type hints for dynamically aliased functions and other components, aiding static analysis tools. Note that not all modules currently have `.pyi` files.
-   **`TYPE_CHECKING`**: Conditional imports within `TYPE_CHECKING` blocks are used to allow type hints to reference types from other layers (frontend referencing backend types, backend referencing frontend types) without creating runtime import cycles.

## Implementation Guidelines

### I/O Operations

When implementing I/O operations:

1.  **Use the Abstraction Layer**: Always use the `ops.save()` and `ops.load()` functions from the ops abstraction layer
2.  **Never Import Backend Directly**: Never use `from ember_ml import backend as K` or similar direct backend imports in frontend code.
3.  **Implement in Backend Folders**: Implement I/O operations in the appropriate backend folder (e.g., `backend/numpy/io_ops.py`)
4.  **Define Interfaces**: (Less common now, but historically in `ops/interfaces/`) Define interfaces in `ops/interfaces/io_ops.py` before implementing in backends.
5.  **Expose in ops/__init__.py**: Expose I/O functions in `ops/__init__.py` using the dynamic aliasing mechanism.

### Feature Extraction

When implementing feature extraction components:

1.  Use the `ColumnFeatureExtractor` pattern for tabular data.
2.  Use the `TemporalStrideProcessor` for time series data.
3.  Ensure all components are backend-agnostic.

### Tensor Operations

When working with tensor operations:

1.  **Use nn.tensor Module**: For tensor operations, use the `nn.tensor` module which provides backend-agnostic tensor operations.
2.  **Common Operations**: Use functions like `random_bernoulli`, `random_uniform`, `transpose`, `shape`, etc. from the `nn.tensor` module.
3.  **Avoid Direct Backend Access**: Never directly access backend-specific tensor implementations in frontend code.
4.  **Handle Device Placement**: Use the device parameter in tensor creation functions to ensure proper device placement.

### Neural Networks

When implementing neural network components:

1.  Use the `nn` module for layers and cells.
2.  Use the `wirings` module (Neuron Maps) for connectivity patterns.
3.  Ensure all components work with the backend abstraction system.
4.  Use activation functions from `nn.activations` module.
5.  Use container modules from `nn.container` module.

### RBM and Liquid Networks

When working with RBMs and liquid networks:

1.  Use the established patterns in `models/rbm/rbm.py` and `models/liquid/` directory.
2.  Ensure compatibility with the feature extraction pipeline.
3.  Follow the CfC architecture for temporal processing where applicable.
4.  Use tensor operations from the nn.tensor module for tensor manipulations.
5.  Use ops functions for mathematical operations to maintain backend agnosticism.

## Development Process Guidelines

### Task Management

1.  **Use Checklists**: Always create and use checklists of tasks or files that need to be modified.
    - Create a checklist at the beginning of each task.
    - Check off items as they are completed.
    - Review the checklist before marking the task as complete.

2.  **Completion Verification**: Never mark anything as done unless its truly complete.
    - All requirements must be fully implemented.
    - All tests must pass.
    - All documentation must be updated.
    - If you are unsure about completion status, switch to Ask mode for clarification.

3.  **Code Inspection**: Always verify implementation details when in doubt.
    - Check backend and other implementation signatures.
    - Inspect the actual code if you have the slightest doubt about behavior.
    - Verify that your implementation matches the expected interface and behavior.

### Example Checklist Format

```
## Task: Implement new feature X

- [ ] Review existing code and documentation
- [ ] Create implementation plan
- [ ] Implement core functionality
- [ ] Add type annotations and update .pyi stubs if necessary - don''t create new .pyi files
- [ ] Implement backend-specific functionality in backend/numpy, backend/torch, and backend/mlx
- [ ] Implement frontend abstraction in ember_ml/ops, ember_ml/ops/stats, ember_ml/ops/linearalg, ember_ml/ops/bitwise
- [ ] Implement tensor operations in ember_ml/nn/tensor and backend/numpy/tensor/ops, backend/torch/tensor/ops, backend/mlx/tensor/ops
- [ ] Write comprehensive docstrings
- [ ] Create unit tests using pytest in tests/<backend>/
- [ ] Test with all backends (NumPy, PyTorch, MLX)
- [ ] Create integration tests
- [ ] Run emberlint on all modified files
- [ ] Verify backend compatibility
- [ ] Update documentation
```

## Final Checklist

Before submitting any code, verify that:

1.  ✅ EmberLint passes with no errors - run `python utils/emberlint.py path/to/file.py --verbose`
2.  ✅ All functions have proper type annotations and corresponding `.pyi` stubs are accurate where `.pyi` files exist.
3.  ✅ All functions have comprehensive docstrings - follow the Google-style format.
4.  ✅ No direct NumPy usage or other backend-specific code in frontend - use the ops abstraction layer.
5.  ✅ No precision-reducing casts - never use `float()` or `int()` in frontend code.
6.  ✅ No direct Python operators on tensors in frontend code - use ops functions for all operations.
7.  ✅ Code follows the established module and file organization - frontend abstractions, backend implementations, etc.
8.  ✅ No backend implementations outside the backend directory - all backend code should be in backend/.
9.  ✅ All tensor operations go through the nn.tensor abstraction layer in frontend code - no direct backend access.
10. ✅ Unit tests cover all code paths - test with different backends.
11. ✅ Integration tests verify component interactions - test across component boundaries.
12. ✅ All tasks in your checklist are completed and verified.

**REMEMBER: ALWAYS RUN EMBERLINT ON EVERY FILE YOU MODIFY**

The quality and consistency of the Ember ML codebase depends on strict adherence to these rules. Backend purity is especially critical to ensure the framework works consistently across different computational backends.

## Step-by-Step Guide for Adding a New Function

When adding a new function to the Ember ML library, follow these steps precisely to ensure proper implementation across the frontend abstraction and backend implementations:

### 1. Define the Frontend Interface and Type Stub

1.  Identify the appropriate frontend module (e.g., `ember_ml/ops/math.py` or a new submodule).
2.  Add the function definition to the `.py` file, using type hints and a docstring.
3.  If a `.pyi` file exists for the module, add the function signature to the corresponding `.pyi` type stub file (e.g., `ember_ml/ops/math.pyi`), including type hints and a basic docstring.

Example (`ops/math.py`):
```python
def new_function(x: TensorLike, y: TensorLike) -> Tensor:
    """Short description."""
    # Implementation will call backend
    pass
```
Example (`ops/math.pyi` if it exists):
```python
def new_function(x: TensorLike, y: TensorLike) -> Tensor: ...
```

### 2. Expose the Function in the Frontend `__init__.py`

1.  Open the `__init__.py` file for the frontend module (e.g., `ember_ml/ops/__init__.py` or `ember_ml/ops/math/__init__.py`).
2.  Add the function name to the `_MASTER_OPS_LIST` or the relevant submodules list.
3.  Ensure the dynamic aliasing mechanism in that `__init__.py` will pick up and expose the new function from the backend.

Example (`ops/__init__.py`):
```python
_MASTER_OPS_LIST = [
    # ... other ops ...
    'new_function',
]
# ... rest of aliasing logic ...
```

### 3. Implement the Function in Each Backend

For each backend (NumPy, PyTorch, MLX), implement the function in the appropriate backend submodule file (e.g., `ember_ml/backend/numpy/math_ops.py`, `ember_ml/backend/torch/math_ops.py`, `ember_ml/backend/mlx/math_ops.py`):

1.  Add the function with proper type annotations and docstring.
2.  Implement the function using the backends native operations.
3.  Ensure the function is exposed in the backend submodules `__init__.py` and the main backend `__init__.py`.

Example (`backend/numpy/math_ops.py`):
```python
def new_function(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    """NumPy implementation of new_function."""
    # Use NumPy operations
    return np.add(x, y) # Example implementation
```

### 4. Validate with EmberLint

1.  Run EmberLint on all modified files:
    ```bash
    python utils/emberlint.py path/to/frontend/file.py --verbose
    python utils/emberlint.py path/to/frontend/__init__.py --verbose
    # Only run if .pyi file exists
    # python utils/emberlint.py path/to/frontend/__init__.pyi --verbose
    python utils/emberlint.py path/to/backend/numpy/file.py --verbose
    python utils/emberlint.py path/to/backend/torch/file.py --verbose
    python utils/emberlint.py path/to/backend/mlx/file.py --verbose
    ```
2.  Fix any issues reported by EmberLint.

### 5. Test the Function

1.  Create a simple test that uses the ops abstraction layer (NOT direct backend calls):
    ```python
    from ember_ml.nn import tensor
    from ember_ml import ops
    result = ops.new_function(tensor.convert_to_tensor([1, 2, 3]), tensor.convert_to_tensor([4, 5, 6]))
    print(result)
    ```
2.  Verify the function works as expected with different backends.

### 6. Add Unit Tests

1.  Add unit tests in the appropriate test file.
2.  Test with all backends.
3.  Test edge cases and error conditions.

### 7. Update Documentation

1.  Add the function to the API documentation markdown files.
2.  Provide examples of usage.

By following these steps consistently, you ensure that new functions maintain the backend purity and architectural integrity of the Ember ML framework.