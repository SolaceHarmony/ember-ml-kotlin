{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Data Preparation and Feature Extraction for Liquid Neural Networks\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Extract and prepare data from BigQuery tables using BigFrames\n",
    "2. Process features through Restricted Boltzmann Machines (RBMs)\n",
    "3. Feed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\n",
    "4. Implement a motor neuron that outputs a value to trigger deeper exploration\n",
    "\n",
    "The pipeline is designed to handle terabyte-sized tables efficiently through chunked processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mlx backend\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline_demo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01memberharmony\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstride_aware_cfc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     create_liquid_network_with_motor_neuron,\n\u001b[1;32m     44\u001b[0m     create_lstm_gated_liquid_network,\n\u001b[1;32m     45\u001b[0m     create_multi_stride_liquid_network\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Import the integrated pipeline\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpipeline_demo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntegratedPipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pipeline_demo'"
     ]
    }
   ],
   "source": [
    "# BigQuery Data Preparation and Feature Extraction for Liquid Neural Networks\n",
    "\n",
    "# This notebook demonstrates how to:\n",
    "# 1. Extract and prepare data from BigQuery tables using BigFrames\n",
    "# 2. Process features through Restricted Boltzmann Machines (RBMs)\n",
    "# 3. Feed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\n",
    "# 4. Implement a motor neuron that outputs a value to trigger deeper exploration\n",
    "\n",
    "# The pipeline is designed to handle terabyte-sized tables efficiently through chunked processing.\n",
    "\n",
    "# Install required packages if needed\n",
    "# !pip install google-cloud-bigquery bigframes\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import bigframes.pandas as bf\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any, Generator\n",
    "\n",
    "# Import ember_ml instead of NumPy and TensorFlow\n",
    "import ember_ml as eh\n",
    "from ember_ml import ops\n",
    "from ember_ml import nn\n",
    "from ember_ml.backend import get_backend\n",
    "\n",
    "# Print the current backend\n",
    "current_backend = get_backend()\n",
    "print(f\"Using {current_backend} backend\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('bigquery_pipeline')\n",
    "\n",
    "# Import our components\n",
    "from ember_ml.nn.features.terabyte_feature_extractor_bigframes import TerabyteFeatureExtractor, TerabyteTemporalStrideProcessor\n",
    "from ember_ml.models.optimized_rbm import OptimizedRBM\n",
    "from ember_ml.models.stride_aware_cfc import (\n",
    "    create_liquid_network_with_motor_neuron,\n",
    "    create_lstm_gated_liquid_network,\n",
    "    create_multi_stride_liquid_network\n",
    ")\n",
    "\n",
    "# Import the integrated pipeline\n",
    "from examples.notebooks.bigquery.pipeline_demo import IntegratedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Connection Setup\n",
    "\n",
    "Set up the connection to BigQuery. You can use service account credentials or application default credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:24:49,793 - terabyte_feature_extractor - INFO - Initialized TerabyteFeatureExtractor with chunk_size=100000, max_memory_gb=16.0\n",
      "2025-03-05 23:24:49,883 - terabyte_feature_extractor - INFO - BigQuery connection set up successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to BigQuery project: massmkt-poc\n",
      "Using location: US\n",
      "Feature extractor initialized with BigFrames support\n"
     ]
    }
   ],
   "source": [
    "# BigQuery Connection Setup\n",
    "#\n",
    "# Set up the connection to BigQuery. You can use service account credentials or application default credentials.\n",
    "\n",
    "# Set your GCP project ID\n",
    "PROJECT_ID = \"massmkt-poc\"  # Replace with your project ID\n",
    "\n",
    "# Path to service account credentials (optional)\n",
    "CREDENTIALS_PATH = '/Users/sydneybach/sydney-bach.json'  # Replace with path to credentials.json if needed\n",
    "\n",
    "# BigQuery location\n",
    "LOCATION = \"US\"\n",
    "\n",
    "# Import BigFrames\n",
    "import bigframes.pandas as bf\n",
    "\n",
    "# Set BigFrames options\n",
    "bf.options.bigquery.project = PROJECT_ID\n",
    "bf.options.bigquery.location = LOCATION\n",
    "\n",
    "# Initialize the feature extractor\n",
    "from ember_ml.nn.features.terabyte_feature_extractor_bigframes import TerabyteFeatureExtractor\n",
    "\n",
    "feature_extractor = TerabyteFeatureExtractor(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    chunk_size=100000,\n",
    "    max_memory_gb=16.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Set up BigQuery connection\n",
    "feature_extractor.setup_bigquery_connection(CREDENTIALS_PATH)\n",
    "\n",
    "print(f\"Connected to BigQuery project: {PROJECT_ID}\")\n",
    "print(f\"Using location: {LOCATION}\")\n",
    "print(f\"Feature extractor initialized with BigFrames support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Available Tables\n",
    "\n",
    "Let's explore the available tables in your BigQuery project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project massmkt-poc:\n",
      "- 180601513\n",
      "- 239703222\n",
      "- 242584221\n",
      "- 267229650\n",
      "- BQML_Datasets\n",
      "- BigQuery_Google_Ads\n",
      "- Event_Data_Dictionary\n",
      "- FieldOps_Reporting_Dataset\n",
      "- MM_LP_Data\n",
      "- MM_LP_POC\n",
      "- Marketing_Cloud\n",
      "- Monitored_Zipcodes_Expanded\n",
      "- Partners\n",
      "- QF_AIOPS\n",
      "- QF_Activation_POC\n",
      "- QF_BUY_FLOW_TRANSACTIONS\n",
      "- QF_Shell_Account_CleanUp_Data\n",
      "- Service_Appointment_History\n",
      "- TEST1\n",
      "- abandoned_jobs\n",
      "- analytics_251783832\n",
      "- analytics_379694883\n",
      "- analytics_405473592\n",
      "- analytics_424581992\n",
      "- analytics_435146347\n",
      "- analytics_451204749\n",
      "- confluent_sink\n",
      "- connected_communities_dev\n",
      "- connected_communities_prod\n",
      "- contact_engine\n",
      "- datafirst_prod\n",
      "- datalake_ingestion_sandbox\n",
      "- design_repair\n",
      "- dev_sandbox\n",
      "- dispatch_events\n",
      "- dispatch_events_test\n",
      "- ds_mmdldev_bluemarble\n",
      "- ds_mmdldev_bluemarble_raw\n",
      "- ds_mmdldev_bluemarble_stg\n",
      "- ds_mmdldev_kafka_master\n",
      "- ds_mmdldev_nokiahal_raw\n",
      "- ds_mmdldev_pc360_raw\n",
      "- ds_mmpoc_dev_consmobile_silver\n",
      "- ds_mmpoc_master\n",
      "- ds_mmpoc_master_uscentral\n",
      "- dwh_staging\n",
      "- dx_ds\n",
      "- dx_ds_central\n",
      "- field_mission\n",
      "- firebase_crashlytics\n",
      "- gcp_cost_monitoring\n",
      "- gen_ai_auto\n",
      "- gen_ai_automation\n",
      "- goals_cart\n",
      "- google_adgroups\n",
      "- google_tag_manager_monitor\n",
      "- hierarchy\n",
      "- ibm_watson_discovery\n",
      "- jira_data\n",
      "- key_tables\n",
      "- key_tables_central\n",
      "- looker_scratch\n",
      "- looker_scratch_ga4\n",
      "- looker_scratch_prod\n",
      "- mdu_ros_data\n",
      "- mmdl_blueMarble\n",
      "- mmdl_pc360\n",
      "- mmdldev_gwynngroup_raw\n",
      "- mmdldev_gwynngroup_stg\n",
      "- mnet_test\n",
      "- nlp\n",
      "- nps\n",
      "- nps_survey_health\n",
      "- performics_historical\n",
      "- polygons\n",
      "- prodapt_dev\n",
      "- prodapt_prod\n",
      "- qf_activations_poc\n",
      "- qf_business_transactions\n",
      "- qf_events\n",
      "- qf_events_of_value\n",
      "- raw_comtech\n",
      "- raw_consmobile\n",
      "- raw_creditcheck\n",
      "- raw_gwynngroup\n",
      "- raw_ibmchat\n",
      "- raw_intrado\n",
      "- raw_modems\n",
      "- raw_salesforce_ctl_test\n",
      "- rto\n",
      "- s3_html_email_import\n",
      "- sagar_test\n",
      "- salesforce_qf\n",
      "- salesforce_raw\n",
      "- sampletest\n",
      "- scale_serp\n",
      "- sfdc_gcp\n",
      "- splunk_dev\n",
      "- sqe\n",
      "- srv_assia_cloudcheck\n",
      "- srv_comtech\n",
      "- srv_consmobile\n",
      "- srv_creditcheck\n",
      "- srv_gwynngroup\n",
      "- srv_ibmchat\n",
      "- srv_intrado\n",
      "- srv_modems\n",
      "- srv_motive\n",
      "- stage_dispatch_events_to_dataprod\n",
      "- stg_intrado\n",
      "- temp\n",
      "- test_datasets\n",
      "- test_sandbox\n",
      "- testingsample\n",
      "\n",
      "Tables in dataset 180601513:\n",
      "- ga_sessions_20190823\n",
      "- ga_sessions_20190824\n",
      "- ga_sessions_20190825\n",
      "- ga_sessions_20190826\n",
      "- ga_sessions_20190827\n",
      "- ga_sessions_20190828\n",
      "- ga_sessions_20190829\n",
      "- ga_sessions_20190830\n",
      "- ga_sessions_20190831\n",
      "- ga_sessions_20190901\n",
      "- ga_sessions_20190902\n",
      "- ga_sessions_20190903\n",
      "- ga_sessions_20190904\n",
      "- ga_sessions_20190905\n",
      "- ga_sessions_20190906\n",
      "- ga_sessions_20190907\n",
      "- ga_sessions_20190908\n",
      "- ga_sessions_20190909\n",
      "- ga_sessions_20190910\n",
      "- ga_sessions_20190911\n",
      "- ga_sessions_20190912\n",
      "- ga_sessions_20190913\n",
      "- ga_sessions_20190914\n",
      "- ga_sessions_20190915\n",
      "- ga_sessions_20190916\n",
      "- ga_sessions_20190917\n",
      "- ga_sessions_20190918\n",
      "- ga_sessions_20190919\n",
      "- ga_sessions_20190920\n",
      "- ga_sessions_20190921\n",
      "- ga_sessions_20190922\n",
      "- ga_sessions_20190923\n",
      "- ga_sessions_20190924\n",
      "- ga_sessions_20190925\n",
      "- ga_sessions_20190926\n",
      "- ga_sessions_20190927\n",
      "- ga_sessions_20190928\n",
      "- ga_sessions_20190929\n",
      "- ga_sessions_20190930\n",
      "- ga_sessions_20191001\n",
      "- ga_sessions_20191002\n",
      "- ga_sessions_20191003\n",
      "- ga_sessions_20191004\n",
      "- ga_sessions_20191005\n",
      "- ga_sessions_20191006\n",
      "- ga_sessions_20191007\n",
      "- ga_sessions_20191008\n",
      "- ga_sessions_20191009\n",
      "- ga_sessions_20191010\n",
      "- ga_sessions_20191011\n",
      "- ga_sessions_20191012\n",
      "- ga_sessions_20191013\n",
      "- ga_sessions_20191014\n",
      "- ga_sessions_20191015\n",
      "- ga_sessions_20191016\n",
      "- ga_sessions_20191017\n",
      "- ga_sessions_20191018\n",
      "- ga_sessions_20191019\n",
      "- ga_sessions_20191020\n",
      "- ga_sessions_20191021\n",
      "- ga_sessions_20191022\n",
      "- ga_sessions_20191023\n",
      "- ga_sessions_20191024\n",
      "- ga_sessions_20191025\n",
      "- ga_sessions_20191026\n",
      "- ga_sessions_20191027\n",
      "- ga_sessions_20191028\n",
      "- ga_sessions_20191029\n",
      "- ga_sessions_20191030\n",
      "- ga_sessions_20191031\n",
      "- ga_sessions_20191101\n",
      "- ga_sessions_20191102\n",
      "- ga_sessions_20191103\n",
      "- ga_sessions_20191104\n",
      "- ga_sessions_20191105\n",
      "- ga_sessions_20191106\n",
      "- ga_sessions_20191107\n",
      "- ga_sessions_20191108\n",
      "- ga_sessions_20191109\n",
      "- ga_sessions_20191110\n",
      "- ga_sessions_20191111\n",
      "- ga_sessions_20191112\n",
      "- ga_sessions_20191113\n",
      "- ga_sessions_20191114\n",
      "- ga_sessions_20191115\n",
      "- ga_sessions_20191116\n",
      "- ga_sessions_20191117\n",
      "- ga_sessions_20191118\n",
      "- ga_sessions_20191119\n",
      "- ga_sessions_20191120\n",
      "- ga_sessions_20191121\n",
      "- ga_sessions_20191122\n",
      "- ga_sessions_20191123\n",
      "- ga_sessions_20191124\n",
      "- ga_sessions_20191125\n",
      "- ga_sessions_20191126\n",
      "- ga_sessions_20191127\n",
      "- ga_sessions_20191128\n",
      "- ga_sessions_20191129\n",
      "- ga_sessions_20191130\n",
      "- ga_sessions_20191201\n",
      "- ga_sessions_20191202\n",
      "- ga_sessions_20191203\n",
      "- ga_sessions_20191204\n",
      "- ga_sessions_20191205\n",
      "- ga_sessions_20191206\n",
      "- ga_sessions_20191207\n",
      "- ga_sessions_20191208\n",
      "- ga_sessions_20191209\n",
      "- ga_sessions_20191210\n",
      "- ga_sessions_20191211\n",
      "- ga_sessions_20191212\n",
      "- ga_sessions_20191213\n",
      "- ga_sessions_20191214\n",
      "- ga_sessions_20191215\n",
      "- ga_sessions_20191216\n",
      "- ga_sessions_20191217\n",
      "- ga_sessions_20191218\n",
      "- ga_sessions_20191219\n",
      "- ga_sessions_20191220\n",
      "- ga_sessions_20191221\n",
      "- ga_sessions_20191222\n",
      "- ga_sessions_20191223\n",
      "- ga_sessions_20191224\n",
      "- ga_sessions_20191225\n",
      "- ga_sessions_20191226\n",
      "- ga_sessions_20191227\n",
      "- ga_sessions_20191228\n",
      "- ga_sessions_20191229\n",
      "- ga_sessions_20191230\n",
      "- ga_sessions_20191231\n",
      "- ga_sessions_20200101\n",
      "- ga_sessions_20200102\n",
      "- ga_sessions_20200103\n",
      "- ga_sessions_20200104\n",
      "- ga_sessions_20200105\n",
      "- ga_sessions_20200106\n",
      "- ga_sessions_20200107\n",
      "- ga_sessions_20200108\n",
      "- ga_sessions_20200109\n",
      "- ga_sessions_20200110\n",
      "- ga_sessions_20200111\n",
      "- ga_sessions_20200112\n",
      "- ga_sessions_20200113\n",
      "- ga_sessions_20200114\n",
      "- ga_sessions_20200115\n",
      "- ga_sessions_20200116\n",
      "- ga_sessions_20200117\n",
      "- ga_sessions_20200118\n",
      "- ga_sessions_20200119\n",
      "- ga_sessions_20200120\n",
      "- ga_sessions_20200121\n",
      "- ga_sessions_20200122\n",
      "- ga_sessions_20200123\n",
      "- ga_sessions_20200124\n",
      "- ga_sessions_20200125\n",
      "- ga_sessions_20200126\n",
      "- ga_sessions_20200127\n",
      "- ga_sessions_20200128\n",
      "- ga_sessions_20200129\n",
      "- ga_sessions_20200130\n",
      "- ga_sessions_20200131\n",
      "- ga_sessions_20200201\n",
      "- ga_sessions_20200202\n",
      "- ga_sessions_20200203\n",
      "- ga_sessions_20200204\n",
      "- ga_sessions_20200205\n",
      "- ga_sessions_20200206\n",
      "- ga_sessions_20200207\n",
      "- ga_sessions_20200208\n",
      "- ga_sessions_20200209\n",
      "- ga_sessions_20200210\n",
      "- ga_sessions_20200211\n",
      "- ga_sessions_20200212\n",
      "- ga_sessions_20200213\n",
      "- ga_sessions_20200214\n",
      "- ga_sessions_20200215\n",
      "- ga_sessions_20200216\n",
      "- ga_sessions_20200217\n",
      "- ga_sessions_20200218\n",
      "- ga_sessions_20200219\n",
      "- ga_sessions_20200220\n",
      "- ga_sessions_20200221\n",
      "- ga_sessions_20200222\n",
      "- ga_sessions_20200223\n",
      "- ga_sessions_20200224\n",
      "- ga_sessions_20200225\n",
      "- ga_sessions_20200226\n",
      "- ga_sessions_20200227\n",
      "- ga_sessions_20200228\n",
      "- ga_sessions_20200229\n",
      "- ga_sessions_20200301\n",
      "- ga_sessions_20200302\n",
      "- ga_sessions_20200303\n",
      "- ga_sessions_20200304\n",
      "- ga_sessions_20200305\n",
      "- ga_sessions_20200306\n",
      "- ga_sessions_20200307\n",
      "- ga_sessions_20200308\n",
      "- ga_sessions_20200309\n",
      "- ga_sessions_20200310\n",
      "- ga_sessions_20200311\n",
      "- ga_sessions_20200312\n",
      "- ga_sessions_20200313\n",
      "- ga_sessions_20200314\n",
      "- ga_sessions_20200315\n",
      "- ga_sessions_20200316\n",
      "- ga_sessions_20200317\n",
      "- ga_sessions_20200318\n",
      "- ga_sessions_20200319\n",
      "- ga_sessions_20200320\n",
      "- ga_sessions_20200321\n",
      "- ga_sessions_20200322\n",
      "- ga_sessions_20200323\n",
      "- ga_sessions_20200324\n",
      "- ga_sessions_20200325\n",
      "- ga_sessions_20200326\n",
      "- ga_sessions_20200327\n",
      "- ga_sessions_20200328\n",
      "- ga_sessions_20200329\n",
      "- ga_sessions_20200330\n",
      "- ga_sessions_20200331\n",
      "- ga_sessions_20200401\n",
      "- ga_sessions_20200402\n",
      "- ga_sessions_20200403\n",
      "- ga_sessions_20200404\n",
      "- ga_sessions_20200405\n",
      "- ga_sessions_20200406\n",
      "- ga_sessions_20200407\n",
      "- ga_sessions_20200408\n",
      "- ga_sessions_20200409\n",
      "- ga_sessions_20200410\n",
      "- ga_sessions_20200411\n",
      "- ga_sessions_20200412\n",
      "- ga_sessions_20200413\n",
      "- ga_sessions_20200414\n",
      "- ga_sessions_20200415\n",
      "- ga_sessions_20200416\n",
      "- ga_sessions_20200417\n",
      "- ga_sessions_20200418\n",
      "- ga_sessions_20200419\n",
      "- ga_sessions_20200420\n",
      "- ga_sessions_20200421\n",
      "- ga_sessions_20200422\n",
      "- ga_sessions_20200423\n",
      "- ga_sessions_20200424\n",
      "- ga_sessions_20200425\n",
      "- ga_sessions_20200426\n",
      "- ga_sessions_20200427\n",
      "- ga_sessions_20200428\n",
      "- ga_sessions_20200429\n",
      "- ga_sessions_20200430\n",
      "- ga_sessions_20200501\n",
      "- ga_sessions_20200502\n",
      "- ga_sessions_20200503\n",
      "- ga_sessions_20200504\n",
      "- ga_sessions_20200505\n",
      "- ga_sessions_20200506\n",
      "- ga_sessions_20200507\n",
      "- ga_sessions_20200508\n",
      "- ga_sessions_20200509\n",
      "- ga_sessions_20200510\n",
      "- ga_sessions_20200511\n",
      "- ga_sessions_20200512\n",
      "- ga_sessions_20200513\n",
      "- ga_sessions_20200514\n",
      "- ga_sessions_20200515\n",
      "- ga_sessions_20200516\n",
      "- ga_sessions_20200517\n",
      "- ga_sessions_20200518\n",
      "- ga_sessions_20200519\n",
      "- ga_sessions_20200520\n",
      "- ga_sessions_20200521\n",
      "- ga_sessions_20200522\n",
      "- ga_sessions_20200523\n",
      "- ga_sessions_20200524\n",
      "- ga_sessions_20200525\n",
      "- ga_sessions_20200526\n",
      "- ga_sessions_20200527\n",
      "- ga_sessions_20200528\n",
      "- ga_sessions_20200529\n",
      "- ga_sessions_20200530\n",
      "- ga_sessions_20200531\n",
      "- ga_sessions_20200601\n",
      "- ga_sessions_20200602\n",
      "- ga_sessions_20200603\n",
      "- ga_sessions_20200604\n",
      "- ga_sessions_20200605\n",
      "- ga_sessions_20200606\n",
      "- ga_sessions_20200607\n",
      "- ga_sessions_20200608\n",
      "- ga_sessions_20200609\n",
      "- ga_sessions_20200610\n",
      "- ga_sessions_20200611\n",
      "- ga_sessions_20200612\n",
      "- ga_sessions_20200613\n",
      "- ga_sessions_20200614\n",
      "- ga_sessions_20200615\n",
      "- ga_sessions_20200616\n",
      "- ga_sessions_20200617\n",
      "- ga_sessions_20200618\n",
      "- ga_sessions_20200619\n",
      "- ga_sessions_20200620\n",
      "- ga_sessions_20200621\n",
      "- ga_sessions_20200622\n",
      "- ga_sessions_20200623\n",
      "- ga_sessions_20200624\n",
      "- ga_sessions_20200625\n",
      "- ga_sessions_20200626\n",
      "- ga_sessions_20200627\n",
      "- ga_sessions_20200628\n",
      "- ga_sessions_20200629\n",
      "- ga_sessions_20200630\n",
      "- ga_sessions_20200701\n",
      "- ga_sessions_20200702\n",
      "- ga_sessions_20200703\n",
      "- ga_sessions_20200704\n",
      "- ga_sessions_20200705\n",
      "- ga_sessions_20200706\n",
      "- ga_sessions_20200707\n",
      "- ga_sessions_20200708\n",
      "- ga_sessions_20200709\n",
      "- ga_sessions_20200710\n",
      "- ga_sessions_20200711\n",
      "- ga_sessions_20200712\n",
      "- ga_sessions_20200713\n",
      "- ga_sessions_20200714\n",
      "- ga_sessions_20200715\n",
      "- ga_sessions_20200716\n",
      "- ga_sessions_20200717\n",
      "- ga_sessions_20200718\n",
      "- ga_sessions_20200719\n",
      "- ga_sessions_20200720\n",
      "- ga_sessions_20200721\n",
      "- ga_sessions_20200722\n",
      "- ga_sessions_20200723\n",
      "- ga_sessions_20200724\n",
      "- ga_sessions_20200725\n",
      "- ga_sessions_20200726\n",
      "- ga_sessions_20200727\n",
      "- ga_sessions_20200728\n",
      "- ga_sessions_20200729\n",
      "- ga_sessions_20200730\n",
      "- ga_sessions_20200731\n",
      "- ga_sessions_20200801\n",
      "- ga_sessions_20200802\n",
      "- ga_sessions_20200803\n",
      "- ga_sessions_20200804\n",
      "- ga_sessions_20200805\n",
      "- ga_sessions_20200806\n",
      "- ga_sessions_20200807\n",
      "- ga_sessions_20200808\n",
      "- ga_sessions_20200809\n",
      "- ga_sessions_20200810\n",
      "- ga_sessions_20200811\n",
      "- ga_sessions_20200812\n",
      "- ga_sessions_20200813\n",
      "- ga_sessions_20200814\n",
      "- ga_sessions_20200815\n",
      "- ga_sessions_20200816\n",
      "- ga_sessions_20200817\n",
      "- ga_sessions_20200818\n",
      "- ga_sessions_20200819\n",
      "- ga_sessions_20200820\n",
      "- ga_sessions_20200821\n",
      "- ga_sessions_20200822\n",
      "- ga_sessions_20200823\n",
      "- ga_sessions_20200824\n",
      "- ga_sessions_20200825\n",
      "- ga_sessions_20200826\n",
      "- ga_sessions_20200827\n",
      "- ga_sessions_20200828\n",
      "- ga_sessions_20200829\n",
      "- ga_sessions_20200830\n",
      "- ga_sessions_20200831\n",
      "- ga_sessions_20200901\n",
      "- ga_sessions_20200902\n",
      "- ga_sessions_20200903\n",
      "- ga_sessions_20200904\n",
      "- ga_sessions_20200905\n",
      "- ga_sessions_20200906\n",
      "- ga_sessions_20200907\n",
      "- ga_sessions_20200908\n",
      "- ga_sessions_20200909\n",
      "- ga_sessions_20200910\n",
      "- ga_sessions_20200911\n",
      "- ga_sessions_20200912\n",
      "- ga_sessions_20200913\n",
      "- ga_sessions_20200914\n",
      "- ga_sessions_20200915\n",
      "- ga_sessions_20200916\n",
      "- ga_sessions_20200917\n",
      "- ga_sessions_20200918\n",
      "- ga_sessions_20200919\n",
      "- ga_sessions_20200920\n",
      "- ga_sessions_20200921\n",
      "- ga_sessions_20200922\n",
      "- ga_sessions_20200923\n",
      "- ga_sessions_20200924\n",
      "- ga_sessions_20200925\n",
      "- ga_sessions_20200926\n",
      "- ga_sessions_20200927\n",
      "- ga_sessions_20200928\n",
      "- ga_sessions_20200929\n",
      "- ga_sessions_20200930\n",
      "- ga_sessions_20201001\n",
      "- ga_sessions_20201002\n",
      "- ga_sessions_20201003\n",
      "- ga_sessions_20201004\n",
      "- ga_sessions_20201005\n",
      "- ga_sessions_20201006\n",
      "- ga_sessions_20201007\n",
      "- ga_sessions_20201008\n",
      "- ga_sessions_20201009\n",
      "- ga_sessions_20201010\n",
      "- ga_sessions_20201011\n",
      "- ga_sessions_20201012\n",
      "- ga_sessions_20201013\n",
      "- ga_sessions_20201014\n",
      "- ga_sessions_20201015\n",
      "- ga_sessions_20201016\n",
      "- ga_sessions_20201017\n",
      "- ga_sessions_20201018\n",
      "- ga_sessions_20201019\n",
      "- ga_sessions_20201020\n",
      "- ga_sessions_20201021\n",
      "- ga_sessions_20201022\n",
      "- ga_sessions_20201023\n",
      "- ga_sessions_20201024\n",
      "- ga_sessions_20201025\n",
      "- ga_sessions_20201026\n",
      "- ga_sessions_20201027\n",
      "- ga_sessions_20201028\n",
      "- ga_sessions_20201029\n",
      "- ga_sessions_20201030\n",
      "- ga_sessions_20201031\n",
      "- ga_sessions_20201101\n",
      "- ga_sessions_20201102\n",
      "- ga_sessions_20201103\n",
      "- ga_sessions_20201104\n",
      "- ga_sessions_20201105\n",
      "- ga_sessions_20201106\n",
      "- ga_sessions_20201107\n",
      "- ga_sessions_20201108\n",
      "- ga_sessions_20201109\n",
      "- ga_sessions_20201110\n",
      "- ga_sessions_20201111\n",
      "- ga_sessions_20201112\n",
      "- ga_sessions_20201113\n",
      "- ga_sessions_20201114\n",
      "- ga_sessions_20201115\n",
      "- ga_sessions_20201116\n",
      "- ga_sessions_20201117\n",
      "- ga_sessions_20201118\n",
      "- ga_sessions_20201119\n",
      "- ga_sessions_20201120\n",
      "- ga_sessions_20201121\n",
      "- ga_sessions_20201122\n",
      "- ga_sessions_20201123\n",
      "- ga_sessions_20201124\n",
      "- ga_sessions_20201125\n",
      "- ga_sessions_20201126\n",
      "- ga_sessions_20201127\n",
      "- ga_sessions_20201128\n",
      "- ga_sessions_20201129\n",
      "- ga_sessions_20201130\n",
      "- ga_sessions_20201201\n",
      "- ga_sessions_20201202\n",
      "- ga_sessions_20201203\n",
      "- ga_sessions_20201204\n",
      "- ga_sessions_20201205\n",
      "- ga_sessions_20201206\n",
      "- ga_sessions_20201207\n",
      "- ga_sessions_20201208\n",
      "- ga_sessions_20201209\n",
      "- ga_sessions_20201210\n",
      "- ga_sessions_20201211\n",
      "- ga_sessions_20201212\n",
      "- ga_sessions_20201213\n",
      "- ga_sessions_20201214\n",
      "- ga_sessions_20201215\n",
      "- ga_sessions_20201216\n",
      "- ga_sessions_20201217\n",
      "- ga_sessions_20201218\n",
      "- ga_sessions_20201219\n",
      "- ga_sessions_20201220\n",
      "- ga_sessions_20201221\n",
      "- ga_sessions_20201222\n",
      "- ga_sessions_20201223\n",
      "- ga_sessions_20201224\n",
      "- ga_sessions_20201225\n",
      "- ga_sessions_20201226\n",
      "- ga_sessions_20201227\n",
      "- ga_sessions_20201228\n",
      "- ga_sessions_20201229\n",
      "- ga_sessions_20201230\n",
      "- ga_sessions_20201231\n",
      "- ga_sessions_20210101\n",
      "- ga_sessions_20210102\n",
      "- ga_sessions_20210103\n",
      "- ga_sessions_20210104\n",
      "- ga_sessions_20210105\n",
      "- ga_sessions_20210106\n",
      "- ga_sessions_20210107\n",
      "- ga_sessions_20210108\n",
      "- ga_sessions_20210109\n",
      "- ga_sessions_20210110\n",
      "- ga_sessions_20210111\n",
      "- ga_sessions_20210112\n",
      "- ga_sessions_20210113\n",
      "- ga_sessions_20210114\n",
      "- ga_sessions_20210115\n",
      "- ga_sessions_20210116\n",
      "- ga_sessions_20210117\n",
      "- ga_sessions_20210118\n",
      "- ga_sessions_20210119\n",
      "- ga_sessions_20210120\n",
      "- ga_sessions_20210121\n",
      "- ga_sessions_20210122\n",
      "- ga_sessions_20210123\n",
      "- ga_sessions_20210124\n",
      "- ga_sessions_20210125\n",
      "- ga_sessions_20210126\n",
      "- ga_sessions_20210127\n",
      "- ga_sessions_20210128\n",
      "- ga_sessions_20210129\n",
      "- ga_sessions_20210130\n",
      "- ga_sessions_20210131\n",
      "- ga_sessions_20210201\n",
      "- ga_sessions_20210202\n",
      "- ga_sessions_20210203\n",
      "- ga_sessions_20210204\n",
      "- ga_sessions_20210205\n",
      "- ga_sessions_20210206\n",
      "- ga_sessions_20210207\n",
      "- ga_sessions_20210208\n",
      "- ga_sessions_20210209\n",
      "- ga_sessions_20210210\n",
      "- ga_sessions_20210211\n",
      "- ga_sessions_20210212\n",
      "- ga_sessions_20210213\n",
      "- ga_sessions_20210214\n",
      "- ga_sessions_20210215\n",
      "- ga_sessions_20210216\n",
      "- ga_sessions_20210217\n",
      "- ga_sessions_20210218\n",
      "- ga_sessions_20210219\n",
      "- ga_sessions_20210220\n",
      "- ga_sessions_20210221\n",
      "- ga_sessions_20210222\n",
      "- ga_sessions_20210223\n",
      "- ga_sessions_20210224\n",
      "- ga_sessions_20210225\n",
      "- ga_sessions_20210226\n",
      "- ga_sessions_20210227\n",
      "- ga_sessions_20210228\n",
      "- ga_sessions_20210301\n",
      "- ga_sessions_20210302\n",
      "- ga_sessions_20210303\n",
      "- ga_sessions_20210304\n",
      "- ga_sessions_20210305\n",
      "- ga_sessions_20210306\n",
      "- ga_sessions_20210307\n",
      "- ga_sessions_20210308\n",
      "- ga_sessions_20210309\n",
      "- ga_sessions_20210310\n",
      "- ga_sessions_20210311\n",
      "- ga_sessions_20210312\n",
      "- ga_sessions_20210313\n",
      "- ga_sessions_20210314\n",
      "- ga_sessions_20210315\n",
      "- ga_sessions_20210316\n",
      "- ga_sessions_20210317\n",
      "- ga_sessions_20210318\n",
      "- ga_sessions_20210319\n",
      "- ga_sessions_20210320\n",
      "- ga_sessions_20210321\n",
      "- ga_sessions_20210322\n",
      "- ga_sessions_20210323\n",
      "- ga_sessions_20210324\n",
      "- ga_sessions_20210325\n",
      "- ga_sessions_20210326\n",
      "- ga_sessions_20210327\n",
      "- ga_sessions_20210328\n",
      "- ga_sessions_20210329\n",
      "- ga_sessions_20210330\n",
      "- ga_sessions_20210331\n",
      "- ga_sessions_20210401\n",
      "- ga_sessions_20210402\n",
      "- ga_sessions_20210403\n",
      "- ga_sessions_20210404\n",
      "- ga_sessions_20210405\n",
      "- ga_sessions_20210406\n",
      "- ga_sessions_20210407\n",
      "- ga_sessions_20210408\n",
      "- ga_sessions_20210409\n",
      "- ga_sessions_20210410\n",
      "- ga_sessions_20210411\n",
      "- ga_sessions_20210412\n",
      "- ga_sessions_20210413\n",
      "- ga_sessions_20210414\n",
      "- ga_sessions_20210415\n",
      "- ga_sessions_20210416\n",
      "- ga_sessions_20210417\n",
      "- ga_sessions_20210418\n",
      "- ga_sessions_20210419\n",
      "- ga_sessions_20210420\n",
      "- ga_sessions_20210421\n",
      "- ga_sessions_20210422\n",
      "- ga_sessions_20210423\n",
      "- ga_sessions_20210424\n",
      "- ga_sessions_20210425\n",
      "- ga_sessions_20210426\n",
      "- ga_sessions_20210427\n",
      "- ga_sessions_20210428\n",
      "- ga_sessions_20210429\n",
      "- ga_sessions_20210430\n",
      "- ga_sessions_20210501\n",
      "- ga_sessions_20210502\n",
      "- ga_sessions_20210503\n",
      "- ga_sessions_20210504\n",
      "- ga_sessions_20210505\n",
      "- ga_sessions_20210506\n",
      "- ga_sessions_intraday_20210507\n",
      "- google_aggregate_historical\n",
      "\n",
      "Details for table 180601513.ga_sessions_20190823:\n",
      "Description: None\n",
      "Row count: 199941\n",
      "Created: 2020-09-29 22:33:29.647000+00:00\n",
      "Last modified: 2020-09-30 05:41:13.380000+00:00\n",
      "\n",
      "Schema:\n",
      "- visitorId (INTEGER)\n",
      "- visitNumber (INTEGER)\n",
      "- visitId (INTEGER)\n",
      "- visitStartTime (INTEGER)\n",
      "- date (STRING)\n",
      "- totals (RECORD)\n",
      "- trafficSource (RECORD)\n",
      "- device (RECORD)\n",
      "- geoNetwork (RECORD)\n",
      "- customDimensions (RECORD)\n",
      "- hits (RECORD)\n",
      "- fullVisitorId (STRING)\n",
      "- userId (STRING)\n",
      "- clientId (STRING)\n",
      "- channelGrouping (STRING)\n",
      "- socialEngagementType (STRING)\n",
      "\n",
      "Preview data:\n",
      "Row((None, 1, 1566562107, 1566562107, '20190823', {'visits': 1, 'hits': 1, 'pageviews': 1, 'timeOnSite': None, 'bounces': 1, 'transactions': None, 'transactionRevenue': None, 'newVisits': 1, 'screenviews': None, 'uniqueScreenviews': None, 'timeOnScreen': None, 'totalTransactionRevenue': None, 'sessionQualityDim': 0}, {'referralPath': '(not set)', 'campaign': '(not set)', 'source': '(direct)', 'medium': '(none)', 'keyword': '(not set)', 'adContent': '(not set)', 'adwordsClickInfo': {'campaignId': None, 'adGroupId': None, 'creativeId': None, 'criteriaId': None, 'page': None, 'slot': None, 'criteriaParameters': None, 'gclId': None, 'customerId': None, 'adNetworkType': None, 'targetingCriteria': None, 'isVideoAd': None}, 'isTrueDirect': True, 'campaignCode': None}, {'browser': 'Chrome', 'browserVersion': '57.0.2987.133', 'browserSize': '1280x960', 'operatingSystem': 'Windows', 'operatingSystemVersion': '10', 'isMobile': False, 'mobileDeviceBranding': None, 'mobileDeviceModel': None, 'mobileInputSelector': None, 'mobileDeviceInfo': '(not set)', 'mobileDeviceMarketingName': None, 'flashVersion': '(not set)', 'javaEnabled': False, 'language': 'en-us', 'screenColors': '0-bit', 'screenResolution': '1280x960', 'deviceCategory': 'desktop'}, {'continent': 'Asia', 'subContinent': 'Eastern Asia', 'country': 'Hong Kong', 'region': '(not set)', 'metro': '(not set)', 'city': '(not set)', 'cityId': '(not set)', 'networkDomain': '(not set)', 'latitude': '0.0000', 'longitude': '0.0000', 'networkLocation': 'microsoft corporation'}, [], [{'hitNumber': 1, 'time': 0, 'hour': 6, 'minute': 8, 'isSecure': None, 'isInteraction': True, 'isEntrance': True, 'isExit': True, 'referer': None, 'page': {'pagePath': '/eam/registration.do?nid=E_C:CL:T212CLKREOBRE162618&mkt_tok=eyJpIjoiWkRka1pUZGhZelppWVRoaSIsInQiOiJUWHdjZU42VCtUQ2lDVGJSMzc0ZklKQVNSamdmdm15MkZEQ25nQm0xT21LVjNoVmJtdUpHdmRJSnBMMEU2Y0JJc1UxeGtyYnU0VHdDa0hnWlJOYVo5VDY3NGxuNjBvelV3OFlrXC9MN0VZelkxMWFrNVhDeTJ0V1VlZ0h5QTh4XC9OIn0=', 'hostname': 'eam.centurylink.com', 'pageTitle': 'CenturyLink - Registration - Authorization', 'searchKeyword': None, 'searchCategory': None, 'pagePathLevel1': '/eam/', 'pagePathLevel2': '/registration.do?nid=E_C:CL:T212CLKREOBRE162618&mkt_tok=eyJpIjoiWkRka1pUZGhZelppWVRoaSIsInQiOiJUWHdjZU42VCtUQ2lDVGJSMzc0ZklKQVNSamdmdm15MkZEQ25nQm0xT21LVjNoVmJtdUpHdmRJSnBMMEU2Y0JJc1UxeGtyYnU0VHdDa0hnWlJOYVo5VDY3NGxuNjBvelV3OFlrXC9MN0VZelkxMWFrNVhDeTJ0V1VlZ0h5QTh4XC9OIn0=', 'pagePathLevel3': '', 'pagePathLevel4': ''}, 'transaction': {'transactionId': None, 'transactionRevenue': None, 'transactionTax': None, 'transactionShipping': None, 'affiliation': None, 'currencyCode': '(not set)', 'localTransactionRevenue': None, 'localTransactionTax': None, 'localTransactionShipping': None, 'transactionCoupon': None}, 'item': {'transactionId': None, 'productName': None, 'productCategory': None, 'productSku': None, 'itemQuantity': None, 'itemRevenue': None, 'currencyCode': '(not set)', 'localItemRevenue': None}, 'contentInfo': None, 'appInfo': {'name': None, 'version': None, 'id': None, 'installerId': None, 'appInstallerId': None, 'appName': None, 'appVersion': None, 'appId': None, 'screenName': 'eam.centurylink.com/eam/registration.do?nid=E_C:CL:T212CLKREOBRE162618&mkt_tok=eyJpIjoiWkRka1pUZGhZelppWVRoaSIsInQiOiJUWHdjZU42VCtUQ2lDVGJSMzc0ZklKQVNSamdmdm15MkZEQ25nQm0xT21LVjNoVmJtdUpHdmRJSnBMMEU2Y0JJc1UxeGtyYnU0VHdDa0hnWlJOYVo5VDY3NGxuNjBvelV3OFlrXC9MN0VZelkxMWFrNVhDeTJ0V1VlZ0h5QTh4XC9OIn0=', 'landingScreenName': 'eam.centurylink.com/eam/registration.do?nid=E_C:CL:T212CLKREOBRE162618&mkt_tok=eyJpIjoiWkRka1pUZGhZelppWVRoaSIsInQiOiJUWHdjZU42VCtUQ2lDVGJSMzc0ZklKQVNSamdmdm15MkZEQ25nQm0xT21LVjNoVmJtdUpHdmRJSnBMMEU2Y0JJc1UxeGtyYnU0VHdDa0hnWlJOYVo5VDY3NGxuNjBvelV3OFlrXC9MN0VZelkxMWFrNVhDeTJ0V1VlZ0h5QTh4XC9OIn0=', 'exitScreenName': 'eam.centurylink.com/eam/registration.do?nid=E_C:CL:T212CLKREOBRE162618&mkt_tok=eyJpIjoiWkRka1pUZGhZelppWVRoaSIsInQiOiJUWHdjZU42VCtUQ2lDVGJSMzc0ZklKQVNSamdmdm15MkZEQ25nQm0xT21LVjNoVmJtdUpHdmRJSnBMMEU2Y0JJc1UxeGtyYnU0VHdDa0hnWlJOYVo5VDY3NGxuNjBvelV3OFlrXC9MN0VZelkxMWFrNVhDeTJ0V1VlZ0h5QTh4XC9OIn0=', 'screenDepth': '0'}, 'exceptionInfo': {'description': None, 'isFatal': True, 'exceptions': None, 'fatalExceptions': None}, 'eventInfo': None, 'product': [], 'promotion': [], 'promotionActionInfo': None, 'refund': None, 'eCommerceAction': {'action_type': '0', 'step': 1, 'option': None}, 'experiment': [], 'publisher': None, 'customVariables': [], 'customDimensions': [], 'customMetrics': [], 'type': 'PAGE', 'social': {'socialInteractionNetwork': None, 'socialInteractionAction': None, 'socialInteractions': None, 'socialInteractionTarget': None, 'socialNetwork': '(not set)', 'uniqueSocialInteractions': None, 'hasSocialSourceReferral': 'No', 'socialInteractionNetworkAction': ' : '}, 'latencyTracking': None, 'sourcePropertyInfo': None, 'contentGroup': {'contentGroup1': '(not set)', 'contentGroup2': '(not set)', 'contentGroup3': '(not set)', 'contentGroup4': '(not set)', 'contentGroup5': '(not set)', 'previousContentGroup1': '(entrance)', 'previousContentGroup2': '(entrance)', 'previousContentGroup3': '(entrance)', 'previousContentGroup4': '(entrance)', 'previousContentGroup5': '(entrance)', 'contentGroupUniqueViews1': None, 'contentGroupUniqueViews2': None, 'contentGroupUniqueViews3': None, 'contentGroupUniqueViews4': None, 'contentGroupUniqueViews5': None}, 'dataSource': 'web', 'publisher_infos': []}], '6572079427996342076', None, '1530181483.1566562108', 'Direct', 'Not Socially Engaged'), {'visitorId': 0, 'visitNumber': 1, 'visitId': 2, 'visitStartTime': 3, 'date': 4, 'totals': 5, 'trafficSource': 6, 'device': 7, 'geoNetwork': 8, 'customDimensions': 9, 'hits': 10, 'fullVisitorId': 11, 'userId': 12, 'clientId': 13, 'channelGrouping': 14, 'socialEngagementType': 15})\n",
      "Row((None, 1, 1566597609, 1566597609, '20190823', {'visits': 1, 'hits': 1, 'pageviews': 1, 'timeOnSite': None, 'bounces': 1, 'transactions': None, 'transactionRevenue': None, 'newVisits': 1, 'screenviews': None, 'uniqueScreenviews': None, 'timeOnScreen': None, 'totalTransactionRevenue': None, 'sessionQualityDim': 0}, {'referralPath': '(not set)', 'campaign': '(not set)', 'source': '(direct)', 'medium': '(none)', 'keyword': '(not set)', 'adContent': '(not set)', 'adwordsClickInfo': {'campaignId': None, 'adGroupId': None, 'creativeId': None, 'criteriaId': None, 'page': None, 'slot': None, 'criteriaParameters': None, 'gclId': None, 'customerId': None, 'adNetworkType': None, 'targetingCriteria': None, 'isVideoAd': None}, 'isTrueDirect': True, 'campaignCode': None}, {'browser': 'Chrome', 'browserVersion': '57.0.2987.133', 'browserSize': '1260x960', 'operatingSystem': 'Windows', 'operatingSystemVersion': '10', 'isMobile': False, 'mobileDeviceBranding': None, 'mobileDeviceModel': None, 'mobileInputSelector': None, 'mobileDeviceInfo': '(not set)', 'mobileDeviceMarketingName': None, 'flashVersion': '(not set)', 'javaEnabled': False, 'language': 'en-us', 'screenColors': '0-bit', 'screenResolution': '1280x960', 'deviceCategory': 'desktop'}, {'continent': 'Americas', 'subContinent': 'Northern America', 'country': 'United States', 'region': 'Texas', 'metro': 'San Antonio TX', 'city': 'San Antonio', 'cityId': '1026759', 'networkDomain': '(not set)', 'latitude': '29.4241', 'longitude': '-98.4936', 'networkLocation': 'microsoft corporation'}, [], [{'hitNumber': 1, 'time': 0, 'hour': 16, 'minute': 0, 'isSecure': None, 'isInteraction': True, 'isEntrance': True, 'isExit': True, 'referer': None, 'page': {'pagePath': '/home/tv/?nid=E_C:CL:T317CREOMYACCTAPP3318&mkt_tok=eyJpIjoiTkRZNFpEUmtZMkl6WmpVeiIsInQiOiJ1XC9VRGFkaUFCSkRJSzlXZ21NRXE5NElUMkJ1R0lZekM4TzFlZmVHclZMMEJCZkVQWHpcL0ZsdjlLQjh2aHI0T0NLcE85UnlrQWZCRUpKWUZwcnl5bVlcL244TUJnS3VBVExjc3pSQ0NVcnB3T1F5TnhNTWpJNWhCcUdLaXE4UzUrbiJ9', 'hostname': 'www.centurylink.com', 'pageTitle': 'DIRECTVÂ® TV Packages | Starting at $35/mo | CenturyLink', 'searchKeyword': None, 'searchCategory': None, 'pagePathLevel1': '/home/', 'pagePathLevel2': '/tv/', 'pagePathLevel3': '/?nid=E_C:CL:T317CREOMYACCTAPP3318&mkt_tok=eyJpIjoiTkRZNFpEUmtZMkl6WmpVeiIsInQiOiJ1XC9VRGFkaUFCSkRJSzlXZ21NRXE5NElUMkJ1R0lZekM4TzFlZmVHclZMMEJCZkVQWHpcL0ZsdjlLQjh2aHI0T0NLcE85UnlrQWZCRUpKWUZwcnl5bVlcL244TUJnS3VBVExjc3pSQ0NVcnB3T1F5TnhNTWpJNWhCcUdLaXE4UzUrbiJ9', 'pagePathLevel4': ''}, 'transaction': {'transactionId': None, 'transactionRevenue': None, 'transactionTax': None, 'transactionShipping': None, 'affiliation': None, 'currencyCode': '(not set)', 'localTransactionRevenue': None, 'localTransactionTax': None, 'localTransactionShipping': None, 'transactionCoupon': None}, 'item': {'transactionId': None, 'productName': None, 'productCategory': None, 'productSku': None, 'itemQuantity': None, 'itemRevenue': None, 'currencyCode': '(not set)', 'localItemRevenue': None}, 'contentInfo': None, 'appInfo': {'name': None, 'version': None, 'id': None, 'installerId': None, 'appInstallerId': None, 'appName': None, 'appVersion': None, 'appId': None, 'screenName': 'www.centurylink.com/home/tv/?nid=E_C:CL:T317CREOMYACCTAPP3318&mkt_tok=eyJpIjoiTkRZNFpEUmtZMkl6WmpVeiIsInQiOiJ1XC9VRGFkaUFCSkRJSzlXZ21NRXE5NElUMkJ1R0lZekM4TzFlZmVHclZMMEJCZkVQWHpcL0ZsdjlLQjh2aHI0T0NLcE85UnlrQWZCRUpKWUZwcnl5bVlcL244TUJnS3VBVExjc3pSQ0NVcnB3T1F5TnhNTWpJNWhCcUdLaXE4UzUrbiJ9', 'landingScreenName': 'www.centurylink.com/home/tv/?nid=E_C:CL:T317CREOMYACCTAPP3318&mkt_tok=eyJpIjoiTkRZNFpEUmtZMkl6WmpVeiIsInQiOiJ1XC9VRGFkaUFCSkRJSzlXZ21NRXE5NElUMkJ1R0lZekM4TzFlZmVHclZMMEJCZkVQWHpcL0ZsdjlLQjh2aHI0T0NLcE85UnlrQWZCRUpKWUZwcnl5bVlcL244TUJnS3VBVExjc3pSQ0NVcnB3T1F5TnhNTWpJNWhCcUdLaXE4UzUrbiJ9', 'exitScreenName': 'www.centurylink.com/home/tv/?nid=E_C:CL:T317CREOMYACCTAPP3318&mkt_tok=eyJpIjoiTkRZNFpEUmtZMkl6WmpVeiIsInQiOiJ1XC9VRGFkaUFCSkRJSzlXZ21NRXE5NElUMkJ1R0lZekM4TzFlZmVHclZMMEJCZkVQWHpcL0ZsdjlLQjh2aHI0T0NLcE85UnlrQWZCRUpKWUZwcnl5bVlcL244TUJnS3VBVExjc3pSQ0NVcnB3T1F5TnhNTWpJNWhCcUdLaXE4UzUrbiJ9', 'screenDepth': '0'}, 'exceptionInfo': {'description': None, 'isFatal': True, 'exceptions': None, 'fatalExceptions': None}, 'eventInfo': None, 'product': [], 'promotion': [], 'promotionActionInfo': None, 'refund': None, 'eCommerceAction': {'action_type': '0', 'step': 1, 'option': None}, 'experiment': [], 'publisher': None, 'customVariables': [], 'customDimensions': [], 'customMetrics': [], 'type': 'PAGE', 'social': {'socialInteractionNetwork': None, 'socialInteractionAction': None, 'socialInteractions': None, 'socialInteractionTarget': None, 'socialNetwork': '(not set)', 'uniqueSocialInteractions': None, 'hasSocialSourceReferral': 'No', 'socialInteractionNetworkAction': ' : '}, 'latencyTracking': None, 'sourcePropertyInfo': None, 'contentGroup': {'contentGroup1': '(not set)', 'contentGroup2': '(not set)', 'contentGroup3': '(not set)', 'contentGroup4': '(not set)', 'contentGroup5': '(not set)', 'previousContentGroup1': '(entrance)', 'previousContentGroup2': '(entrance)', 'previousContentGroup3': '(entrance)', 'previousContentGroup4': '(entrance)', 'previousContentGroup5': '(entrance)', 'contentGroupUniqueViews1': None, 'contentGroupUniqueViews2': None, 'contentGroupUniqueViews3': None, 'contentGroupUniqueViews4': None, 'contentGroupUniqueViews5': None}, 'dataSource': 'web', 'publisher_infos': []}], '8560437814739624425', None, '1993132246.1566597609', 'Direct', 'Not Socially Engaged'), {'visitorId': 0, 'visitNumber': 1, 'visitId': 2, 'visitStartTime': 3, 'date': 4, 'totals': 5, 'trafficSource': 6, 'device': 7, 'geoNetwork': 8, 'customDimensions': 9, 'hits': 10, 'fullVisitorId': 11, 'userId': 12, 'clientId': 13, 'channelGrouping': 14, 'socialEngagementType': 15})\n",
      "Row((None, 1, 1566546827, 1566546827, '20190823', {'visits': 1, 'hits': 1, 'pageviews': 1, 'timeOnSite': None, 'bounces': 1, 'transactions': None, 'transactionRevenue': None, 'newVisits': 1, 'screenviews': None, 'uniqueScreenviews': None, 'timeOnScreen': None, 'totalTransactionRevenue': None, 'sessionQualityDim': 0}, {'referralPath': '(not set)', 'campaign': '(not set)', 'source': '(direct)', 'medium': '(none)', 'keyword': '(not set)', 'adContent': '(not set)', 'adwordsClickInfo': {'campaignId': None, 'adGroupId': None, 'creativeId': None, 'criteriaId': None, 'page': None, 'slot': None, 'criteriaParameters': None, 'gclId': None, 'customerId': None, 'adNetworkType': None, 'targetingCriteria': None, 'isVideoAd': None}, 'isTrueDirect': True, 'campaignCode': None}, {'browser': 'Chrome', 'browserVersion': '57.0.2987.133', 'browserSize': '1260x960', 'operatingSystem': 'Windows', 'operatingSystemVersion': '10', 'isMobile': False, 'mobileDeviceBranding': None, 'mobileDeviceModel': None, 'mobileInputSelector': None, 'mobileDeviceInfo': '(not set)', 'mobileDeviceMarketingName': None, 'flashVersion': '(not set)', 'javaEnabled': False, 'language': 'en-us', 'screenColors': '0-bit', 'screenResolution': '1280x960', 'deviceCategory': 'desktop'}, {'continent': 'Americas', 'subContinent': 'Northern America', 'country': 'United States', 'region': '(not set)', 'metro': '(not set)', 'city': '(not set)', 'cityId': '(not set)', 'networkDomain': '(not set)', 'latitude': '0.0000', 'longitude': '0.0000', 'networkLocation': 'microsoft corporation'}, [], [{'hitNumber': 1, 'time': 0, 'hour': 1, 'minute': 53, 'isSecure': None, 'isInteraction': True, 'isEntrance': True, 'isExit': True, 'referer': None, 'page': {'pagePath': '/home/phone/?nid=E_C:Q:T19CTRGT01AE114814&mkt_tok=eyJpIjoiTmprME5XRTBZVEE1WkdJeiIsInQiOiJadnoxU0lQb0VBb0VORGh6a1NXaG9TUXIzdWo1cGpCSXQ3YnY5bEgxNFhWM1Z4TnpEek0zbTFPNWxqMWtFdjA0VHdOWE85Zk1XVTFqM1RrK3R2VVlQclc2ZGl5WEkrbTRZSUtsYzJFVHppNDE2WmI2bDNvOU1aQ0FXQ3AxTlFOWiJ9', 'hostname': 'www.centurylink.com', 'pageTitle': 'Home Phone Service | Unlimited Nationwide Calling | CenturyLink', 'searchKeyword': None, 'searchCategory': None, 'pagePathLevel1': '/home/', 'pagePathLevel2': '/phone/', 'pagePathLevel3': '/?nid=E_C:Q:T19CTRGT01AE114814&mkt_tok=eyJpIjoiTmprME5XRTBZVEE1WkdJeiIsInQiOiJadnoxU0lQb0VBb0VORGh6a1NXaG9TUXIzdWo1cGpCSXQ3YnY5bEgxNFhWM1Z4TnpEek0zbTFPNWxqMWtFdjA0VHdOWE85Zk1XVTFqM1RrK3R2VVlQclc2ZGl5WEkrbTRZSUtsYzJFVHppNDE2WmI2bDNvOU1aQ0FXQ3AxTlFOWiJ9', 'pagePathLevel4': ''}, 'transaction': {'transactionId': None, 'transactionRevenue': None, 'transactionTax': None, 'transactionShipping': None, 'affiliation': None, 'currencyCode': '(not set)', 'localTransactionRevenue': None, 'localTransactionTax': None, 'localTransactionShipping': None, 'transactionCoupon': None}, 'item': {'transactionId': None, 'productName': None, 'productCategory': None, 'productSku': None, 'itemQuantity': None, 'itemRevenue': None, 'currencyCode': '(not set)', 'localItemRevenue': None}, 'contentInfo': None, 'appInfo': {'name': None, 'version': None, 'id': None, 'installerId': None, 'appInstallerId': None, 'appName': None, 'appVersion': None, 'appId': None, 'screenName': 'www.centurylink.com/home/phone/?nid=E_C:Q:T19CTRGT01AE114814&mkt_tok=eyJpIjoiTmprME5XRTBZVEE1WkdJeiIsInQiOiJadnoxU0lQb0VBb0VORGh6a1NXaG9TUXIzdWo1cGpCSXQ3YnY5bEgxNFhWM1Z4TnpEek0zbTFPNWxqMWtFdjA0VHdOWE85Zk1XVTFqM1RrK3R2VVlQclc2ZGl5WEkrbTRZSUtsYzJFVHppNDE2WmI2bDNvOU1aQ0FXQ3AxTlFOWiJ9', 'landingScreenName': 'www.centurylink.com/home/phone/?nid=E_C:Q:T19CTRGT01AE114814&mkt_tok=eyJpIjoiTmprME5XRTBZVEE1WkdJeiIsInQiOiJadnoxU0lQb0VBb0VORGh6a1NXaG9TUXIzdWo1cGpCSXQ3YnY5bEgxNFhWM1Z4TnpEek0zbTFPNWxqMWtFdjA0VHdOWE85Zk1XVTFqM1RrK3R2VVlQclc2ZGl5WEkrbTRZSUtsYzJFVHppNDE2WmI2bDNvOU1aQ0FXQ3AxTlFOWiJ9', 'exitScreenName': 'www.centurylink.com/home/phone/?nid=E_C:Q:T19CTRGT01AE114814&mkt_tok=eyJpIjoiTmprME5XRTBZVEE1WkdJeiIsInQiOiJadnoxU0lQb0VBb0VORGh6a1NXaG9TUXIzdWo1cGpCSXQ3YnY5bEgxNFhWM1Z4TnpEek0zbTFPNWxqMWtFdjA0VHdOWE85Zk1XVTFqM1RrK3R2VVlQclc2ZGl5WEkrbTRZSUtsYzJFVHppNDE2WmI2bDNvOU1aQ0FXQ3AxTlFOWiJ9', 'screenDepth': '0'}, 'exceptionInfo': {'description': None, 'isFatal': True, 'exceptions': None, 'fatalExceptions': None}, 'eventInfo': None, 'product': [], 'promotion': [], 'promotionActionInfo': None, 'refund': None, 'eCommerceAction': {'action_type': '0', 'step': 1, 'option': None}, 'experiment': [], 'publisher': None, 'customVariables': [], 'customDimensions': [], 'customMetrics': [], 'type': 'PAGE', 'social': {'socialInteractionNetwork': None, 'socialInteractionAction': None, 'socialInteractions': None, 'socialInteractionTarget': None, 'socialNetwork': '(not set)', 'uniqueSocialInteractions': None, 'hasSocialSourceReferral': 'No', 'socialInteractionNetworkAction': ' : '}, 'latencyTracking': None, 'sourcePropertyInfo': None, 'contentGroup': {'contentGroup1': '(not set)', 'contentGroup2': '(not set)', 'contentGroup3': '(not set)', 'contentGroup4': '(not set)', 'contentGroup5': '(not set)', 'previousContentGroup1': '(entrance)', 'previousContentGroup2': '(entrance)', 'previousContentGroup3': '(entrance)', 'previousContentGroup4': '(entrance)', 'previousContentGroup5': '(entrance)', 'contentGroupUniqueViews1': None, 'contentGroupUniqueViews2': None, 'contentGroupUniqueViews3': None, 'contentGroupUniqueViews4': None, 'contentGroupUniqueViews5': None}, 'dataSource': 'web', 'publisher_infos': []}], '8987949762642680716', None, '2092670128.1566546828', 'Direct', 'Not Socially Engaged'), {'visitorId': 0, 'visitNumber': 1, 'visitId': 2, 'visitStartTime': 3, 'date': 4, 'totals': 5, 'trafficSource': 6, 'device': 7, 'geoNetwork': 8, 'customDimensions': 9, 'hits': 10, 'fullVisitorId': 11, 'userId': 12, 'clientId': 13, 'channelGrouping': 14, 'socialEngagementType': 15})\n",
      "Row((None, 1, 1566611656, 1566611656, '20190823', {'visits': 1, 'hits': 1, 'pageviews': 1, 'timeOnSite': None, 'bounces': 1, 'transactions': None, 'transactionRevenue': None, 'newVisits': 1, 'screenviews': None, 'uniqueScreenviews': None, 'timeOnScreen': None, 'totalTransactionRevenue': None, 'sessionQualityDim': 0}, {'referralPath': '(not set)', 'campaign': '(not set)', 'source': '(direct)', 'medium': '(none)', 'keyword': '(not set)', 'adContent': '(not set)', 'adwordsClickInfo': {'campaignId': None, 'adGroupId': None, 'creativeId': None, 'criteriaId': None, 'page': None, 'slot': None, 'criteriaParameters': None, 'gclId': None, 'customerId': None, 'adNetworkType': None, 'targetingCriteria': None, 'isVideoAd': None}, 'isTrueDirect': True, 'campaignCode': None}, {'browser': 'Chrome', 'browserVersion': '57.0.2987.133', 'browserSize': '1280x960', 'operatingSystem': 'Windows', 'operatingSystemVersion': '10', 'isMobile': False, 'mobileDeviceBranding': None, 'mobileDeviceModel': None, 'mobileInputSelector': None, 'mobileDeviceInfo': '(not set)', 'mobileDeviceMarketingName': None, 'flashVersion': '(not set)', 'javaEnabled': False, 'language': 'en-us', 'screenColors': '0-bit', 'screenResolution': '1280x960', 'deviceCategory': 'desktop'}, {'continent': 'Americas', 'subContinent': 'Northern America', 'country': 'United States', 'region': 'Washington', 'metro': 'Spokane WA', 'city': 'Quincy', 'cityId': '1027721', 'networkDomain': '(not set)', 'latitude': '47.2343', 'longitude': '-119.8525', 'networkLocation': 'microsoft corporation'}, [], [{'hitNumber': 1, 'time': 0, 'hour': 19, 'minute': 54, 'isSecure': None, 'isInteraction': True, 'isEntrance': True, 'isExit': True, 'referer': None, 'page': {'pagePath': '/eam/loginTrouble/resetPwdToken.do?token=Hqqo0d6FrRWcOHavTTFmdSla3zHALEoCq82rNBWnQWVV4JOSrBdxzkeJJw7/qQsuhDJVIWoGuZdxorRp7BpLxg==&refCode=MA_00012', 'hostname': 'eam.centurylink.com', 'pageTitle': 'CenturyLink - Reset Password', 'searchKeyword': None, 'searchCategory': None, 'pagePathLevel1': '/eam/', 'pagePathLevel2': '/loginTrouble/', 'pagePathLevel3': '/resetPwdToken.do?token=Hqqo0d6FrRWcOHavTTFmdSla3zHALEoCq82rNBWnQWVV4JOSrBdxzkeJJw7/qQsuhDJVIWoGuZdxorRp7BpLxg==&refCode=MA_00012', 'pagePathLevel4': ''}, 'transaction': {'transactionId': None, 'transactionRevenue': None, 'transactionTax': None, 'transactionShipping': None, 'affiliation': None, 'currencyCode': '(not set)', 'localTransactionRevenue': None, 'localTransactionTax': None, 'localTransactionShipping': None, 'transactionCoupon': None}, 'item': {'transactionId': None, 'productName': None, 'productCategory': None, 'productSku': None, 'itemQuantity': None, 'itemRevenue': None, 'currencyCode': '(not set)', 'localItemRevenue': None}, 'contentInfo': None, 'appInfo': {'name': None, 'version': None, 'id': None, 'installerId': None, 'appInstallerId': None, 'appName': None, 'appVersion': None, 'appId': None, 'screenName': 'eam.centurylink.com/eam/loginTrouble/resetPwdToken.do?token=Hqqo0d6FrRWcOHavTTFmdSla3zHALEoCq82rNBWnQWVV4JOSrBdxzkeJJw7/qQsuhDJVIWoGuZdxorRp7BpLxg==&refCode=MA_00012', 'landingScreenName': 'eam.centurylink.com/eam/loginTrouble/resetPwdToken.do?token=Hqqo0d6FrRWcOHavTTFmdSla3zHALEoCq82rNBWnQWVV4JOSrBdxzkeJJw7/qQsuhDJVIWoGuZdxorRp7BpLxg==&refCode=MA_00012', 'exitScreenName': 'eam.centurylink.com/eam/loginTrouble/resetPwdToken.do?token=Hqqo0d6FrRWcOHavTTFmdSla3zHALEoCq82rNBWnQWVV4JOSrBdxzkeJJw7/qQsuhDJVIWoGuZdxorRp7BpLxg==&refCode=MA_00012', 'screenDepth': '0'}, 'exceptionInfo': {'description': None, 'isFatal': True, 'exceptions': None, 'fatalExceptions': None}, 'eventInfo': None, 'product': [], 'promotion': [], 'promotionActionInfo': None, 'refund': None, 'eCommerceAction': {'action_type': '0', 'step': 1, 'option': None}, 'experiment': [], 'publisher': None, 'customVariables': [], 'customDimensions': [], 'customMetrics': [], 'type': 'PAGE', 'social': {'socialInteractionNetwork': None, 'socialInteractionAction': None, 'socialInteractions': None, 'socialInteractionTarget': None, 'socialNetwork': '(not set)', 'uniqueSocialInteractions': None, 'hasSocialSourceReferral': 'No', 'socialInteractionNetworkAction': ' : '}, 'latencyTracking': None, 'sourcePropertyInfo': None, 'contentGroup': {'contentGroup1': '(not set)', 'contentGroup2': '(not set)', 'contentGroup3': '(not set)', 'contentGroup4': '(not set)', 'contentGroup5': '(not set)', 'previousContentGroup1': '(entrance)', 'previousContentGroup2': '(entrance)', 'previousContentGroup3': '(entrance)', 'previousContentGroup4': '(entrance)', 'previousContentGroup5': '(entrance)', 'contentGroupUniqueViews1': None, 'contentGroupUniqueViews2': None, 'contentGroupUniqueViews3': None, 'contentGroupUniqueViews4': None, 'contentGroupUniqueViews5': None}, 'dataSource': 'web', 'publisher_infos': []}], '9165215879131076809', None, '2133943112.1566611657', 'Direct', 'Not Socially Engaged'), {'visitorId': 0, 'visitNumber': 1, 'visitId': 2, 'visitStartTime': 3, 'date': 4, 'totals': 5, 'trafficSource': 6, 'device': 7, 'geoNetwork': 8, 'customDimensions': 9, 'hits': 10, 'fullVisitorId': 11, 'userId': 12, 'clientId': 13, 'channelGrouping': 14, 'socialEngagementType': 15})\n",
      "Row((None, 1, 1566583702, 1566583702, '20190823', {'visits': 1, 'hits': 1, 'pageviews': 1, 'timeOnSite': None, 'bounces': 1, 'transactions': None, 'transactionRevenue': None, 'newVisits': 1, 'screenviews': None, 'uniqueScreenviews': None, 'timeOnScreen': None, 'totalTransactionRevenue': None, 'sessionQualityDim': 0}, {'referralPath': '(not set)', 'campaign': '(not set)', 'source': '(direct)', 'medium': '(none)', 'keyword': '(not set)', 'adContent': '(not set)', 'adwordsClickInfo': {'campaignId': None, 'adGroupId': None, 'creativeId': None, 'criteriaId': None, 'page': None, 'slot': None, 'criteriaParameters': None, 'gclId': None, 'customerId': None, 'adNetworkType': None, 'targetingCriteria': None, 'isVideoAd': None}, 'isTrueDirect': True, 'campaignCode': None}, {'browser': 'Chrome', 'browserVersion': '57.0.2987.133', 'browserSize': '1280x960', 'operatingSystem': 'Windows', 'operatingSystemVersion': '10', 'isMobile': False, 'mobileDeviceBranding': None, 'mobileDeviceModel': None, 'mobileInputSelector': None, 'mobileDeviceInfo': '(not set)', 'mobileDeviceMarketingName': None, 'flashVersion': '(not set)', 'javaEnabled': False, 'language': 'en-us', 'screenColors': '0-bit', 'screenResolution': '1280x960', 'deviceCategory': 'desktop'}, {'continent': 'Americas', 'subContinent': 'Northern America', 'country': 'United States', 'region': '(not set)', 'metro': '(not set)', 'city': '(not set)', 'cityId': '(not set)', 'networkDomain': '(not set)', 'latitude': '0.0000', 'longitude': '0.0000', 'networkLocation': 'microsoft corporation'}, [], [{'hitNumber': 1, 'time': 0, 'hour': 12, 'minute': 8, 'isSecure': None, 'isInteraction': True, 'isEntrance': True, 'isExit': True, 'referer': None, 'page': {'pagePath': '/eam/loginTrouble/resetPwdToken.do?token=H+KMlEYB7CIHI67oc/HiJ00928Uhtumlso4tgD5TVDbsjRUMvEI3S7ZRhAXsXbtR', 'hostname': 'eam.centurylink.com', 'pageTitle': 'CenturyLink - Reset Password', 'searchKeyword': None, 'searchCategory': None, 'pagePathLevel1': '/eam/', 'pagePathLevel2': '/loginTrouble/', 'pagePathLevel3': '/resetPwdToken.do?token=H+KMlEYB7CIHI67oc/HiJ00928Uhtumlso4tgD5TVDbsjRUMvEI3S7ZRhAXsXbtR', 'pagePathLevel4': ''}, 'transaction': {'transactionId': None, 'transactionRevenue': None, 'transactionTax': None, 'transactionShipping': None, 'affiliation': None, 'currencyCode': '(not set)', 'localTransactionRevenue': None, 'localTransactionTax': None, 'localTransactionShipping': None, 'transactionCoupon': None}, 'item': {'transactionId': None, 'productName': None, 'productCategory': None, 'productSku': None, 'itemQuantity': None, 'itemRevenue': None, 'currencyCode': '(not set)', 'localItemRevenue': None}, 'contentInfo': None, 'appInfo': {'name': None, 'version': None, 'id': None, 'installerId': None, 'appInstallerId': None, 'appName': None, 'appVersion': None, 'appId': None, 'screenName': 'eam.centurylink.com/eam/loginTrouble/resetPwdToken.do?token=H+KMlEYB7CIHI67oc/HiJ00928Uhtumlso4tgD5TVDbsjRUMvEI3S7ZRhAXsXbtR', 'landingScreenName': 'eam.centurylink.com/eam/loginTrouble/resetPwdToken.do?token=H+KMlEYB7CIHI67oc/HiJ00928Uhtumlso4tgD5TVDbsjRUMvEI3S7ZRhAXsXbtR', 'exitScreenName': 'eam.centurylink.com/eam/loginTrouble/resetPwdToken.do?token=H+KMlEYB7CIHI67oc/HiJ00928Uhtumlso4tgD5TVDbsjRUMvEI3S7ZRhAXsXbtR', 'screenDepth': '0'}, 'exceptionInfo': {'description': None, 'isFatal': True, 'exceptions': None, 'fatalExceptions': None}, 'eventInfo': None, 'product': [], 'promotion': [], 'promotionActionInfo': None, 'refund': None, 'eCommerceAction': {'action_type': '0', 'step': 1, 'option': None}, 'experiment': [], 'publisher': None, 'customVariables': [], 'customDimensions': [], 'customMetrics': [], 'type': 'PAGE', 'social': {'socialInteractionNetwork': None, 'socialInteractionAction': None, 'socialInteractions': None, 'socialInteractionTarget': None, 'socialNetwork': '(not set)', 'uniqueSocialInteractions': None, 'hasSocialSourceReferral': 'No', 'socialInteractionNetworkAction': ' : '}, 'latencyTracking': None, 'sourcePropertyInfo': None, 'contentGroup': {'contentGroup1': '(not set)', 'contentGroup2': '(not set)', 'contentGroup3': '(not set)', 'contentGroup4': '(not set)', 'contentGroup5': '(not set)', 'previousContentGroup1': '(entrance)', 'previousContentGroup2': '(entrance)', 'previousContentGroup3': '(entrance)', 'previousContentGroup4': '(entrance)', 'previousContentGroup5': '(entrance)', 'contentGroupUniqueViews1': None, 'contentGroupUniqueViews2': None, 'contentGroupUniqueViews3': None, 'contentGroupUniqueViews4': None, 'contentGroupUniqueViews5': None}, 'dataSource': 'web', 'publisher_infos': []}], '1314171341722626966', None, '305979359.1566583702', 'Direct', 'Not Socially Engaged'), {'visitorId': 0, 'visitNumber': 1, 'visitId': 2, 'visitStartTime': 3, 'date': 4, 'totals': 5, 'trafficSource': 6, 'device': 7, 'geoNetwork': 8, 'customDimensions': 9, 'hits': 10, 'fullVisitorId': 11, 'userId': 12, 'clientId': 13, 'channelGrouping': 14, 'socialEngagementType': 15})\n"
     ]
    }
   ],
   "source": [
    "# Explore Available Tables\n",
    "#\n",
    "# Let's explore the available tables in your BigQuery project.\n",
    "\n",
    "# Import BigQuery client\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# List datasets\n",
    "datasets = list(client.list_datasets())\n",
    "print(f\"Datasets in project {PROJECT_ID}:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"- {dataset.dataset_id}\")\n",
    "\n",
    "# Choose a dataset to explore\n",
    "if datasets:\n",
    "    dataset_id = datasets[0].dataset_id\n",
    "    print(f\"\\nTables in dataset {dataset_id}:\")\n",
    "    tables = list(client.list_tables(dataset_id))\n",
    "    for table in tables:\n",
    "        print(f\"- {table.table_id}\")\n",
    "        \n",
    "    # Get more details about the first table\n",
    "    if tables:\n",
    "        first_table = tables[0]\n",
    "        table_ref = f\"{dataset_id}.{first_table.table_id}\"\n",
    "        table = client.get_table(table_ref)\n",
    "        \n",
    "        print(f\"\\nDetails for table {table_ref}:\")\n",
    "        print(f\"Description: {table.description}\")\n",
    "        print(f\"Row count: {table.num_rows}\")\n",
    "        print(f\"Created: {table.created}\")\n",
    "        print(f\"Last modified: {table.modified}\")\n",
    "        \n",
    "        print(\"\\nSchema:\")\n",
    "        for field in table.schema:\n",
    "            print(f\"- {field.name} ({field.field_type})\")\n",
    "            \n",
    "        # Preview data\n",
    "        print(\"\\nPreview data:\")\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{table_ref}` LIMIT 5\"\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        \n",
    "        for row in results:\n",
    "            print(row)\n",
    "else:\n",
    "    print(\"No datasets found in this project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from BigQuery\n",
    "\n",
    "Now let's extract features from a BigQuery table. Replace `TABLE_ID` with the table you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:02,646 - terabyte_feature_extractor - INFO - Initialized TerabyteFeatureExtractor with chunk_size=100000, max_memory_gb=16.0\n",
      "2025-03-05 23:25:02,718 - terabyte_feature_extractor - INFO - BigQuery connection set up successfully\n",
      "2025-03-05 23:25:02,842 - terabyte_feature_extractor - INFO - Starting data preparation for table TEST1.ctl_modem_speedtest_event\n",
      "2025-03-05 23:25:02,843 - terabyte_feature_extractor - INFO - Limiting to 10000 rows (1 chunks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mlx backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job faf11a84-df7d-458c-a4d4-0fb01c8d270b is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:faf11a84-df7d-458c-a4d4-0fb01c8d270b&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 16cc4fd4-085b-47f2-872f-98e9002b9d75 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:16cc4fd4-085b-47f2-872f-98e9002b9d75&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 195244f7-efe0-4292-8791-0bb049f201e4 is DONE. 8 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:195244f7-efe0-4292-8791-0bb049f201e4&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:09,980 - terabyte_feature_extractor - INFO - Table TEST1.ctl_modem_speedtest_event has approximately 241 rows\n",
      "2025-03-05 23:25:09,981 - terabyte_feature_extractor - INFO - Processing approximately 241 rows in 1 chunks of 100000\n",
      "2025-03-05 23:25:09,981 - terabyte_feature_extractor - INFO - Processing chunk 1/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job d01ef61b-20ba-42ba-a539-69501bf721cb is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:d01ef61b-20ba-42ba-a539-69501bf721cb&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:13,598 - terabyte_feature_extractor - INFO - Chunk 1/1 processed in 3.62s. Progress: 100.0%. Estimated time remaining: 0.00s\n",
      "2025-03-05 23:25:13,599 - terabyte_feature_extractor - INFO - Current memory usage: 1.24 GB\n",
      "2025-03-05 23:25:13,744 - terabyte_feature_extractor - INFO - Completed processing 1 chunks in 3.76s\n",
      "2025-03-05 23:25:13,745 - terabyte_feature_extractor - INFO - Combining BigFrames DataFrame results\n",
      "2025-03-05 23:25:13,746 - terabyte_feature_extractor - INFO - Detected 3 numeric, 0 datetime, 12 categorical, 0 boolean, and 0 struct columns\n",
      "2025-03-05 23:25:13,746 - terabyte_feature_extractor - INFO - Splitting data with 241 rows\n",
      "2025-03-05 23:25:13,766 - terabyte_feature_extractor - INFO - Random split ratios: 80/20 (train/temp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job e39a89b0-be55-4579-9f09-4a7f1c9cfabe is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:e39a89b0-be55-4579-9f09-4a7f1c9cfabe&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:15,476 - terabyte_feature_extractor - INFO - Validation/test split ratios: 50/50 from temp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job fa228f02-c6d3-4629-8573-1a10bc8e38b8 is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:fa228f02-c6d3-4629-8573-1a10bc8e38b8&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 97f537f6-3743-48ce-a087-f7bbe1cb8950 is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:97f537f6-3743-48ce-a087-f7bbe1cb8950&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job b46ee698-1593-41c8-86b2-2536aa3d08ae is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:b46ee698-1593-41c8-86b2-2536aa3d08ae&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:20,891 - terabyte_feature_extractor - INFO - Split result: 194 train, 16 validation, 31 test rows\n",
      "2025-03-05 23:25:20,892 - terabyte_feature_extractor - INFO - Processing training data\n",
      "2025-03-05 23:25:20,893 - terabyte_feature_extractor - INFO - Preparing dataframe (is_train=True) with 194 rows\n",
      "2025-03-05 23:25:20,894 - terabyte_feature_extractor - INFO - Initial features: ['downloadThroughput', 'uploadThroughput']\n",
      "2025-03-05 23:25:20,896 - terabyte_feature_extractor - INFO - Encoding 12 categorical columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 007d887c-d2ad-4ca2-9ba6-b5b470d2aa7f is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:007d887c-d2ad-4ca2-9ba6-b5b470d2aa7f&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job ab43ad33-9454-4ae0-81e1-62ff87585482 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:ab43ad33-9454-4ae0-81e1-62ff87585482&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:28,526 - terabyte_feature_extractor - INFO - Final features: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job c9bf4a62-93b8-451f-b764-cc006ecc098b is DONE. 98.1 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:c9bf4a62-93b8-451f-b764-cc006ecc098b&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:31,744 - terabyte_feature_extractor - WARNING - No features to fit imputer and scaler\n",
      "2025-03-05 23:25:31,744 - terabyte_feature_extractor - INFO - Scaler and imputer fitted and applied on training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Load job 2e0c290d-5935-4eba-9914-ed2e58d59eb6 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:2e0c290d-5935-4eba-9914-ed2e58d59eb6&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:36,001 - terabyte_feature_extractor - INFO - Dataframe prepared successfully with 0 features\n",
      "2025-03-05 23:25:36,002 - terabyte_feature_extractor - INFO - Processing validation data\n",
      "2025-03-05 23:25:36,003 - terabyte_feature_extractor - INFO - Preparing dataframe (is_train=False) with 16 rows\n",
      "2025-03-05 23:25:36,003 - terabyte_feature_extractor - INFO - Initial features: ['downloadThroughput', 'uploadThroughput']\n",
      "2025-03-05 23:25:36,004 - terabyte_feature_extractor - INFO - Encoding 12 categorical columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job fe0fb753-8924-4ee6-a6eb-a05711594ad1 is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:fe0fb753-8924-4ee6-a6eb-a05711594ad1&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n",
      "/Volumes/stuff/Projects/LNNDemo/emberharmony/features/terabyte_feature_extractor_bigframes.py:670: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dummy[col] = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Load job be6c1dd3-6cf3-408b-b30e-dfdf07ddb467 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:be6c1dd3-6cf3-408b-b30e-dfdf07ddb467&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:44,527 - terabyte_feature_extractor - INFO - Final features: ['eventId_00d19697_47b6_483a_9318_2b036b8255dd', 'eventId_017f4c5b_1638_4f27_99af_0f1a9719514a', 'eventId_02d395d9_182c_47c4_bd9c_4cc64a5b28fc', 'eventId_0315f760_b158_4f76_9617_5055a3d2cc9b', 'eventId_040682e7_186d_4b83_add4_aa89787c7286', 'eventId_0551eeb4_6b24_4519_84e2_3a297d6a1df8', 'eventId_05f352e3_4003_48e4_a572_ce06a85abf12', 'eventId_077117fc_54e3_4323_972b_286446e21c82', 'eventId_078017fc_6234_46ec_9536_df0a9a111e2a', 'eventId_09f53b24_ce2f_4216_b89a_ca43b813c00c', 'eventId_0b5f6ed7_a811_4fa4_bbb4_0a6754505334', 'eventId_15769083_76c2_40ac_9e20_a5bff5961b68', 'eventId_1624fa6a_ef14_4678_812e_77b20e02dd06', 'eventId_1814b51f_90ff_4527_b04d_4b9790d8d9f8', 'eventId_1a0ff74a_3e32_44c7_8b34_fa1386f5d7b0', 'eventId_1aaeb3f4_b3fd_4771_9eeb_bc0858a866c2', 'eventId_1b75b9e2_6102_44d4_b6ae_8435dfce5345', 'eventId_204a6791_1eec_41bb_b88d_bd48e5e9cabc', 'eventId_21f7892b_a65f_4974_b623_659817851950', 'eventId_22336d2e_a082_4fbc_96b8_8c0aabd4ecb1', 'eventId_2312ff8e_f53f_4b8f_986d_c506f59be23d', 'eventId_23131d8b_e22b_4600_8d40_5ca779a13e7d', 'eventId_2328d6b9_064b_4e7a_9183_1d947e3346a6', 'eventId_238ef783_5421_4d22_acea_9b8df5ea7f52', 'eventId_256bc41a_5140_47c6_8968_27111a8848c8', 'eventId_25915a33_ac1a_4361_ae55_884ae873f385', 'eventId_26682f49_6c4b_4b5d_9383_e9de80983e53', 'eventId_278726ec_1636_42ac_8b5f_bfacf2d8e80b', 'eventId_28fae62a_a4a0_45d4_b49d_7ac4f50fa6e6', 'eventId_29538154_97f2_46f5_afea_91f876598a20', 'eventId_29a85cbf_7bc4_4fcc_9be0_089fd466efbf', 'eventId_29d27775_beb4_49b8_88c2_e1f23122c664', 'eventId_2a29179b_7862_49a4_97a6_9bdf74ad44cb', 'eventId_2a476578_878c_4cb7_bbc6_b6ba9e4fb0d6', 'eventId_2cd90327_5b39_4a00_b848_9aeed61d935e', 'eventId_2cf3be42_71ec_4fe2_8553_266af14a66ad', 'eventId_2d7047f7_5e9d_4506_bebc_3f8b8047ef42', 'eventId_2db5e1dd_9061_454f_a9c6_b431f0cb4ea3', 'eventId_2e0613b7_012b_4d4f_8757_8354a214b918', 'eventId_2ecc72b8_a79f_45eb_96c4_fc2b69ed9523', 'eventId_34de6253_7f37_43fb_8e35_033f67536b68', 'eventId_38628e68_2016_4c69_bd62_90cfeeb7cb73', 'eventId_38af3f38_5c28_4893_8401_1917e1ac23e5', 'eventId_38bc8a7b_6fc4_4b63_91c8_87887c200311', 'eventId_395c027c_738e_4afa_8aaa_81f2bbeb43a4', 'eventId_3b0bc1d0_5f0f_4d3e_976a_8b125af18562', 'eventId_3c558138_cdf7_406b_b9c0_14417bd0737e', 'eventId_3c9da047_86b4_499f_a549_dd7df7bd767a', 'eventId_3d3216a3_3353_49e6_8c99_ae1b2a1ae344', 'eventId_41638de1_0c9f_4340_a311_d898e07a530a', 'eventId_42046a78_8f9c_4677_b0f1_ffeeb1ac6512', 'eventId_42b68027_4604_44c2_9c50_8501426fd70f', 'eventId_4426d3e6_4204_4a84_bff5_8160284b9b5c', 'eventId_45157eb3_ab99_49bf_9ca2_79b19ad154dc', 'eventId_45893b86_6e1b_4d7c_9a0d_9fe3e1ab9e76', 'eventId_472d6444_8e46_45ee_9981_3a81466d560a', 'eventId_48816058_c878_4929_b9b5_f340580fedd6', 'eventId_48988126_3475_48c4_be69_3a82c5031498', 'eventId_499b84d6_b16e_4aa6_9464_7ed89a6f4767', 'eventId_4a00f359_dcf7_46b6_a48e_09ac957be579', 'eventId_4a62f701_bf58_410d_a331_9f53537a5ce3', 'eventId_4aa8d86b_aa76_4ac4_b9e6_8f6be9d60bcc', 'eventId_4bce38f6_18d1_433d_894c_e082d5e9305e', 'eventId_4feffc47_907b_4356_80a3_5949182c1a0f', 'eventId_4ff94e21_960e_4b3f_84b5_4c71c22cfd85', 'eventId_51e85b18_1133_4757_86ad_66b6ebd60fc5', 'eventId_5399a059_a850_4ab5_8eb3_43334f955b20', 'eventId_54c7f2f2_620f_4491_98bc_29e122f55a59', 'eventId_554b7182_3ffa_4e5c_b288_44ef348d1758', 'eventId_55fa7328_6a92_4286_b8ea_fd76561d5e01', 'eventId_563c79fe_6084_468d_a29b_946200ec8b28', 'eventId_56cb5568_978c_483d_b2d6_cff12f8f8677', 'eventId_5a9e8cee_51c9_4704_979a_b28c8fad6505', 'eventId_5b621d02_edd2_4b45_aefe_83710dffcb57', 'eventId_621edf4e_1619_4908_8a41_eeb4bc4273c1', 'eventId_62e35b71_deb8_465d_8a94_4ab9e5e2faf5', 'eventId_6494823b_80f4_4be8_8ce9_97f6e8e3679c', 'eventId_68299a08_a0a9_4a6d_82c4_52dcf974c252', 'eventId_683529b4_7621_4bc8_be64_e2b45cd02446', 'eventId_6a034560_52bf_455f_850a_d51f3437ee92', 'eventId_6aa425a2_e1c7_4661_9b44_cc9c27ff5092', 'eventId_6bdc608a_cae5_46c0_876f_ac0a84d3b04b', 'eventId_6cb2f0b8_06b0_4b30_8bbe_6db6f608c2f1', 'eventId_6d9208d8_156a_41ce_b234_0a454eefdf97', 'eventId_6db72d58_f494_4aa7_a4c2_e7812dd0e5b7', 'eventId_6de2c760_438e_408e_addd_6eebcb6ddb5c', 'eventId_6f173c83_219a_4466_b8fc_07e60745147e', 'eventId_6f520b5c_da4b_4c37_9a8e_c6ba384cec60', 'eventId_710b919d_42b9_4de6_b55e_6e0cc73cd467', 'eventId_718aa84f_9d06_4f60_9630_a26d72a48942', 'eventId_73246a9a_c312_4ee7_87f9_5254d87ca2ed', 'eventId_73941d42_0e5b_4bf8_9be4_f1d2f8d51a9e', 'eventId_73f72e0a_6cba_4b25_bca3_4a8408b47fa9', 'eventId_751a23b5_61f9_40af_a14e_7721a336c5d5', 'eventId_759c7742_9232_41f6_b345_c1c7c31ffd44', 'eventId_779851c8_faf9_43fb_a756_1f517622b72b', 'eventId_7824908a_648d_47ac_a727_c97681a1fc9b', 'eventId_78b6fa75_b26b_4696_a085_9a69cfbbf29b', 'eventId_7d27fa68_fa03_4855_8be5_0ab3c325baf1', 'eventId_7ed4daa6_c207_48dd_888c_a25613e446e1', 'eventId_822b79cc_9c0e_4940_b011_da3ae0d91fd4', 'eventId_834fcafd_cabd_4c49_90ef_1069a0f108c6', 'eventId_85caf7c4_3130_4ffc_aa5b_25f1b1f72acc', 'eventId_869cab8e_7176_459d_bac1_b558fd6e4068', 'eventId_87164054_5a94_4753_9b91_33d4f812b29e', 'eventId_89618653_f82c_4a13_95c2_2bb6eff7fbc8', 'eventId_8990478a_3319_404b_b018_7aa0e313f5e8', 'eventId_8ac060d9_05d1_4c8c_83a7_dcde3332d0e9', 'eventId_8bae7f45_2222_42d2_8da4_b531f7cca2fa', 'eventId_8d34c508_d2f6_4a16_a8d9_c0cde1220fc4', 'eventId_9064bb4f_9475_409a_9eda_cf661c6439a9', 'eventId_91df8e07_c57c_429d_90d5_62b9b5daac19', 'eventId_92099762_9a35_4320_8349_0cf6acdb5caf', 'eventId_931a30a5_496a_46cc_aa49_2807abc117ca', 'eventId_939c3bad_7d1e_49f1_bb31_0a90b06ff243', 'eventId_94c0f433_aebe_41de_ab7d_919c3ae727e4', 'eventId_95dfa6b7_7b26_4024_859b_36e4c5231829', 'eventId_9ac8e7da_2bc9_4410_8964_6271fd16a46d', 'eventId_a07be9ed_7327_4b28_b5e6_996ff66d184d', 'eventId_a2734bdc_4cc2_49b1_ab86_68c1ce591364', 'eventId_a2c9dad7_5a50_4e4f_a2dc_42237b76b319', 'eventId_a3876bee_da16_482d_b2b4_2309afe14452', 'eventId_a3e03d9d_42bd_4e59_9da6_954c5b34fd69', 'eventId_a55b69c3_ff1e_4255_9eed_872652513ce6', 'eventId_a7d4a970_4637_426c_8d93_bb8f593f6ade', 'eventId_ab234b70_3277_4d0d_8112_b2e918304483', 'eventId_ab6c0d1f_d69e_42a6_8b30_65871ec645f1', 'eventId_ab7efc84_b516_43f8_942b_8498f8308d86', 'eventId_ad21fd9d_d39f_4b92_ab7c_e3d68f06fb3f', 'eventId_b0163bd0_99a8_4dc4_84fc_641447f4aab5', 'eventId_b0be9756_32e0_4821_b91a_b76c88222b6e', 'eventId_b0d21961_1c6c_4fda_a19f_9be241080cb2', 'eventId_b0f6929a_de66_4478_bbb4_17b6b015bee6', 'eventId_b1cfe75d_7a58_4684_91c6_b7498c117fce', 'eventId_b1e3fd0d_ecc3_4df6_87cd_da00444d4092', 'eventId_b2a7d23c_8ebf_43a6_a7ff_e521d908534a', 'eventId_b443fd5a_e621_42db_ad11_e1388cd57c14', 'eventId_b6173773_abde_4063_b336_59b0b5b3ec8c', 'eventId_b624188b_176d_424f_b211_64b335a279ed', 'eventId_b7530a1d_6ca7_4c46_829f_29761009fe88', 'eventId_b98904a7_8c7b_44be_883b_580eaef026b2', 'eventId_ba565ee1_9008_4c53_979f_b85e0702cf9d', 'eventId_bb003d4d_3831_4d3e_8657_b40cce44bd9b', 'eventId_bdfd7409_6271_4d13_aba0_aa7017937694', 'eventId_be21a8a0_3e66_4811_89c2_24fdff612037', 'eventId_c1e3d523_ede9_4ffd_b43a_2038727a3e13', 'eventId_c4610217_931e_48d4_b1bb_efd8458335a5', 'eventId_c75ef1d5_825f_40c2_990a_9e28bfac9588', 'eventId_c7961398_569e_4832_a60e_13091d442fe6', 'eventId_c7e56441_7c14_46e8_9ab2_3d592298c79e', 'eventId_cd951c48_14df_4b0b_a19e_0e0937b06a1c', 'eventId_d0f23eed_ba77_4ac6_98a8_46fcb38cb387', 'eventId_d3d02f25_911f_4977_b789_9b282150a8b7', 'eventId_d4e3370d_42c9_4224_9de3_dccf462d4e55', 'eventId_d5812b5f_62b1_4d4a_aaa8_9d96ebe48571', 'eventId_d5a62a3d_83a9_42cb_b55a_fbf42f3b9271', 'eventId_d6b9b42a_5100_4385_82ee_bc5f6b226e83', 'eventId_d8d4ce50_ef8a_4df2_b564_e451c85d3992', 'eventId_dd69c12b_65d3_49d9_8324_afcb3866b09a', 'eventId_dd71a5a2_1761_4c0b_9219_d66d08c3e527', 'eventId_dd7e1a2a_6a2c_4bff_804d_9fef5bd35a23', 'eventId_df8e5584_7af2_4f0a_a0c6_14ec42572f12', 'eventId_dff4a88a_8b84_4ae0_b739_244be0da84de', 'eventId_e0a15ccd_4df5_40e2_be72_a907cb1848c7', 'eventId_e2ff9316_15fd_4198_b78c_27bbe3082b5c', 'eventId_e38b9a86_9ffc_459f_8c2f_34d20b2ba505', 'eventId_e572033a_2e6a_45af_9f98_d3729af4e45b', 'eventId_e5ad0add_545e_41d9_86da_7b2c4317f3ef', 'eventId_e78e91ee_bd8c_4864_bb99_2c8c4c9af5ac', 'eventId_e80c7384_4e54_4182_bb61_167247f9c3d4', 'eventId_e9bd8905_4627_419d_87ef_f058936ec0ff', 'eventId_ea19614a_e933_4924_ac45_fed3f1696b98', 'eventId_ea8f6be9_0228_4b46_b8eb_e4ef221fd825', 'eventId_eab34c5c_8765_4e98_ba1a_e4f5ed7cf3d3', 'eventId_eaec7863_c7ba_4d26_8cd2_41453f62091e', 'eventId_ed9d9c72_29ed_4835_9903_1825ef2686f8', 'eventId_edd22ac0_85db_4f05_bd4b_7a123bce709f', 'eventId_eddf6ee2_c3cd_4a0d_8687_430eca327838', 'eventId_ee5857b6_f5cb_432b_8001_0438c16b63e8', 'eventId_f0c90717_3f10_4500_9c58_8f2fd5f4792f', 'eventId_f11f4c17_c6ce_4dae_80ec_70c95a2b7377', 'eventId_f294ce93_4c3a_4138_ba02_0a828c4c8514', 'eventId_f31b1df2_7795_4abc_8c7c_c733fc3dc50e', 'eventId_f3665b0e_dac1_40c1_8e7e_d17b764e9c1c', 'eventId_f418fadc_7699_474e_b268_576a9eb74be3', 'eventId_f5f1d510_7039_44af_9851_b21f40a3f395', 'eventId_f62acba0_cbff_4339_88a2_728d5d7d6d0b', 'eventId_f6926676_530e_4c78_aa41_71059a3cb791', 'eventId_f75cf9ae_5004_441f_8dec_b0bdec457ec7', 'eventId_f93d62f6_882e_4e68_b0c6_48ceb39c1424', 'eventId_fa0fb8af_d9eb_4d1b_822a_074c47000bbf', 'eventId_fd81df5d_9648_4e02_9c15_f31380c43d0b', 'eventId_fe3c6374_f16c_44bd_80a9_0d2e93a9e14e', 'eventId_fea17501_4f28_4818_8317_05895a334124', 'eventTimestamp_2023_07_14_16_13_38_996', 'eventTimestamp_2023_07_14_16_26_02_644', 'eventTimestamp_2023_07_17_15_24_23_769', 'eventTimestamp_2023_07_17_22_36_49_455', 'eventTimestamp_2023_07_17_22_49_34_675', 'eventTimestamp_2023_07_17_22_53_40_361', 'eventTimestamp_2023_07_17_22_58_40_092', 'eventTimestamp_2023_07_18_06_56_56_489', 'eventTimestamp_2023_07_18_07_19_55_559', 'eventTimestamp_2023_07_18_07_34_40_264', 'eventTimestamp_2023_07_18_07_47_33_107', 'eventTimestamp_2023_07_19_09_48_12_929', 'eventTimestamp_2023_07_19_13_49_31_281', 'eventTimestamp_2023_07_19_13_55_28_606', 'eventTimestamp_2023_07_19_14_04_55_098', 'eventTimestamp_2023_07_19_14_23_28_152', 'eventTimestamp_2023_07_19_15_18_43_698', 'eventTimestamp_2023_07_19_15_22_57_17', 'eventTimestamp_2023_07_19_15_46_35_166', 'eventTimestamp_2023_07_19_16_22_30_873', 'eventTimestamp_2023_07_20_11_27_31_28', 'eventTimestamp_2023_07_20_11_56_32_13', 'eventTimestamp_2023_07_20_12_16_20_135', 'eventTimestamp_2023_07_20_13_07_45_113', 'eventTimestamp_2023_07_25_14_20_47_714', 'eventTimestamp_2023_08_03_08_09_25_347', 'eventTimestamp_2023_08_03_08_10_16_76', 'eventTimestamp_2023_08_03_08_13_24_024', 'eventTimestamp_2023_08_03_08_14_14_376', 'eventTimestamp_2023_08_03_08_14_39_42', 'eventTimestamp_2023_08_03_08_15_04_05', 'eventTimestamp_2023_08_09_12_35_09_901', 'eventTimestamp_2023_08_14_11_03_55_697', 'eventTimestamp_2023_08_14_11_23_39_923', 'eventTimestamp_2023_08_14_11_26_47_271', 'eventTimestamp_2023_08_14_11_28_18_776', 'eventTimestamp_2023_08_14_11_28_43_187', 'eventTimestamp_2023_08_14_11_29_58_008', 'eventTimestamp_2023_08_14_11_30_21_922', 'eventTimestamp_2023_08_14_11_31_15_329', 'eventTimestamp_2023_08_18_06_56_12_275', 'eventTimestamp_2023_08_18_06_58_23_024', 'eventTimestamp_2023_08_18_07_11_26_635', 'eventTimestamp_2023_08_18_07_23_00_148', 'eventTimestamp_2023_08_18_09_36_31_474', 'eventTimestamp_2023_08_18_12_22_11_185', 'eventTimestamp_2023_08_18_12_25_19_698', 'eventTimestamp_2023_08_18_12_28_47_262', 'eventTimestamp_2023_08_18_12_32_04_095', 'eventTimestamp_2023_08_23_10_48_13_179', 'eventTimestamp_2023_08_23_10_55_35_591', 'eventTimestamp_2023_08_23_11_07_29_539', 'eventTimestamp_2023_08_24_11_17_48_932', 'eventTimestamp_2023_08_24_11_48_57_84', 'eventTimestamp_2023_09_06_10_12_57_563', 'eventTimestamp_2023_09_06_19_31_37_839', 'eventTimestamp_2023_09_06_21_40_38_054', 'eventTimestamp_2023_09_07_17_26_04_305', 'eventTimestamp_2023_09_07_17_29_08_842', 'eventTimestamp_2023_09_07_17_33_35_162', 'eventTimestamp_2023_09_07_17_37_19_358', 'eventTimestamp_2023_09_07_17_40_26_826', 'eventTimestamp_2023_09_07_17_53_49_975', 'eventTimestamp_2023_09_07_18_08_15_594', 'eventTimestamp_2023_09_07_18_21_06_747', 'eventTimestamp_2023_09_07_18_26_28_892', 'eventTimestamp_2023_09_07_18_27_58_59', 'eventTimestamp_2023_09_07_18_42_06_079', 'eventTimestamp_2023_09_11_06_05_54_485', 'eventTimestamp_2023_09_11_06_09_52_961', 'eventTimestamp_2023_09_11_06_17_43_501', 'eventTimestamp_2023_09_11_06_26_58_94', 'eventTimestamp_2023_09_11_06_37_45_538', 'eventTimestamp_2023_09_11_06_39_02_42', 'eventTimestamp_2023_09_11_06_51_50_261', 'eventTimestamp_2023_09_11_07_05_09_632', 'eventTimestamp_2023_09_11_07_19_52_238', 'eventTimestamp_2023_09_11_07_22_49_696', 'eventTimestamp_2023_09_11_07_23_56_963', 'eventTimestamp_2023_09_11_16_58_13_382', 'eventTimestamp_2023_09_11_17_01_56_818', 'eventTimestamp_2023_09_11_22_12_57_873', 'eventTimestamp_2023_09_11_22_16_48_395', 'eventTimestamp_2023_09_11_22_30_58_255', 'eventTimestamp_2023_09_12_04_01_31_97', 'eventTimestamp_2023_09_12_06_32_30_352', 'eventTimestamp_2023_09_12_06_34_24_539', 'eventTimestamp_2023_09_12_18_33_02_593', 'eventTimestamp_2023_09_12_22_41_47_136', 'eventTimestamp_2023_09_12_22_45_40_437', 'eventTimestamp_2023_09_12_22_57_03_696', 'eventTimestamp_2023_09_12_22_59_35_848', 'eventTimestamp_2023_09_12_23_00_52_295', 'eventTimestamp_2023_09_12_23_02_38_945', 'eventTimestamp_2023_09_12_23_03_43_731', 'eventTimestamp_2023_11_29_18_33_19_147', 'eventTimestamp_2023_11_29_18_53_37_279', 'eventTimestamp_2024_02_02_06_35_05_173', 'eventTimestamp_2024_02_02_07_22_40_795', 'eventTimestamp_2024_02_02_07_34_33_106', 'eventTimestamp_2024_02_02_07_40_08_352', 'eventTimestamp_2024_02_02_07_43_49_132', 'eventTimestamp_2024_02_02_08_03_08_336', 'eventTimestamp_2024_02_02_08_08_09_687', 'eventTimestamp_2024_02_02_08_13_34_701', 'eventTimestamp_2024_02_02_09_19_02_675', 'eventTimestamp_2024_02_02_09_28_59_782', 'eventTimestamp_2024_02_02_09_46_11_292', 'eventTimestamp_2024_02_02_10_14_38_355', 'eventTimestamp_2024_02_02_12_47_43_136', 'eventTimestamp_2024_02_02_14_19_14_092', 'eventTimestamp_2024_02_02_14_22_25_749', 'eventTimestamp_2024_02_02_15_13_52_752', 'eventTimestamp_2024_02_02_15_32_28_474', 'eventTimestamp_2024_02_02_15_35_41_939', 'eventTimestamp_2024_02_02_15_45_41_881', 'eventTimestamp_2024_02_02_16_54_46_654', 'eventTimestamp_2024_02_02_17_03_16_319', 'eventTimestamp_2024_02_02_17_19_28_355', 'eventTimestamp_2024_02_05_06_51_05_376', 'eventTimestamp_2024_02_05_07_00_59_14', 'eventTimestamp_2024_02_05_10_57_41_903', 'eventTimestamp_2024_02_05_15_27_04_337', 'eventTimestamp_2024_02_05_15_41_31_261', 'eventTimestamp_2024_02_05_15_57_36_297', 'eventTimestamp_2024_02_05_16_07_09_094', 'eventTimestamp_2024_02_06_14_53_33_468', 'eventTimestamp_2024_02_06_15_01_14_613', 'eventTimestamp_2024_02_06_15_26_33_469', 'eventTimestamp_2024_02_06_16_19_59_034', 'eventTimestamp_2024_02_08_03_31_43_995', 'eventTimestamp_2024_02_08_18_45_17_349', 'eventTimestamp_2024_02_08_19_57_08_325', 'eventTimestamp_2024_02_08_20_16_47_42', 'eventTimestamp_2024_02_12_08_56_50_489', 'eventTimestamp_2024_02_12_09_06_39_922', 'eventTimestamp_2024_02_12_09_15_59_993', 'eventTimestamp_2024_02_12_10_07_40_943', 'eventTimestamp_2024_02_12_10_12_33_458', 'eventTimestamp_2024_02_15_07_44_43_343', 'eventTimestamp_2024_02_15_07_50_39_797', 'eventTimestamp_2024_02_15_11_03_39_914', 'eventTimestamp_2024_02_20_06_37_14_104', 'eventTimestamp_2024_02_20_06_41_17_242', 'eventTimestamp_2024_02_20_06_48_02_567', 'eventTimestamp_2024_02_20_06_52_03_958', 'eventTimestamp_2024_02_20_06_56_08_324', 'eventTimestamp_2024_02_20_07_17_01_603', 'eventTimestamp_2024_02_20_07_37_15_988', 'eventTimestamp_2024_02_20_08_54_28_727', 'eventTimestamp_2024_02_20_09_08_09_292', 'eventTimestamp_2024_02_20_09_16_18_313', 'eventTimestamp_2024_02_20_09_34_13_662', 'eventTimestamp_2024_02_20_09_39_15_702', 'eventTimestamp_2024_02_20_09_46_14_059', 'eventTimestamp_2024_02_20_09_50_17_105', 'eventTimestamp_2024_02_20_10_18_15_757', 'eventTimestamp_2024_02_20_10_24_31_563', 'eventTimestamp_2024_02_20_10_29_13_18', 'eventTimestamp_2024_02_20_10_33_33_863', 'eventTimestamp_2024_02_20_10_36_12_151', 'eventTimestamp_2024_02_20_12_31_28_101', 'eventTimestamp_2024_02_20_12_35_43_217', 'eventTimestamp_2024_02_20_12_42_03_902', 'eventTimestamp_2024_02_20_14_22_55_909', 'eventTimestamp_2024_02_20_14_51_38_41', 'eventTimestamp_2024_02_20_14_55_24_663', 'eventTimestamp_2024_02_20_17_37_17_716', 'eventTimestamp_2024_02_20_17_46_16_306', 'eventTimestamp_2024_02_20_17_53_21_577', 'eventTimestamp_2024_02_20_17_55_05_229', 'eventTimestamp_2024_02_21_20_41_44_026', 'eventTimestamp_2024_02_21_20_55_54_884', 'eventTimestamp_2024_02_27_10_03_11_898', 'eventTimestamp_2024_02_29_14_47_02_541', 'eventTimestamp_2024_03_05_11_17_50_121', 'eventTimestamp_2024_03_05_12_44_50_566', 'eventTimestamp_2024_03_05_13_57_35_465', 'eventTimestamp_2024_03_05_14_00_33_206', 'eventTimestamp_2024_03_05_14_21_27_539', 'eventTimestamp_2024_03_05_15_13_29_959', 'eventTimestamp_2024_03_06_16_19_33_357', 'eventTimestamp_2024_03_06_17_28_33_37', 'eventTimestamp_2024_03_06_17_38_13_579', 'eventTimestamp_2024_03_06_17_43_19_663', 'eventTimestamp_2024_03_08_08_13_59_104', 'eventTimestamp_2024_03_08_08_17_56_407', 'eventTimestamp_2024_03_11_08_45_00_018', 'eventTimestamp_2024_03_11_09_03_45_668', 'eventTimestamp_2024_03_11_09_24_45_148', 'eventTimestamp_2024_03_11_09_34_24_625', 'eventTimestamp_2024_03_11_09_40_13_578', 'eventTimestamp_2024_03_12_06_41_31_825', 'eventTimestamp_2024_03_15_19_40_15_847', 'wtn_1000246730', 'wtn_1000247195', 'wtn_1000302712', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100000138', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500022989', 'wtn_1500025020', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500074137', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2125264357', 'serialNumber_C4000XG2128282175', 'serialNumber_C4000XG2134319195', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2222000023', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231010781', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2219005115', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000LZ', 'productClass_C4000XG', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job b9a80c04-b0f4-4504-932b-b2fbdf35b796 is DONE. 57.7 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:b9a80c04-b0f4-4504-932b-b2fbdf35b796&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 23:25:47,848 - terabyte_feature_extractor - ERROR - Imputer and scaler must be fitted on training data first\n",
      "2025-03-05 23:25:47,849 - terabyte_feature_extractor - ERROR - Failed to process validation data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction failed\n"
     ]
    }
   ],
   "source": [
    "# Set the table ID\n",
    "TABLE_ID = \"TEST1.ctl_modem_speedtest_event\"  # Replace with your table ID\n",
    "\n",
    "# Set the target column (optional)\n",
    "TARGET_COLUMN = 'downloadLatency'  # Replace with your target column if needed\n",
    "\n",
    "# Set a limit for testing (remove for full dataset)\n",
    "LIMIT = 10000\n",
    "\n",
    "# Import the BigFrames-only version of the feature extractor\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the ember_ml directory to the Python path if needed\n",
    "if not any(p.endswith('ember_ml') for p in sys.path):\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd())))\n",
    "\n",
    "# Import ember_ml instead of NumPy\n",
    "import ember_ml as eh\n",
    "from ember_ml import ops\n",
    "from ember_ml.backend import get_backend\n",
    "\n",
    "# Import BigFrames\n",
    "import bigframes.pandas as bf\n",
    "\n",
    "# Print the current backend\n",
    "current_backend = get_backend()\n",
    "print(f\"Using {current_backend} backend\")\n",
    "\n",
    "# Import the BigFrames-only version of the feature extractor\n",
    "from ember_ml.nn.features.terabyte_feature_extractor_bigframes import TerabyteFeatureExtractor, TerabyteTemporalStrideProcessor\n",
    "\n",
    "# Initialize the feature extractor\n",
    "feature_extractor = TerabyteFeatureExtractor(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    chunk_size=100000,\n",
    "    max_memory_gb=16.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Set up BigQuery connection\n",
    "feature_extractor.setup_bigquery_connection(CREDENTIALS_PATH)\n",
    "\n",
    "# Extract features\n",
    "result = feature_extractor.prepare_data(\n",
    "    table_id=TABLE_ID,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    limit=LIMIT,\n",
    "    force_categorical_columns=[\n",
    "        \"eventType\", \"eventSource\", \"eventCategory\", \"eventPublisherId\",\n",
    "        \"productClass\", \"downloadTestStatus\", \"uploadState\", \"uploadTestStatus\",\n",
    "        \"wtn\", \"serialNumber\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if result is not None:\n",
    "    train_df, val_df, test_df, train_features, val_features, test_features, scaler, imputer = result\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Validation shape: {val_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(f\"Features: {train_features}\")\n",
    "    \n",
    "    # Now we can use these DataFrames directly with BigFrames operations\n",
    "    # For example, to get the first few rows of the training data:\n",
    "    print(\"\\nFirst few rows of training data:\")\n",
    "    print(train_df.head())\n",
    "    \n",
    "    # Or to get summary statistics:\n",
    "    print(\"\\nSummary statistics for training data:\")\n",
    "    print(train_df[train_features].describe())\n",
    "    \n",
    "    # Define a function to convert BigFrames to ember_ml tensors\n",
    "    def bigframes_to_tensor(bf_df, columns):\n",
    "        \"\"\"\n",
    "        Convert BigFrames DataFrame to ember_ml tensor.\n",
    "        \n",
    "        Args:\n",
    "            bf_df: BigFrames DataFrame\n",
    "            columns: Columns to include\n",
    "            \n",
    "        Returns:\n",
    "            ember_ml tensor\n",
    "        \"\"\"\n",
    "        from ember_ml.nn import tensor\n",
    "        # We need to convert to numpy array first, then to ember_ml tensor\n",
    "        # This is a temporary step until BigFrames supports direct conversion\n",
    "        array_data = bf_df[columns].to_numpy()\n",
    "        return tensor.convert_to_tensor(array_data)\n",
    "    \n",
    "    # Convert BigFrames DataFrames directly to ember_ml tensors\n",
    "    print(\"\\nConverting BigFrames DataFrames to ember_ml tensors...\")\n",
    "    train_tensor = bigframes_to_tensor(train_df, train_features)\n",
    "    val_tensor = bigframes_to_tensor(val_df, val_features)\n",
    "    test_tensor = bigframes_to_tensor(test_df, test_features)\n",
    "    \n",
    "    print(\"\\nConverted to ember_ml tensors for GPU acceleration\")\n",
    "    print(f\"Train tensor shape: {ops.shape(train_tensor)}\")\n",
    "    print(f\"Validation tensor shape: {ops.shape(val_tensor)}\")\n",
    "    print(f\"Test tensor shape: {ops.shape(test_tensor)}\")\n",
    "    \n",
    "    # Perform some basic operations with ember_ml ops\n",
    "    print(\"\\nBasic statistics using ember_ml ops:\")\n",
    "    print(f\"Mean of train features: {ops.stats.mean(train_tensor, axis=0)[:5]}...\")  # Show first 5 means\n",
    "    print(f\"Standard deviation of train features: {ops.sqrt(ops.stats.mean(ops.square(train_tensor - ops.stats.mean(train_tensor, axis=0)), axis=0))[:5]}...\")  # Show first 5 stds\n",
    "    print(f\"Min of train features: {stats.min(train_tensor, axis=0)[:5]}...\")  # Show first 5 mins\n",
    "    print(f\"Max of train features: {stats.max(train_tensor, axis=0)[:5]}...\")  # Show first 5 maxs\n",
    "else:\n",
    "    print(\"Feature extraction failed\")\n",
    "    # Create empty variables to avoid NameError in subsequent cells\n",
    "    import bigframes.pandas as bf\n",
    "    train_df = bf.DataFrame()\n",
    "    val_df = bf.DataFrame()\n",
    "    test_df = bf.DataFrame()\n",
    "    train_features = []\n",
    "    val_features = []\n",
    "    test_features = []\n",
    "    train_tensor = ops.zeros((0, 0))\n",
    "    val_tensor = ops.zeros((0, 0))\n",
    "    test_tensor = ops.zeros((0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Temporal Stride Processing\n",
    "\n",
    "Now let's apply temporal stride processing to the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ember_ml instead of NumPy\n",
    "import ember_ml as eh\n",
    "from ember_ml import ops\n",
    "from ember_ml.backend import get_backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import BigFrames\n",
    "import bigframes.pandas as bf\n",
    "\n",
    "# Print the current backend\n",
    "current_backend = get_backend()\n",
    "print(f\"Using {current_backend} backend\")\n",
    "\n",
    "# Create temporal processor\n",
    "temporal_processor = TerabyteTemporalStrideProcessor(\n",
    "    window_size=10,\n",
    "    stride_perspectives=[1, 3, 5],\n",
    "    pca_components=32,\n",
    "    batch_size=10000,\n",
    "    use_incremental_pca=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define a function to convert BigFrames to ember_ml tensors\n",
    "def bigframes_to_tensor(bf_df, columns):\n",
    "    \"\"\"\n",
    "    Convert BigFrames DataFrame to ember_ml tensor.\n",
    "    \n",
    "    Args:\n",
    "        bf_df: BigFrames DataFrame\n",
    "        columns: Columns to include\n",
    "        \n",
    "    Returns:\n",
    "        ember_ml tensor\n",
    "    \"\"\"\n",
    "    from ember_ml.nn import tensor\n",
    "    # We need to convert to numpy array first, then to ember_ml tensor\n",
    "    # This is a temporary step until BigFrames supports direct conversion\n",
    "    array_data = bf_df[columns].to_numpy()\n",
    "    return tensor.convert_to_tensor(array_data)\n",
    "\n",
    "# Define a generator to yield data in batches\n",
    "def data_generator(df, features, batch_size=10000):\n",
    "    # Convert BigFrames DataFrame to ember_ml tensors in batches\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Get a batch of data\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Convert directly to ember_ml tensor using our helper function\n",
    "        yield bigframes_to_tensor(batch, features)\n",
    "\n",
    "# Process data - make sure train_df and train_features are defined\n",
    "if len(train_df) > 0 and len(train_features) > 0:\n",
    "    # Process the data through the temporal stride processor\n",
    "    stride_perspectives = temporal_processor.process_large_dataset(\n",
    "        data_generator(train_df, train_features, batch_size=10000)\n",
    "    )\n",
    "    \n",
    "    # Print stride perspective shapes\n",
    "    for stride, data in stride_perspectives.items():\n",
    "        # Use ops.shape instead of .shape\n",
    "        print(f\"Stride {stride}: shape {ops.shape(data)}\")\n",
    "    \n",
    "    # Visualize explained variance for each stride\n",
    "    explained_variances = [temporal_processor.get_explained_variance(stride) for stride in stride_perspectives.keys()]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(stride_perspectives.keys(), explained_variances)\n",
    "    plt.xlabel('Stride Length')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Stride Length')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize feature importance for the first stride\n",
    "    first_stride = list(stride_perspectives.keys())[0]\n",
    "    feature_importance = temporal_processor.get_feature_importance(first_stride)\n",
    "    \n",
    "    if feature_importance is not None:\n",
    "        # Reshape to match the original feature dimensions\n",
    "        window_size = temporal_processor.window_size\n",
    "        feature_dim = len(train_features)\n",
    "        \n",
    "        # Use ops.reshape instead of .reshape\n",
    "        reshaped_importance = ops.reshape(feature_importance, (window_size, feature_dim))\n",
    "        \n",
    "        # Convert to numpy for matplotlib\n",
    "        reshaped_importance_np = ops.to_numpy(reshaped_importance)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(reshaped_importance_np, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar(label='Feature Importance')\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Time Step')\n",
    "        plt.title(f'Feature Importance Across Time Steps (Stride {first_stride})')\n",
    "        \n",
    "        # Add feature names on x-axis if not too many\n",
    "        if len(train_features) <= 20:\n",
    "            plt.xticks(range(len(train_features)), train_features, rotation=90)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Apply temporal processing to create multi-stride features\n",
    "    print(\"\\nApplying temporal processing to create multi-stride features...\")\n",
    "    \n",
    "    # Convert train_df directly to ember_ml tensor\n",
    "    train_tensor = bigframes_to_tensor(train_df, train_features)\n",
    "    \n",
    "    # Process through each stride perspective\n",
    "    multi_stride_features = {}\n",
    "    for stride, pca in temporal_processor.pca_models.items():\n",
    "        # Create windowed data\n",
    "        windows = temporal_processor.create_windows(train_tensor, stride)\n",
    "        \n",
    "        # Flatten windows\n",
    "        batch_size = ops.shape(windows)[0]\n",
    "        flattened = ops.reshape(windows, (batch_size, -1))\n",
    "        \n",
    "        # Transform with PCA - use ember_ml ops for PCA if possible\n",
    "        if hasattr(eh, 'pca'):\n",
    "            # If ember_ml has PCA implementation\n",
    "            transformed = eh.pca.transform(flattened, n_components=pca.n_components_)\n",
    "        else:\n",
    "            from ember_ml.nn import tensor\n",
    "            # Fallback to scikit-learn PCA which requires numpy\n",
    "            transformed = pca.transform(ops.to_numpy(flattened))\n",
    "            transformed = tensor.convert_to_tensor(transformed)\n",
    "        \n",
    "        # Store the transformed features\n",
    "        multi_stride_features[stride] = transformed\n",
    "        \n",
    "        print(f\"Stride {stride} features shape: {ops.shape(multi_stride_features[stride])}\")\n",
    "    \n",
    "    # Demonstrate how to combine multi-stride features\n",
    "    print(\"\\nCombining multi-stride features...\")\n",
    "    \n",
    "    # Get a list of all stride features\n",
    "    stride_features_list = [multi_stride_features[stride] for stride in sorted(multi_stride_features.keys())]\n",
    "    \n",
    "    # Concatenate along feature dimension (axis 1)\n",
    "    combined_features = ops.concatenate(stride_features_list, axis=1)\n",
    "    \n",
    "    print(f\"Combined multi-stride features shape: {ops.shape(combined_features)}\")\n",
    "    \n",
    "    # Calculate correlation between strides using ember_ml ops\n",
    "    print(\"\\nCalculating correlation between stride features...\")\n",
    "    \n",
    "    # Calculate correlation matrix using ember_ml ops\n",
    "    centered_features = combined_features - ops.stats.mean(combined_features, axis=0)\n",
    "    corr_matrix = ops.matmul(\n",
    "        ops.transpose(centered_features),\n",
    "        centered_features\n",
    "    ) / ops.shape(combined_features)[0]\n",
    "    \n",
    "    # Convert to numpy only for visualization with matplotlib\n",
    "    corr_matrix_np = ops.to_numpy(corr_matrix)\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(corr_matrix_np, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('Multi-stride Feature Correlations')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMulti-stride temporal processing complete!\")\n",
    "else:\n",
    "    print(\"Cannot process data: train_df or train_features is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Restricted Boltzmann Machine and Create Liquid Neural Network\n",
    "\n",
    "Now let's train an RBM on the extracted features, feed them into a liquid neural network, and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import ember_ml instead of TensorFlow and NumPy\n",
    "import ember_ml as eh\n",
    "from ember_ml import ops\n",
    "from ember_ml import nn\n",
    "from ember_ml.backend import get_backend\n",
    "\n",
    "# Import BigFrames\n",
    "import bigframes.pandas as bf\n",
    "\n",
    "# Import the RBM and liquid network components\n",
    "from ember_ml.models.optimized_rbm import OptimizedRBM\n",
    "from ember_ml.models.stride_aware_cfc import (\n",
    "    create_liquid_network_with_motor_neuron,\n",
    "    create_lstm_gated_liquid_network,\n",
    "    create_multi_stride_liquid_network\n",
    ")\n",
    "\n",
    "# Print the current backend\n",
    "current_backend = get_backend()\n",
    "print(f\"Using {current_backend} backend\")\n",
    "\n",
    "# Define a function to convert BigFrames to ember_ml tensors\n",
    "def bigframes_to_tensor(bf_df, columns):\n",
    "    \"\"\"\n",
    "    Convert BigFrames DataFrame to ember_ml tensor.\n",
    "    \n",
    "    Args:\n",
    "        bf_df: BigFrames DataFrame\n",
    "        columns: Columns to include\n",
    "        \n",
    "    Returns:\n",
    "        ember_ml tensor\n",
    "    \"\"\"\n",
    "    # We need to convert to numpy array first, then to ember_ml tensor\n",
    "    # This is a temporary step until BigFrames supports direct conversion\n",
    "    array_data = bf_df[columns].to_numpy()\n",
    "    return tensor.convert_to_tensor(array_data)\n",
    "\n",
    "# Create RBM\n",
    "if len(train_features) > 0:\n",
    "    # Initialize RBM\n",
    "    rbm = OptimizedRBM(\n",
    "        n_visible=len(train_features),\n",
    "        n_hidden=64,\n",
    "        learning_rate=0.01,\n",
    "        momentum=0.5,\n",
    "        weight_decay=0.0001,\n",
    "        batch_size=100,\n",
    "        use_binary_states=False,\n",
    "        use_gpu=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Define a generator to yield data in batches directly from BigFrames\n",
    "    def rbm_data_generator(bf_df, features, batch_size=100):\n",
    "        # Get total size\n",
    "        total_size = len(bf_df)\n",
    "        \n",
    "        # Create random indices for shuffling\n",
    "        random_values = ops.random_uniform((total_size,))\n",
    "        indices = ops.to_numpy(ops.argsort(random_values))\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            end_idx = min(i + batch_size, total_size)\n",
    "            batch_indices = indices[i:end_idx]\n",
    "            \n",
    "            # Get batch from BigFrames DataFrame\n",
    "            batch = bf_df.iloc[batch_indices]\n",
    "            \n",
    "            # Convert directly to ember_ml tensor\n",
    "            yield bigframes_to_tensor(batch, features)\n",
    "    \n",
    "    # Train RBM\n",
    "    training_errors = rbm.train_in_chunks(\n",
    "        rbm_data_generator(train_df, train_features, batch_size=100),\n",
    "        epochs=10,\n",
    "        k=1\n",
    "    )\n",
    "    \n",
    "    # Plot training errors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ops.to_numpy(training_errors))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title('RBM Training Error')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract RBM features\n",
    "    def feature_generator(bf_df, features, batch_size=1000):\n",
    "        # Get total size\n",
    "        total_size = len(bf_df)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            end_idx = min(i + batch_size, total_size)\n",
    "            \n",
    "            # Get batch from BigFrames DataFrame\n",
    "            batch = bf_df.iloc[i:end_idx]\n",
    "            \n",
    "            # Convert directly to ember_ml tensor\n",
    "            yield bigframes_to_tensor(batch, features)\n",
    "    \n",
    "    # Extract features from RBM\n",
    "    train_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(train_df, train_features, batch_size=1000)\n",
    "    )\n",
    "    \n",
    "    val_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(val_df, val_features, batch_size=1000)\n",
    "    )\n",
    "    \n",
    "    test_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(test_df, test_features, batch_size=1000)\n",
    "    )\n",
    "    \n",
    "    # Convert to ember_ml tensors if they aren't already\n",
    "    train_rbm_features = tensor.convert_to_tensor(train_rbm_features)\n",
    "    val_rbm_features = tensor.convert_to_tensor(val_rbm_features)\n",
    "    test_rbm_features = tensor.convert_to_tensor(test_rbm_features)\n",
    "    \n",
    "    print(f\"Train RBM features shape: {ops.shape(train_rbm_features)}\")\n",
    "    print(f\"Validation RBM features shape: {ops.shape(val_rbm_features)}\")\n",
    "    print(f\"Test RBM features shape: {ops.shape(test_rbm_features)}\")\n",
    "    \n",
    "    # Visualize RBM feature distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot histograms for first 16 RBM features\n",
    "    for i in range(min(16, ops.shape(train_rbm_features)[1])):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        # Convert to numpy for matplotlib\n",
    "        feature_np = ops.to_numpy(train_rbm_features[:, i])\n",
    "        plt.hist(feature_np, bins=30, alpha=0.7)\n",
    "        plt.title(f'Feature {i+1}')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle('RBM Feature Distributions', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize feature correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Use ember_ml ops for correlation\n",
    "    centered_features = train_rbm_features - ops.stats.mean(train_rbm_features, axis=0)\n",
    "    corr_matrix = ops.matmul(\n",
    "        ops.transpose(centered_features),\n",
    "        centered_features\n",
    "    ) / ops.shape(train_rbm_features)[0]\n",
    "    \n",
    "    # Convert to numpy only for visualization with matplotlib\n",
    "    corr_matrix_np = ops.to_numpy(corr_matrix)\n",
    "    \n",
    "    plt.imshow(corr_matrix_np, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('RBM Feature Correlations')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create dummy targets for demonstration\n",
    "    # In a real application, you would use actual targets from your data\n",
    "    train_targets = ops.random_uniform((ops.shape(train_rbm_features)[0], 1))\n",
    "    val_targets = ops.random_uniform((ops.shape(val_rbm_features)[0], 1))\n",
    "    test_targets = ops.random_uniform((ops.shape(test_rbm_features)[0], 1))\n",
    "    \n",
    "    # Reshape RBM features for sequence input\n",
    "    train_rbm_seq = ops.reshape(train_rbm_features, \n",
    "                               (ops.shape(train_rbm_features)[0], 1, ops.shape(train_rbm_features)[1]))\n",
    "    val_rbm_seq = ops.reshape(val_rbm_features,\n",
    "                             (ops.shape(val_rbm_features)[0], 1, ops.shape(val_rbm_features)[1]))\n",
    "    test_rbm_seq = ops.reshape(test_rbm_features,\n",
    "                              (ops.shape(test_rbm_features)[0], 1, ops.shape(test_rbm_features)[1]))\n",
    "    \n",
    "    # Create liquid neural network using ember_ml's built-in components\n",
    "    liquid_network = create_liquid_network_with_motor_neuron(\n",
    "        input_dim=ops.shape(train_rbm_features)[1],\n",
    "        units=128,\n",
    "        output_dim=1,\n",
    "        sparsity_level=0.5,\n",
    "        stride_length=1,\n",
    "        time_scale_factor=1.0,\n",
    "        threshold=0.5,\n",
    "        adaptive_threshold=True,\n",
    "        mixed_memory=True\n",
    "    )\n",
    "    \n",
    "    # Set up callbacks using ember_ml's callback system\n",
    "    callbacks = [\n",
    "        # Early stopping\n",
    "        eh.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        eh.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        \n",
    "        # Logging\n",
    "        eh.callbacks.ModelCheckpoint(\n",
    "            filepath='./models/liquid_network_checkpoint',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train liquid network using ember_ml's training API\n",
    "    history = liquid_network.fit(\n",
    "        train_rbm_seq,\n",
    "        train_targets,\n",
    "        validation_data=(val_rbm_seq, val_targets),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['mae'], label='Train')\n",
    "    plt.plot(history['val_mae'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Process test data\n",
    "    outputs = liquid_network.predict(test_rbm_seq)\n",
    "    \n",
    "    # Extract motor neuron outputs and trigger signals\n",
    "    if isinstance(outputs, list):\n",
    "        motor_outputs = outputs[0]\n",
    "        trigger_signals = outputs[1][0]  # First element is trigger\n",
    "        threshold_values = outputs[1][1]  # Second element is threshold\n",
    "    else:\n",
    "        motor_outputs = outputs\n",
    "        trigger_signals = ops.cast(motor_outputs > 0.5, ops.float32)\n",
    "        threshold_values = ops.full_like(motor_outputs, 0.5)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Motor neuron output range: {stats.min(motor_outputs):.4f} to {stats.max(motor_outputs):.4f}\")\n",
    "    print(f\"Trigger rate: {ops.stats.mean(trigger_signals):.4f}\")\n",
    "    \n",
    "    # Plot motor neuron outputs and triggers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(ops.to_numpy(motor_outputs[:100]), label='Motor Neuron Output')\n",
    "    plt.plot(ops.to_numpy(threshold_values[:100]), 'r--', label='Threshold')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Output Value')\n",
    "    plt.title('Motor Neuron Output and Threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(ops.to_numpy(trigger_signals[:100]), 'g', label='Trigger Signal')\n",
    "    plt.axhline(y=ops.to_numpy(ops.stats.mean(trigger_signals)), color='r', linestyle='--', \n",
    "               label=f'Trigger Rate: {ops.to_numpy(ops.stats.mean(trigger_signals)):.2f}')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Trigger (0/1)')\n",
    "    plt.title('Exploration Trigger Signals')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze triggered samples\n",
    "    triggered_indices = ops.where(trigger_signals == 1)\n",
    "    non_triggered_indices = ops.where(trigger_signals == 0)\n",
    "    \n",
    "    print(f\"Number of triggered samples: {ops.shape(triggered_indices)[0]}\")\n",
    "    print(f\"Number of non-triggered samples: {ops.shape(non_triggered_indices)[0]}\")\n",
    "    \n",
    "    # Compare RBM features for triggered vs. non-triggered samples\n",
    "    if ops.shape(triggered_indices)[0] > 0 and ops.shape(non_triggered_indices)[0] > 0:\n",
    "        # Calculate mean features\n",
    "        triggered_mean = ops.stats.mean(ops.gather(test_rbm_features, triggered_indices), axis=0)\n",
    "        non_triggered_mean = ops.stats.means.means.mean(ops.gather(test_rbm_features, non_triggered_indices), axis=0)\n",
    "        \n",
    "        # Calculate feature difference\n",
    "        feature_diff = ops.subtract(triggered_mean, non_triggered_mean)\n",
    "        \n",
    "        # Plot feature difference\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(range(ops.shape(feature_diff)[0]), ops.to_numpy(feature_diff))\n",
    "        plt.xlabel('RBM Feature')\n",
    "        plt.ylabel('Difference (Triggered - Non-triggered)')\n",
    "        plt.title('Feature Difference Between Triggered and Non-triggered Samples')\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot feature distributions for top 3 differentiating features\n",
    "        top_features = ops.to_numpy(ops.argsort(ops.abs(feature_diff)))[-3:]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, feature_idx in enumerate(top_features):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            \n",
    "            # Get feature values for triggered and non-triggered samples\n",
    "            triggered_features = ops.to_numpy(ops.gather(\n",
    "                ops.gather(test_rbm_features, triggered_indices),\n",
    "                [feature_idx], axis=1\n",
    "            ))\n",
    "            \n",
    "            non_triggered_features = ops.to_numpy(ops.gather(\n",
    "                ops.gather(test_rbm_features, non_triggered_indices),\n",
    "                [feature_idx], axis=1\n",
    "            ))\n",
    "            \n",
    "            plt.hist(triggered_features, bins=20, alpha=0.5, label='Triggered')\n",
    "            plt.hist(non_triggered_features, bins=20, alpha=0.5, label='Non-triggered')\n",
    "            plt.xlabel(f'Feature {feature_idx}')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title(f'Feature {feature_idx} Distribution')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Save models\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    # Save RBM\n",
    "    rbm.save('./models/rbm.npy')\n",
    "    print(\"RBM saved to ./models/rbm.npy\")\n",
    "    \n",
    "    # Save liquid network\n",
    "    liquid_network.save('./models/liquid_network')\n",
    "    print(\"Liquid network saved to ./models/liquid_network\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot train RBM: train_features is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models\n",
    "\n",
    "Let's save the trained models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "#\n",
    "# Let's save the trained models for future use.\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Create directory for models\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "# Save RBM if it exists\n",
    "if 'rbm' in globals():\n",
    "    rbm.save('./models/rbm.npy')\n",
    "    print(\"RBM saved to ./models/rbm.npy\")\n",
    "    \n",
    "    # Save RBM configuration\n",
    "    rbm_config = {\n",
    "        'n_visible': rbm.n_visible,\n",
    "        'n_hidden': rbm.n_hidden,\n",
    "        'learning_rate': rbm.learning_rate,\n",
    "        'momentum': rbm.momentum,\n",
    "        'weight_decay': rbm.weight_decay,\n",
    "        'batch_size': rbm.batch_size,\n",
    "        'use_binary_states': rbm.use_binary_states\n",
    "    }\n",
    "    \n",
    "    with open('./models/rbm_config.json', 'w') as f:\n",
    "        json.dump(rbm_config, f, indent=2)\n",
    "    \n",
    "    print(\"RBM configuration saved to ./models/rbm_config.json\")\n",
    "else:\n",
    "    print(\"Cannot save RBM: not trained\")\n",
    "\n",
    "# Save liquid network if it exists\n",
    "if 'liquid_network' in globals():\n",
    "    liquid_network.save('./models/liquid_network')\n",
    "    print(\"Liquid network saved to ./models/liquid_network\")\n",
    "    \n",
    "    # Save liquid network configuration\n",
    "    try:\n",
    "        liquid_config = {\n",
    "            'input_dim': liquid_network.input_shape[-1],\n",
    "            'units': liquid_network.layers[1].units if hasattr(liquid_network.layers[1], 'units') else None,\n",
    "            'output_dim': liquid_network.output_shape[-1],\n",
    "            'sparsity_level': 0.5,  # Default value, might not be accurate\n",
    "            'threshold': 0.5  # Default value, might not be accurate\n",
    "        }\n",
    "        \n",
    "        with open('./models/liquid_network_config.json', 'w') as f:\n",
    "            json.dump(liquid_config, f, indent=2)\n",
    "        \n",
    "        print(\"Liquid network configuration saved to ./models/liquid_network_config.json\")\n",
    "    except:\n",
    "        print(\"Could not save liquid network configuration\")\n",
    "else:\n",
    "    print(\"Cannot save liquid network: not trained\")\n",
    "\n",
    "# Save feature extractor configuration\n",
    "if 'feature_extractor' in globals():\n",
    "    feature_config = {\n",
    "        'project_id': feature_extractor.project_id,\n",
    "        'location': feature_extractor.location,\n",
    "        'chunk_size': feature_extractor.chunk_size,\n",
    "        'max_memory_gb': feature_extractor.max_memory_gb\n",
    "    }\n",
    "    \n",
    "    with open('./models/feature_extractor_config.json', 'w') as f:\n",
    "        json.dump(feature_config, f, indent=2)\n",
    "    \n",
    "    print(\"Feature extractor configuration saved to ./models/feature_extractor_config.json\")\n",
    "    \n",
    "    # Save feature names\n",
    "    if 'train_features' in globals() and train_features:\n",
    "        with open('./models/feature_names.json', 'w') as f:\n",
    "            json.dump(train_features, f, indent=2)\n",
    "        \n",
    "        print(\"Feature names saved to ./models/feature_names.json\")\n",
    "\n",
    "# Save scaler and imputer if they exist\n",
    "if 'scaler' in globals() and scaler is not None:\n",
    "    with open('./models/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(\"Scaler saved to ./models/scaler.pkl\")\n",
    "\n",
    "if 'imputer' in globals() and imputer is not None:\n",
    "    with open('./models/imputer.pkl', 'wb') as f:\n",
    "        pickle.dump(imputer, f)\n",
    "    \n",
    "    print(\"Imputer saved to ./models/imputer.pkl\")\n",
    "\n",
    "print(\"\\nAll models and configurations saved to ./models/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Integrated Pipeline\n",
    "\n",
    "Now let's demonstrate how to use the integrated pipeline for a more streamlined workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ember_ml instead of TensorFlow and NumPy\n",
    "import ember_ml as eh\n",
    "from ember_ml import ops\n",
    "from ember_ml import nn\n",
    "from ember_ml.backend import get_backend\n",
    "\n",
    "# Import BigFrames\n",
    "import bigframes.pandas as bf\n",
    "\n",
    "# Print the current backend\n",
    "current_backend = get_backend()\n",
    "print(f\"Using {current_backend} backend\")\n",
    "\n",
    "# Import the integrated pipeline\n",
    "from examples.notebooks.bigquery.pipeline_demo import IntegratedPipeline\n",
    "\n",
    "# Define a function to convert BigFrames to ember_ml tensors\n",
    "def bigframes_to_tensor(bf_df, columns):\n",
    "    \"\"\"\n",
    "    Convert BigFrames DataFrame to ember_ml tensor.\n",
    "    \n",
    "    Args:\n",
    "        bf_df: BigFrames DataFrame\n",
    "        columns: Columns to include\n",
    "        \n",
    "    Returns:\n",
    "        ember_ml tensor\n",
    "    \"\"\"\n",
    "    # We need to convert to numpy array first, then to ember_ml tensor\n",
    "    # This is a temporary step until BigFrames supports direct conversion\n",
    "    array_data = bf_df[columns].to_numpy()\n",
    "    return tensor.convert_to_tensor(array_data)\n",
    "\n",
    "# Create integrated pipeline with BigFrames support\n",
    "class BigFramesIntegratedPipeline(IntegratedPipeline):\n",
    "    \"\"\"\n",
    "    Extended version of IntegratedPipeline that supports BigFrames DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize_feature_extractor(self, credentials_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor with BigFrames support.\n",
    "        \n",
    "        Args:\n",
    "            credentials_path: Optional path to service account credentials\n",
    "        \"\"\"\n",
    "        # Import the BigFrames-only version of the feature extractor\n",
    "        from ember_ml.nn.features.terabyte_feature_extractor_bigframes import TerabyteFeatureExtractor\n",
    "        \n",
    "        # Initialize the feature extractor\n",
    "        self.feature_extractor = TerabyteFeatureExtractor(\n",
    "            project_id=self.project_id,\n",
    "            location=\"US\",\n",
    "            chunk_size=100000,\n",
    "            max_memory_gb=16.0,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Set up BigQuery connection\n",
    "        self.feature_extractor.setup_bigquery_connection(credentials_path)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Feature extractor initialized with BigFrames support\")\n",
    "    \n",
    "    def extract_features(self, table_id, target_column=None, limit=None, force_categorical_columns=None):\n",
    "        \"\"\"\n",
    "        Extract features from a BigQuery table using BigFrames.\n",
    "        \n",
    "        Args:\n",
    "            table_id: BigQuery table ID (dataset.table)\n",
    "            target_column: Target variable name\n",
    "            limit: Optional row limit for testing\n",
    "            force_categorical_columns: Columns to force as categorical\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: (train_features, val_features, test_features)\n",
    "        \"\"\"\n",
    "        if self.feature_extractor is None:\n",
    "            raise ValueError(\"Feature extractor not initialized. Call initialize_feature_extractor() first.\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Extracting features from {table_id}\")\n",
    "        \n",
    "        # Extract features\n",
    "        result = self.feature_extractor.prepare_data(\n",
    "            table_id=table_id,\n",
    "            target_column=target_column,\n",
    "            limit=limit,\n",
    "            force_categorical_columns=force_categorical_columns\n",
    "        )\n",
    "        \n",
    "        if result is None:\n",
    "            if self.verbose:\n",
    "                print(\"Feature extraction failed\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Unpack result\n",
    "        train_df, val_df, test_df, train_features, val_features, test_features, self.scaler, self.imputer = result\n",
    "        \n",
    "        # Store DataFrames\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Extracted {len(train_features)} features\")\n",
    "            print(f\"Train shape: {train_df.shape}\")\n",
    "            print(f\"Validation shape: {val_df.shape}\")\n",
    "            print(f\"Test shape: {test_df.shape}\")\n",
    "        \n",
    "        # Convert BigFrames DataFrames directly to ember_ml tensors\n",
    "        train_features_array = bigframes_to_tensor(train_df, train_features)\n",
    "        val_features_array = bigframes_to_tensor(val_df, val_features)\n",
    "        test_features_array = bigframes_to_tensor(test_df, test_features)\n",
    "        \n",
    "        return train_features_array, val_features_array, test_features_array\n",
    "    \n",
    "    def apply_temporal_processing(self, features):\n",
    "        \"\"\"\n",
    "        Apply temporal stride processing to features.\n",
    "        \n",
    "        Args:\n",
    "            features: Feature tensor\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Stride perspectives\n",
    "        \"\"\"\n",
    "        if self.temporal_processor is None:\n",
    "            from ember_ml.nn.features.terabyte_feature_extractor_bigframes import TerabyteTemporalStrideProcessor\n",
    "            \n",
    "            self.temporal_processor = TerabyteTemporalStrideProcessor(\n",
    "                window_size=10,\n",
    "                stride_perspectives=self.stride_perspectives,\n",
    "                pca_components=32,\n",
    "                batch_size=10000,\n",
    "                use_incremental_pca=True,\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "        \n",
    "        # Process features through temporal processor\n",
    "        # Wrap in a generator to match the expected interface\n",
    "        def single_batch_generator(tensor):\n",
    "            yield tensor\n",
    "        \n",
    "        stride_perspectives = self.temporal_processor.process_large_dataset(\n",
    "            single_batch_generator(features)\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            for stride, data in stride_perspectives.items():\n",
    "                print(f\"Stride {stride}: shape {ops.shape(data)}\")\n",
    "        \n",
    "        return stride_perspectives\n",
    "    \n",
    "    def train_rbm(self, features, epochs=10):\n",
    "        \"\"\"\n",
    "        Train RBM on features.\n",
    "        \n",
    "        Args:\n",
    "            features: Feature tensor\n",
    "            epochs: Number of epochs\n",
    "            \n",
    "        Returns:\n",
    "            OptimizedRBM: Trained RBM\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Training RBM...\")\n",
    "        \n",
    "        # Initialize RBM\n",
    "        self.rbm = OptimizedRBM(\n",
    "            n_visible=ops.shape(features)[1],\n",
    "            n_hidden=self.rbm_hidden_units,\n",
    "            learning_rate=0.01,\n",
    "            momentum=0.5,\n",
    "            weight_decay=0.0001,\n",
    "            batch_size=100,\n",
    "            use_binary_states=False,\n",
    "            use_gpu=self.use_gpu,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Define a generator to yield data in batches\n",
    "        def rbm_data_generator(tensor, batch_size=100):\n",
    "            # Get total size\n",
    "            total_size = ops.shape(tensor)[0]\n",
    "            \n",
    "            # Generate random indices for shuffling\n",
    "            indices = ops.argsort(ops.random_uniform((total_size,)))\n",
    "            \n",
    "            # Shuffle tensor\n",
    "            shuffled = ops.gather(tensor, indices)\n",
    "            \n",
    "            # Yield batches\n",
    "            for i in range(0, total_size, batch_size):\n",
    "                end_idx = min(i + batch_size, total_size)\n",
    "                yield ops.slice(shuffled, [i, 0], [end_idx - i, ops.shape(tensor)[1]])\n",
    "        \n",
    "        # Train RBM\n",
    "        training_errors = self.rbm.train_in_chunks(\n",
    "            rbm_data_generator(features),\n",
    "            epochs=epochs,\n",
    "            k=1\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"RBM training complete. Final error: {training_errors[-1]:.4f}\")\n",
    "        \n",
    "        return self.rbm\n",
    "    \n",
    "    def extract_rbm_features(self, features):\n",
    "        \"\"\"\n",
    "        Extract features from trained RBM.\n",
    "        \n",
    "        Args:\n",
    "            features: Feature tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: RBM features\n",
    "        \"\"\"\n",
    "        if self.rbm is None:\n",
    "            raise ValueError(\"RBM not trained. Call train_rbm() first.\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Extracting RBM features...\")\n",
    "        \n",
    "        # Define a generator to yield data in batches\n",
    "        def feature_generator(tensor, batch_size=1000):\n",
    "            # Get total size\n",
    "            total_size = ops.shape(tensor)[0]\n",
    "            \n",
    "            # Yield batches\n",
    "            for i in range(0, total_size, batch_size):\n",
    "                end_idx = min(i + batch_size, total_size)\n",
    "                yield ops.slice(tensor, [i, 0], [end_idx - i, ops.shape(tensor)[1]])\n",
    "        \n",
    "        # Extract features\n",
    "        rbm_features = self.rbm.transform_in_chunks(\n",
    "            feature_generator(features)\n",
    "        )\n",
    "        \n",
    "        # Convert to ember_ml tensor if not already\n",
    "        rbm_features = tensor.convert_to_tensor(rbm_features)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"RBM features shape: {ops.shape(rbm_features)}\")\n",
    "        \n",
    "        return rbm_features\n",
    "    \n",
    "    def train_liquid_network(self, features, targets, validation_data=None, epochs=50, batch_size=32, network_type='motor_neuron'):\n",
    "        \"\"\"\n",
    "        Train liquid neural network on RBM features.\n",
    "        \n",
    "        Args:\n",
    "            features: RBM feature tensor\n",
    "            targets: Target tensor\n",
    "            validation_data: Tuple of (val_features, val_targets)\n",
    "            epochs: Number of epochs\n",
    "            batch_size: Batch size\n",
    "            network_type: Type of liquid network ('motor_neuron', 'lstm_gated', or 'multi_stride')\n",
    "            \n",
    "        Returns:\n",
    "            Model: Trained liquid network\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Training {network_type} liquid network...\")\n",
    "        \n",
    "        # Reshape features for sequence input\n",
    "        features_seq = ops.reshape(features, (ops.shape(features)[0], 1, ops.shape(features)[1]))\n",
    "        \n",
    "        # Prepare validation data\n",
    "        if validation_data is not None:\n",
    "            val_features, val_targets = validation_data\n",
    "            val_features_seq = ops.reshape(val_features, (ops.shape(val_features)[0], 1, ops.shape(val_features)[1]))\n",
    "            validation_data = (val_features_seq, val_targets)\n",
    "        \n",
    "        # Create liquid network\n",
    "        if network_type == 'motor_neuron':\n",
    "            self.liquid_network = create_liquid_network_with_motor_neuron(\n",
    "                input_dim=ops.shape(features)[1],\n",
    "                units=self.cfc_units,\n",
    "                output_dim=ops.shape(targets)[1],\n",
    "                sparsity_level=self.sparsity_level,\n",
    "                stride_length=1,\n",
    "                time_scale_factor=1.0,\n",
    "                threshold=self.threshold,\n",
    "                adaptive_threshold=True,\n",
    "                mixed_memory=True\n",
    "            )\n",
    "        elif network_type == 'lstm_gated':\n",
    "            self.liquid_network = create_lstm_gated_liquid_network(\n",
    "                input_dim=ops.shape(features)[1],\n",
    "                cfc_units=self.cfc_units,\n",
    "                lstm_units=self.lstm_units,\n",
    "                output_dim=ops.shape(targets)[1],\n",
    "                sparsity_level=self.sparsity_level\n",
    "            )\n",
    "        elif network_type == 'multi_stride':\n",
    "            self.liquid_network = create_multi_stride_liquid_network(\n",
    "                input_dim=ops.shape(features)[1],\n",
    "                units=self.cfc_units,\n",
    "                output_dim=ops.shape(targets)[1],\n",
    "                stride_perspectives=self.stride_perspectives,\n",
    "                sparsity_level=self.sparsity_level\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown network type: {network_type}\")\n",
    "        \n",
    "        # Set up callbacks\n",
    "        callbacks = [\n",
    "            # Early stopping\n",
    "            eh.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            eh.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            \n",
    "            # Model checkpoint\n",
    "            eh.callbacks.ModelCheckpoint(\n",
    "                filepath='./models/liquid_network_checkpoint',\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train liquid network\n",
    "        history = self.liquid_network.fit(\n",
    "            features_seq,\n",
    "            targets,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1 if self.verbose else 0\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Liquid network training complete.\")\n",
    "        \n",
    "        return self.liquid_network\n",
    "    \n",
    "    def process_data(self, features):\n",
    "        \"\"\"\n",
    "        Process data through the trained liquid network.\n",
    "        \n",
    "        Args:\n",
    "            features: RBM feature tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple: (motor_outputs, trigger_signals)\n",
    "        \"\"\"\n",
    "        if self.liquid_network is None:\n",
    "            raise ValueError(\"Liquid network not trained. Call train_liquid_network() first.\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Processing data through liquid network...\")\n",
    "        \n",
    "        # Reshape features for sequence input\n",
    "        features_seq = ops.reshape(features, (ops.shape(features)[0], 1, ops.shape(features)[1]))\n",
    "        \n",
    "        # Process data\n",
    "        outputs = self.liquid_network.predict(features_seq)\n",
    "        \n",
    "        # Extract motor neuron outputs and trigger signals\n",
    "        if isinstance(outputs, list):\n",
    "            motor_outputs = outputs[0]\n",
    "            trigger_signals = outputs[1][0]  # First element is trigger\n",
    "        else:\n",
    "            motor_outputs = outputs\n",
    "            trigger_signals = ops.cast(motor_outputs > self.threshold, ops.float32)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Motor neuron output range: {stats.min(motor_outputs):.4f} to {stats.max(motor_outputs):.4f}\")\n",
    "            print(f\"Trigger rate: {ops.stats.mean(trigger_signals):.4f}\")\n",
    "        \n",
    "        return motor_outputs, trigger_signals\n",
    "\n",
    "# Create integrated pipeline\n",
    "pipeline = BigFramesIntegratedPipeline(\n",
    "    project_id=PROJECT_ID,\n",
    "    rbm_hidden_units=64,\n",
    "    cfc_units=128,\n",
    "    lstm_units=32,\n",
    "    stride_perspectives=[1, 3, 5],\n",
    "    sparsity_level=0.5,\n",
    "    threshold=0.5,\n",
    "    use_gpu=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize feature extractor\n",
    "pipeline.initialize_feature_extractor(CREDENTIALS_PATH)\n",
    "\n",
    "# Extract features\n",
    "train_features_pipeline, val_features_pipeline, test_features_pipeline = pipeline.extract_features(\n",
    "    table_id=TABLE_ID,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    limit=LIMIT,\n",
    "    force_categorical_columns=[\n",
    "        \"eventType\", \"eventSource\", \"eventCategory\", \"eventPublisherId\",\n",
    "        \"productClass\", \"downloadTestStatus\", \"uploadState\", \"uploadTestStatus\",\n",
    "        \"wtn\", \"serialNumber\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check if features were extracted successfully\n",
    "if train_features_pipeline is not None:\n",
    "    # Apply temporal processing\n",
    "    train_temporal = pipeline.apply_temporal_processing(train_features_pipeline)\n",
    "    \n",
    "    # Train RBM\n",
    "    pipeline.train_rbm(train_features_pipeline, epochs=10)\n",
    "    \n",
    "    # Extract RBM features\n",
    "    train_rbm_features_pipeline = pipeline.extract_rbm_features(train_features_pipeline)\n",
    "    val_rbm_features_pipeline = pipeline.extract_rbm_features(val_features_pipeline)\n",
    "    test_rbm_features_pipeline = pipeline.extract_rbm_features(test_features_pipeline)\n",
    "    \n",
    "    # Create dummy targets for demonstration\n",
    "    train_targets_pipeline = ops.random_uniform((ops.shape(train_rbm_features_pipeline)[0], 1))\n",
    "    val_targets_pipeline = ops.random_uniform((ops.shape(val_rbm_features_pipeline)[0], 1))\n",
    "    \n",
    "    # Train liquid network\n",
    "    pipeline.train_liquid_network(\n",
    "        features=train_rbm_features_pipeline,\n",
    "        targets=train_targets_pipeline,\n",
    "        validation_data=(val_rbm_features_pipeline, val_targets_pipeline),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        network_type='lstm_gated'\n",
    "    )\n",
    "    \n",
    "    # Process test data\n",
    "    motor_outputs_pipeline, trigger_signals_pipeline = pipeline.process_data(test_rbm_features_pipeline)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Processed {ops.shape(test_rbm_features_pipeline)[0]} test samples\")\n",
    "    print(f\"Motor neuron output range: {stats.min(motor_outputs_pipeline):.4f} to {stats.max(motor_outputs_pipeline):.4f}\")\n",
    "    print(f\"Trigger rate: {ops.stats.mean(trigger_signals_pipeline):.4f}\")\n",
    "    \n",
    "    # Save models\n",
    "    pipeline.save_model('./models')\n",
    "    \n",
    "    # Print pipeline summary\n",
    "    print(pipeline.summary())\n",
    "    \n",
    "    # Plot motor neuron outputs and triggers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(ops.to_numpy(motor_outputs_pipeline[:100]), label='Motor Neuron Output')\n",
    "    threshold_values = ops.full((100,), 0.5)\n",
    "    plt.plot(ops.to_numpy(threshold_values), 'r--', label='Threshold')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Output Value')\n",
    "    plt.title('Motor Neuron Output and Threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(ops.to_numpy(trigger_signals_pipeline[:100]), 'g', label='Trigger Signal')\n",
    "    trigger_mean = ops.stats.mean(trigger_signals_pipeline)\n",
    "    plt.axhline(y=ops.to_numpy(trigger_mean), color='r', linestyle='--', \n",
    "               label=f'Trigger Rate: {ops.to_numpy(trigger_mean):.2f}')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Trigger (0/1)')\n",
    "    plt.title('Exploration Trigger Signals')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Pipeline feature extraction failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "#\n",
    "# In this notebook, we've demonstrated how to:\n",
    "\n",
    "# 1. Extract and prepare data from BigQuery tables using our terabyte-scale feature extractor with BigFrames\n",
    "print(\"â Extracted and prepared data from BigQuery tables using BigFrames\")\n",
    "\n",
    "# 2. Apply temporal stride processing to capture patterns at different time scales\n",
    "print(\"â Applied temporal stride processing to capture patterns at different time scales\")\n",
    "\n",
    "# 3. Train a Restricted Boltzmann Machine to learn latent representations\n",
    "print(\"â Trained a Restricted Boltzmann Machine to learn latent representations\")\n",
    "\n",
    "# 4. Feed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\n",
    "print(\"â Fed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\")\n",
    "\n",
    "# 5. Implement a motor neuron that outputs a value to trigger deeper exploration\n",
    "print(\"â Implemented a motor neuron that outputs a value to trigger deeper exploration\")\n",
    "\n",
    "# 6. Analyze the results to understand which samples trigger deeper exploration\n",
    "print(\"â Analyzed the results to understand which samples trigger deeper exploration\")\n",
    "\n",
    "# 7. Used emberharmony's GPU-friendly operations throughout the pipeline\n",
    "print(\"â Used emberharmony's GPU-friendly operations throughout the pipeline\")\n",
    "\n",
    "# This pipeline can be used for processing terabyte-sized tables efficiently through chunked processing,\n",
    "# making it suitable for large-scale data analysis and exploration.\n",
    "\n",
    "# Import the get_backend function\n",
    "from ember_ml import get_backend\n",
    "\n",
    "# Summary of the pipeline\n",
    "print(\"\\nSummary of the pipeline:\")\n",
    "print(\"------------------------\")\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Table ID: {TABLE_ID}\")\n",
    "print(f\"Target column: {TARGET_COLUMN}\")\n",
    "print(f\"Number of features: {len(train_features) if 'train_features' in globals() else 'N/A'}\")\n",
    "print(f\"Number of training samples: {len(train_df) if 'train_df' in globals() else 'N/A'}\")\n",
    "print(f\"Number of validation samples: {len(val_df) if 'val_df' in globals() else 'N/A'}\")\n",
    "print(f\"Number of test samples: {len(test_df) if 'test_df' in globals() else 'N/A'}\")\n",
    "print(f\"RBM hidden units: {rbm.n_hidden if 'rbm' in globals() else 'N/A'}\")\n",
    "print(f\"Liquid network type: {'Motor neuron' if 'liquid_network' in globals() else 'N/A'}\")\n",
    "print(f\"Backend used: {get_backend()}\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"----------\")\n",
    "print(\"1. Fine-tune the RBM and liquid network hyperparameters\")\n",
    "print(\"2. Experiment with different stride perspectives\")\n",
    "print(\"3. Try different liquid network architectures\")\n",
    "print(\"4. Apply the pipeline to other BigQuery tables\")\n",
    "print(\"5. Implement a feedback loop for continuous learning\")\n",
    "\n",
    "# Load the saved models for inference\n",
    "print(\"\\nTo load the saved models for inference:\")\n",
    "print(\"-------------------------------------\")\n",
    "print(\"```python\")\n",
    "print(\"from emberharmony.models.optimized_rbm import OptimizedRBM\")\n",
    "print(\"from emberharmony.core.stride_aware_cfc import create_liquid_network_with_motor_neuron\")\n",
    "print(\"from emberharmony.backend import get_backend\")\n",
    "print(\"\")\n",
    "print(\"# Print the current backend\")\n",
    "print(\"current_backend = get_backend()\")\n",
    "print(\"print(f\\\"Using {current_backend} backend\\\")\")\n",
    "print(\"\")\n",
    "print(\"# Load RBM\")\n",
    "print(\"rbm = OptimizedRBM(n_visible=len(features), n_hidden=64)\")\n",
    "print(\"rbm.load('./models/rbm.npy')\")\n",
    "print(\"\")\n",
    "print(\"# Load liquid network\")\n",
    "print(\"liquid_network = create_liquid_network_with_motor_neuron(input_dim=64, units=128, output_dim=1)\")\n",
    "print(\"liquid_network.load_weights('./models/liquid_network')\")\n",
    "print(\"```\")\n",
    "\n",
    "# Thank you message\n",
    "print(\"\\nThank you for exploring this notebook!\")\n",
    "print(\"For more information, please refer to the documentation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
