{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Example with Ember ML (using Evolutionary Strategies)\n",
    "\n",
    "This notebook demonstrates a reinforcement learning (RL) approach using the Ember ML framework, specifically tailored for environments where a full automatic differentiation system like `GradientTape` might not be available or suitable. We will implement a simple policy network and train it using an Evolutionary Strategies (ES)-inspired method, showcasing an inventive way to achieve policy learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Ember ML components\n",
    "from ember_ml.ops import set_backend\n",
    "from ember_ml.nn import tensor\n",
    "from ember_ml import ops\n",
    "from ember_ml.nn.modules import Dense, Module, Parameter # Using Dense for the policy network\n",
    "from ember_ml.training import Adam # Using Adam optimizer for parameter updates\n",
    "\n",
    "# Set a backend (choose 'numpy', 'torch', or 'mlx')\n",
    "# You can change this to see how the code runs on different backends\n",
    "set_backend('numpy')\n",
    "print(f\"Using backend: {ops.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define a Simple Environment\n",
    "\n",
    "We'll create a very basic environment: a 1D agent trying to reach a target location. The agent receives a reward based on its proximity to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple1DEnvironment:\n",
    "    def __init__(self, target_position=10.0, max_steps=100):\n",
    "        self.target_position = target_position\n",
    "        self.max_steps = max_steps\n",
    "        self.current_position = 0.0\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = 0.0\n",
    "        self.current_step = 0\n",
    "        return tensor.convert_to_tensor([self.current_position], dtype=tensor.float32) # State is current position\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is a continuous value representing movement (-1 to 1)\n",
    "        self.current_position += action\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Reward: negative of distance to target\n",
    "        reward = -ops.abs(self.current_position - self.target_position)\n",
    "\n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # State for the next step\n",
    "        next_state = tensor.convert_to_tensoronvert_to_tensor([self.current_position], dtype=tensor.float32)\n",
    "\n",
    "        return next_state, reward, done, {} # Return state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Policy Network\n",
    "\n",
    "We'll use a simple Dense network as our policy. It will take the current position as input and output a continuous action (movement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(in_features=input_size, out_features=16, activation='relu')\n",
    "        self.dense2 = Dense(in_features=16, out_features=output_size, activation='tanh') # Tanh to output action between -1 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is a tensor\n",
    "        x = tensor.convert_to_tensor(x, dtype=tensor.float32)\n",
    "        x = self.dense1(x)\n",
    "        action = self.dense2(x)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evolutionary Strategies (ES) Training Loop\n",
    "\n",
    "Instead of backpropagation, ES trains the policy by evaluating many perturbed versions of the policy and updating the parameters based on which perturbations yield better rewards. This is a gradient-free optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_episodes = 200\n",
    "learning_rate = 0.01\n",
    "noise_std = 0.1 # Standard deviation of the noise added to parameters\n",
    "num_perturbations = 10 # Number of perturbed policies to evaluate per episode\n",
    "\n",
    "# Environment and Policy Initialization\n",
    "env = Simple1DEnvironment()\n",
    "policy = PolicyNetwork(input_size=1, output_size=1)\n",
    "\n",
    "# Get trainable variables (parameters) of the policy\n",
    "policy_parameters = policy.trainable_variables\n",
    "\n",
    "# Create an optimizer (will be used to apply calculated updates)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Store episode rewards for plotting\n",
    "episode_rewards = []\n",
    "\n",
    "print(\"Starting ES training...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Evaluate the current policy (for baseline performance)\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Get action from the policy\n",
    "        action = policy(tensor.convert_to_tensor([state], dtype=tensor.float32))\n",
    "        # Convert action tensor to numpy scalar for environment step\n",
    "        action_np = tensor.to_numpy(action).squeeze()\n",
    "        state, reward, done, _ = env.step(action_np)\n",
    "        total_reward += reward\n",
    "    baseline_reward = total_reward\n",
    "\n",
    "    # Generate perturbations and evaluate perturbed policies\n",
    "    parameter_updates = [tensor.zeros_like(p.data) for p in policy_parameters] # Initialize updates to zero\n",
    "    \n",
    "    for _ in range(num_perturbations):\n",
    "        # Create perturbed parameters by adding noise\n",
    "        perturbed_parameters_plus = []\n",
    "        perturbed_parameters_minus = []\n",
    "        noise = []\n",
    "        \n",
    "        for p in policy_parameters:\n",
    "            # Generate noise with the same shape as the parameter\n",
    "            p_noise = tensor.random_normal(tensor.shape(p.data), mean=0.0, stddev=noise_std)\n",
    "            noise.append(p_noise)\n",
    "            \n",
    "            # Create perturbed parameters (positive and negative perturbations)\n",
    "            perturbed_parameters_plus.append(ops.add(p.data, p_noise))\n",
    "            perturbed_parameters_minus.append(ops.subtract(p.data, p_noise))\n",
    "        \n",
    "        # Create temporary policies with perturbed parameters\n",
    "        # This requires manually setting the parameter data for the temporary policies\n",
    "        # A more robust implementation might involve cloning the policy and setting parameters\n",
    "        \n",
    "        # Evaluate positive perturbation\n",
    "        temp_policy_plus = PolicyNetwork(input_size=1, output_size=1)\n",
    "        # Manually set parameter data (requires careful handling of parameter order)\n",
    "        for i, p in enumerate(temp_policy_plus.trainable_variables):\n",
    "             p.data = perturbed_parameters_plus[i]\n",
    "             \n",
    "        state = env.reset()\n",
    "        perturbed_reward_plus = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = temp_policy_plus(tensor.convert_to_tensor([state], dtype=tensor.float32))\n",
    "            action_np = tensor.to_numpy(action).squeeze()\n",
    "            state, reward, done, _ = env.step(action_np)\n",
    "            perturbed_reward_plus += reward\n",
    "            \n",
    "        # Evaluate negative perturbation\n",
    "        temp_policy_minus = PolicyNetwork(input_size=1, output_size=1)\n",
    "        # Manually set parameter data\n",
    "        for i, p in enumerate(temp_policy_minus.trainable_variables):\n",
    "             p.data = perturbed_parameters_minus[i]\n",
    "             \n",
    "        state = env.reset()\n",
    "        perturbed_reward_minus = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = temp_policy_minus(tensor.convert_to_tensor([state], dtype=tensor.float32))\n",
    "            action_np = tensor.to_numpy(action).squeeze()\n",
    "            state, reward, done, _ = env.step(action_np)\n",
    "            perturbed_reward_minus += reward\n",
    "            \n",
    "        # Calculate parameter updates based on rewards and noise (ES update rule)\n",
    "        # Update = (Reward_plus - Reward_minus) * Noise / (2 * num_perturbations * noise_std^2)\n",
    "        # Simplified update: (Reward_plus - Reward_minus) * Noise\n",
    "        reward_diff = tensor.convert_to_tensor(perturbed_reward_plus - perturbed_reward_minus, dtype=tensor.float32)\n",
    "        \n",
    "        for i in range(len(policy_parameters)):\n",
    "            # Accumulate updates for each parameter\n",
    "            # Note: This is a simplified update rule for demonstration\n",
    "            update = ops.multiply(reward_diff, noise[i])\n",
    "            parameter_updates[i] = ops.add(parameter_updates[i], update)\n",
    "\n",
    "    # Apply the accumulated parameter updates using the optimizer\n",
    "    # The optimizer's apply_gradients expects a list of (gradient, parameter) tuples\n",
    "    # In ES, our 'gradient' is the calculated update direction\n",
    "    \n",
    "    # Average the updates over the number of perturbations\n",
    "    averaged_updates = [ops.divide(update, tensor.convert_to_tensor(num_perturbations, dtype=tensor.float32)) for update in parameter_updates]\n",
    "    \n",
    "    # Apply updates using the optimizer\n",
    "    # We need to create dummy gradients that the optimizer can process\n",
    "    # For Adam, the update rule is roughly: param = param - learning_rate * m / (sqrt(v) + epsilon)\n",
    "    # In ES, our 'update' is the direction. We can feed this direction to the optimizer\n",
    "    # as if it were a gradient, and the optimizer will scale it by the learning rate\n",
    "    # and apply its internal momentum/adaptive learning rate logic.\n",
    "    \n",
    "    # Create a list of (update, parameter) tuples for apply_gradients\n",
    "    update_tuples = list(zip(averaged_updates, policy_parameters))\n",
    "    \n",
    "    # Apply the updates\n",
    "    optimizer.apply_gradients(update_tuples)\n",
    "\n",
    "    # Record and print results\n",
    "    episode_rewards.append(baseline_reward)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Baseline Reward: {baseline_reward:.2f}\")\n",
    "\n",
    "print(\"ES training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Training Progress\n",
    "\n",
    "Plot the episode rewards over time to see if the policy is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Episode Rewards during ES Training')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrate the Trained Policy\n",
    "\n",
    "Run the trained policy in the environment for a full episode to see its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demonstrating trained policy...\")\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "trajectory = [state[0]] # Store positions for visualization\n",
    "\n",
    "# Set policy to evaluation mode (if applicable, though Dense is stateless)\n",
    "policy.eval()\n",
    "\n",
    "while not done:\n",
    "    action = policy(tensor.convert_to_tensor([state], dtype=tensor.float32))\n",
    "    action_np = tensor.to_numpy(action).squeeze()\n",
    "    state, reward, done, _ = env.step(action_np)\n",
    "    total_reward += reward\n",
    "    trajectory.append(state[0])\n",
    "\n",
    "print(f\"Demonstration finished. Total Reward: {total_reward:.2f}\")\n",
    "print(f\"Final Position: {state[0]:.2f}\")\n",
    "\n",
    "# Visualize the agent's trajectory\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(trajectory)\n",
    "plt.axhline(y=env.target_position, color='r', linestyle='--', label='Target Position')\n",
    "plt.title('Agent Trajectory with Trained Policy')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Position')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to implement a reinforcement learning solution in Ember ML using an Evolutionary Strategies-inspired approach. By evaluating perturbed policy parameters and using the optimizer to apply updates based on performance, we were able to train a simple policy network to solve a basic 1D environment task. This highlights the flexibility of Ember ML's components for implementing inventive training methods even without a full automatic differentiation system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
