{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines (RBMs) in Ember ML\n",
    "\n",
    "This notebook provides an introduction to Restricted Boltzmann Machines (RBMs) using the Ember ML framework. We'll cover:\n",
    "\n",
    "1. What are RBMs and how do they work?\n",
    "2. Creating and training an RBM\n",
    "3. Using RBMs for feature learning\n",
    "4. Visualizing RBM weights and reconstructions\n",
    "\n",
    "RBMs are generative stochastic neural networks that can learn a probability distribution over their inputs. They consist of a visible layer and a hidden layer, with no connections between units within the same layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Ember ML components\n",
    "from ember_ml.ops import set_backend\n",
    "from ember_ml.nn import tensor\n",
    "from ember_ml import ops\n",
    "from ember_ml.models.rbm import RestrictedBoltzmannMachine, train_rbm\n",
    "\n",
    "# Set a backend (choose 'numpy', 'torch', or 'mlx')\n",
    "set_backend('numpy')\n",
    "print(f\"Using backend: {ops.get_backend()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tensor.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are RBMs?\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a type of generative stochastic artificial neural network that can learn a probability distribution over its inputs.\n",
    "\n",
    "Key characteristics of RBMs:\n",
    "- They have a visible layer (input) and a hidden layer\n",
    "- Connections exist only between visible and hidden units (not within layers)\n",
    "- They are energy-based models trained using contrastive divergence\n",
    "- They can be used for dimensionality reduction, feature learning, and generative modeling\n",
    "\n",
    "Let's create a simple diagram to visualize the RBM architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of RBM architecture\n",
    "def plot_rbm_architecture(visible_size=6, hidden_size=4):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot visible units\n",
    "    visible_x = tensor.ones(visible_size) * 1\n",
    "    visible_y = tensor.linspace(1, visible_size, visible_size)\n",
    "    plt.scatter(visible_x, visible_y, s=500, c='blue', label='Visible Units')\n",
    "    \n",
    "    # Plot hidden units\n",
    "    hidden_x = tensor.one.ones(hidden_size) * 4\n",
    "    hidden_y = tensor.linspace(1.5, hidden_size+0.5, hidden_size)\n",
    "    plt.scatter(hidden_x, hidden_y, s=500, c='red', label='Hidden Units')\n",
    "    \n",
    "    # Plot connections\n",
    "    for i in range(visible_size):\n",
    "        for j in range(hidden_size):\n",
    "            plt.plot([1, 4], [visible_y[i], hidden_y[j]], 'k-', alpha=0.2)\n",
    "    \n",
    "    # Add labels\n",
    "    for i in range(visible_size):\n",
    "        plt.text(1, visible_y[i], f\"v{i+1}\", ha='center', va='center', color='white', fontsize=12)\n",
    "    \n",
    "    for i in range(hidden_size):\n",
    "        plt.text(4, hidden_y[i], f\"h{i+1}\", ha='center', va='center', color='white', fontsize=12)\n",
    "    \n",
    "    plt.title('Restricted Boltzmann Machine Architecture', fontsize=14)\n",
    "    plt.xlim(0, 5)\n",
    "    plt.ylim(0, visible_size+1)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 0), ncol=2)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display the RBM architecture\n",
    "plot_rbm_architecture(visible_size=6, hidden_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Simple Data for Training\n",
    "\n",
    "Let's generate some simple data to train our RBM. We'll create patterns with correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple data with correlations\n",
    "def generate_simple_data(n_samples=1000, n_features=10):\n",
    "    # Generate random data\n",
    "    data = tensor.random_normal((n_samples, n_features), mean=0.0, stddev=1.0)\n",
    "    data = tensor.to_numpy(data)\n",
    "    \n",
    "    # Add correlations\n",
    "    data_tensor = tensor.convert_to_tensor(data)\n",
    "    \n",
    "    # Make feature 1 and 2 correlated\n",
    "    data_tensor = tensor.index_update(\n",
    "        data_tensor, \n",
    "        tensor.index[:, 1], \n",
    "        ops.add(data_tensor[:, 1], ops.multiply(data_tensor[:, 0], 0.8))\n",
    "    )\n",
    "    \n",
    "    # Make feature 3 and 4 correlated\n",
    "    data_tensor = tensor.index_update(\n",
    "        data_tensor, \n",
    "        tensor.index[:, 3], \n",
    "        ops.add(data_tensor[:, 3], ops.multiply(data_tensor[:, 2], 0.8))\n",
    "    )\n",
    "    \n",
    "    # Make feature 5 a function of features 0, 2\n",
    "    data_tensor = tensor.index_update(\n",
    "        data_tensor, \n",
    "        tensor.index[:, 5], \n",
    "        ops.add(ops.multiply(data_tensor[:, 0], 0.4), ops.multiply(data_tensor[:, 2], 0.4))\n",
    "    )\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    data = tensor.to_numpy(data_tensor)\n",
    "    \n",
    "    # Scale to [0, 1] for binary RBM\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "data = generate_simple_data(n_samples=1000, n_features=10)\n",
    "print(f\"Generated data shape: {data.shape}\")\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = np.corrcoef(data.T)\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.xticks(range(10), [f'F{i}' for i in range(10)])\n",
    "plt.yticks(range(10), [f'F{i}' for i in range(10)])\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.text(j, i, f\"{correlation_matrix[i, j]:.2f}\", \n",
    "                 ha=\"center\", va=\"center\", color=\"white\" if abs(correlation_matrix[i, j]) > 0.5 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Train an RBM\n",
    "\n",
    "Now let's create an RBM and train it on our data using Ember ML's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "\n",
    "\n",
    "n_samples = data.shape[0]\n",
    "n_train = int(n_samples * 0.8)\n",
    "\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:]\n",
    "\n",
    "print(f\"Training set: {train_data.shape} samples\")\n",
    "print(f\"Validation set: {val_data.shape} samples\")\n",
    "\n",
    "# Convert to EmberTensor\n",
    "train_tensor = tensor.convert_to_tensor(train_data, dtype=tensor.float32, device='cpu')\n",
    "val_tensor = tensor.convert_to_tensor(val_data, dtype=tensor.float32, device='cpu')\n",
    "\n",
    "# Create a simple data generator for training\n",
    "def data_generator(data_tensor, batch_size=32):\n",
    "    n_samples = tensor.shape(data_tensor)[0]\n",
    "    indices = list(range(n_samples))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        batch_indices = indices[i:min(i + batch_size, n_samples)]\n",
    "        batch_data = np.take(tensor.to_numpy(data_tensor), batch_indices, axis=0)\n",
    "        yield tensor.convert_to_tensor(batch_data, dtype=tensor.float32, device='cpu')\n",
    "\n",
    "# Create RBM\n",
    "visible_size = data.shape[1]  # Number of features\n",
    "hidden_size = 5               # Number of hidden units\n",
    "\n",
    "print(f\"Creating RBM with {visible_size} visible units and {hidden_size} hidden units\")\n",
    "rbm = RestrictedBoltzmannMachine(visible_size=visible_size, hidden_size=hidden_size, device='cpu')\n",
    "\n",
    "# Train RBM\n",
    "print(\"\\nTraining RBM...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create data generator\n",
    "train_gen = data_generator(train_tensor, batch_size=32)\n",
    "\n",
    "# Train the RBM\n",
    "losses = train_rbm(\n",
    "    rbm=rbm,\n",
    "    data_generator=train_gen,\n",
    "    epochs=50,\n",
    "    k=1  # Number of Gibbs sampling steps\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress by plotting the loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, 'b-')\n",
    "plt.title('RBM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize RBM Weights\n",
    "\n",
    "The weights of an RBM represent the connections between visible and hidden units. Let's visualize these weights to understand what patterns the RBM has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights\n",
    "weights = tensor.to_numpy(rbm.weights.data)\n",
    "\n",
    "# Plot the weights\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(weights, cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar(label='Weight Value')\n",
    "plt.title('RBM Weights (Visible x Hidden)')\n",
    "plt.xlabel('Hidden Units')\n",
    "plt.ylabel('Visible Units')\n",
    "plt.xticks(range(hidden_size), [f'H{i}' for i in range(hidden_size)])\n",
    "plt.yticks(range(visible_size), [f'V{i}' for i in range(visible_size)])\n",
    "\n",
    "# Add weight values\n",
    "for i in range(visible_size):\n",
    "    for j in range(hidden_size):\n",
    "        plt.text(j, i, f\"{weights[i, j]:.2f}\", \n",
    "                 ha=\"center\", va=\"center\", \n",
    "                 color=\"white\" if abs(weights[i, j]) > 0.5 * weights.max() else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Hidden Unit Activations\n",
    "\n",
    "Let's analyze what each hidden unit has learned by examining its activations for different input patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden activations for validation data\n",
    "hidden_probs, _ = rbm.visible_to_hidden(val_tensor)\n",
    "hidden_activations = tensor.to_numpy(hidden_probs)\n",
    "\n",
    "# Plot distribution of hidden unit activations\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(hidden_size):\n",
    "    plt.subplot(hidden_size, 1, i+1)\n",
    "    plt.hist(hidden_activations[:, i], bins=30, alpha=0.7)\n",
    "    plt.title(f'Hidden Unit {i} Activation Distribution')\n",
    "    plt.xlim(0, 1)\n",
    "    if i == hidden_size - 1:\n",
    "        plt.xlabel('Activation')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation between hidden units\n",
    "plt.figure(figsize=(8, 6))\n",
    "hidden_corr = np.corrcoef(hidden_activations.T)\n",
    "plt.imshow(hidden_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Hidden Unit Correlations')\n",
    "plt.xticks(range(hidden_size), [f'H{i}' for i in range(hidden_size)])\n",
    "plt.yticks(range(hidden_size), [f'H{i}' for i in range(hidden_size)])\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(hidden_size):\n",
    "    for j in range(hidden_size):\n",
    "        plt.text(j, i, f\"{hidden_corr[i, j]:.2f}\", \n",
    "                 ha=\"center\", va=\"center\", \n",
    "                 color=\"white\" if abs(hidden_corr[i, j]) > 0.5 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reconstruct Data\n",
    "\n",
    "One way to evaluate an RBM is to see how well it can reconstruct the original data. Let's reconstruct some validation samples and compare them to the originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few samples from validation set\n",
    "n_samples_to_show = 5\n",
    "sample_indices = ops.random_choice(len(val_data), n_samples_to_show, replace=False)\n",
    "samples = val_tensor[sample_indices]\n",
    "\n",
    "# Reconstruct samples\n",
    "reconstructed = rbm.reconstruct(samples)\n",
    "reconstructed_np = tensor.to_numpy(reconstructed)\n",
    "samples_np = tensor.to_numpy(samples)\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(n_samples_to_show):\n",
    "    # Plot original\n",
    "    plt.subplot(n_samples_to_show, 2, 2*i+1)\n",
    "    plt.bar(range(visible_size), samples_np[i], color='blue', alpha=0.7)\n",
    "    plt.title(f'Sample {i+1} - Original')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(range(visible_size), [f'V{j}' for j in range(visible_size)], rotation=45)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot reconstruction\n",
    "    plt.subplot(n_samples_to_show, 2, 2*i+2)\n",
    "    plt.bar(range(visible_size), reconstructed_np[i], color='red', alpha=0.7)\n",
    "    plt.title(f'Sample {i+1} - Reconstructed')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(range(visible_size), [f'V{j}' for j in range(visible_size)], rotation=45)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = stats.mean((samples_np - reconstructed_np) ** 2)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate New Samples\n",
    "\n",
    "RBMs are generative models, which means they can generate new samples that follow the learned distribution. Let's generate some new samples from our trained RBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new samples\n",
    "n_generated = 5\n",
    "generated_samples = rbm.sample(n_generated, num_gibbs_steps=1000)\n",
    "generated_np = tensor.to_numpy(generated_samples)\n",
    "\n",
    "# Plot generated samples\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(n_generated):\n",
    "    plt.subplot(n_generated, 1, i+1)\n",
    "    plt.bar(range(visible_size), generated_np[i], color='green', alpha=0.7)\n",
    "    plt.title(f'Generated Sample {i+1}')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(range(visible_size), [f'V{j}' for j in range(visible_size)])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Learning with RBMs\n",
    "\n",
    "RBMs can be used for feature learning and dimensionality reduction. Let's use the hidden layer activations as learned features and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden activations for all data\n",
    "all_data_tensor = tensor.convert_to_tensor(data, dtype=tensor.float32, device='cpu')\n",
    "all_hidden_probs, _ = rbm.visible_to_hidden(all_data_tensor)\n",
    "all_hidden_activations = tensor.to_numpy(all_hidden_probs)\n",
    "\n",
    "# Plot 2D scatter plot of first two hidden units\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(all_hidden_activations[:, 0], all_hidden_activations[:, 1], alpha=0.5)\n",
    "plt.title('Data Projected onto First Two Hidden Units')\n",
    "plt.xlabel('Hidden Unit 0 Activation')\n",
    "plt.ylabel('Hidden Unit 1 Activation')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If we have more than 2 hidden units, create a pairplot\n",
    "if hidden_size > 2:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i in range(min(4, hidden_size)):\n",
    "        for j in range(min(4, hidden_size)):\n",
    "            plt.subplot(min(4, hidden_size), min(4, hidden_size), i*min(4, hidden_size) + j + 1)\n",
    "            if i == j:\n",
    "                plt.hist(all_hidden_activations[:, i], bins=20, alpha=0.7)\n",
    "                plt.title(f'H{i}')\n",
    "            else:\n",
    "                plt.scatter(all_hidden_activations[:, j], all_hidden_activations[:, i], alpha=0.1, s=5)\n",
    "                plt.xlabel(f'H{j}')\n",
    "                plt.ylabel(f'H{i}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've explored the basics of Restricted Boltzmann Machines using Ember ML. We've covered:\n",
    "\n",
    "1. The architecture and principles of RBMs\n",
    "2. Training an RBM on synthetic data\n",
    "3. Visualizing RBM weights and hidden unit activations\n",
    "4. Reconstructing data and generating new samples\n",
    "5. Using RBMs for feature learning\n",
    "\n",
    "RBMs can be used for various tasks, including:\n",
    "- Dimensionality reduction\n",
    "- Feature learning\n",
    "- Generative modeling\n",
    "- Anomaly detection (as shown in the advanced anomaly detection notebook)\n",
    "- Collaborative filtering and recommendation systems\n",
    "\n",
    "Ember ML provides a flexible, backend-agnostic implementation of RBMs that can be used across different computational backends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
