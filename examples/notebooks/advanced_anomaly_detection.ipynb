{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Anomaly Detection with Ember ML\n",
    "\n",
    "This notebook demonstrates how to use Restricted Boltzmann Machines (RBMs) for anomaly detection with the Ember ML framework. We'll explore how to:\n",
    "\n",
    "1. Generate synthetic data with anomalies\n",
    "2. Train an RBM-based anomaly detector\n",
    "3. Evaluate detection performance\n",
    "4. Visualize the results\n",
    "\n",
    "This example showcases Ember ML's backend-agnostic capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: get_stats_module - Backend base module name: ember_ml.backend.torch\n",
      "DEBUG: get_stats_module - Constructed module name: ember_ml.backend.torch.stats\n",
      "DEBUG: get_stats_module - Successfully imported module: ember_ml.backend.torch.stats\n",
      "Using backend: torch\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Ember ML components\n",
    "from ember_ml.ops import set_backend\n",
    "from ember_ml.nn import tensor\n",
    "from ember_ml import ops\n",
    "from ember_ml.models.rbm_anomaly_detector import RBMBasedAnomalyDetector\n",
    "from ember_ml.visualization.rbm_visualizer import RBMVisualizer\n",
    "\n",
    "# Set a backend (choose 'numpy', 'torch', or 'mlx')\n",
    "set_backend('torch')  # Using PyTorch for GPU operations\n",
    "print(f\"Using backend: {ops.get_backend()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tensor.set_seed(42)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('outputs/plots', exist_ok=True)\n",
    "os.makedirs('outputs/models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data with Anomalies\n",
    "\n",
    "We'll create synthetic data with three types of anomalies:\n",
    "1. **Spike anomalies**: Sudden spikes in individual features\n",
    "2. **Correlation anomalies**: Breaking the normal correlation patterns\n",
    "3. **Collective anomalies**: Unusual patterns across multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "divide() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Make sure we're using the right data types when generating data\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomaly_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Plot a few features over time\u001b[39;00m\n\u001b[1;32m     92\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mgenerate_data\u001b[0;34m(n_samples, n_features, anomaly_fraction)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Add temporal patterns\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m---> 38\u001b[0m     time_value \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     sin_value \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmultiply(ops\u001b[38;5;241m.\u001b[39msin(time_value), \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     40\u001b[0m     row \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mslice_tensor(normal_tensor, [i, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: divide() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def generate_data(n_samples=1000, n_features=10, anomaly_fraction=0.05) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic data with anomalies.\"\"\"\n",
    "    # Import necessary modules\n",
    "    from ember_ml.nn import tensor\n",
    "    from ember_ml import ops\n",
    "    \n",
    "    # Generate normal data\n",
    "    normal_data = tensor.random_normal((n_samples, n_features), mean=0.0, stddev=1.0)\n",
    "    normal_data = tensor.to_numpy(normal_data)\n",
    "    \n",
    "    # Add correlations between features\n",
    "    normal_tensor = tensor.convert_to_tensor(normal_data)\n",
    "    for i in range(1, n_features):\n",
    "        feature_i = tensor.slice_tensor(normal_tensor, [0, i], [-1, 1])\n",
    "        feature_0 = tensor.slice_tensor(normal_tensor, [0, 0], [-1, 1])\n",
    "        weighted_i = ops.multiply(feature_i, 0.5)\n",
    "        weighted_0 = ops.multiply(feature_0, 0.5)\n",
    "        combined = ops.add(weighted_i, weighted_0)\n",
    "        # Update the tensor using bracket assignment\n",
    "        # Create updated tensor by combining slices before and after the modified column\n",
    "        if i > 0:\n",
    "            left_slice = tensor.slice_tensor(normal_tensor, [0, 0], [-1, i])\n",
    "            combined_reshaped = tensor.reshape(combined, [-1, 1])\n",
    "            if i < n_features - 1:\n",
    "                right_slice = tensor.slice_tensor(normal_tensor, [0, i+1], [-1, -1])\n",
    "                normal_tensor = tensor.concatenate([left_slice, combined_reshaped, right_slice], axis=1)\n",
    "            else:\n",
    "                normal_tensor = tensor.concatenate([left_slice, combined_reshaped], axis=1)\n",
    "        else:\n",
    "            combined_reshaped = tensor.reshape(combined, [-1, 1])\n",
    "            right_slice = tensor.slice_tensor(normal_tensor, [0, i+1], [-1, -1])\n",
    "            normal_tensor = tensor.concatenate([combined_reshaped, right_slice], axis=1)\n",
    "    \n",
    "    # Add temporal patterns\n",
    "    for i in range(n_samples):\n",
    "        time_value = ops.divide(tensor.convert_to_tensor(i, dtype=tensor.float32), 50.0, dtype=tensor.float32)\n",
    "        sin_value = ops.multiply(ops.sin(time_value), 0.5, dtype=tensor.float32)\n",
    "        row = tensor.slice_tensor(normal_tensor, [i, 0], [1, -1])\n",
    "        updated_row = ops.add(row, sin_value)\n",
    "        # Update the tensor using bracket assignment\n",
    "        normal_tensor[i, :] = tensor.squeeze(updated_row, axis=0)\n",
    "    \n",
    "    \n",
    "    # Generate anomalies\n",
    "    n_anomalies = int(n_samples * anomaly_fraction)\n",
    "    anomaly_indices = tensor.convert_to_tensor(ops.random_choice(n_samples, n_anomalies, replace=False))\n",
    "    \n",
    "    # Create different types of anomalies\n",
    "    normal_tensor = tensor.convert_to_tensor(normal_data)\n",
    "    for idx in anomaly_indices:\n",
    "        anomaly_type = tensor.convert_to_tensor(np.random.randint(0, 3))\n",
    "        \n",
    "        if anomaly_type == 0:  # Spike anomaly\n",
    "            feature_idx = tensor.convert_to_tensor(np.random.randint(0, n_features))\n",
    "            spike_value = tensor.convert_to_tensor(tensor.random_uniform(3.0, 5.0))\n",
    "            current_value = normal_tensor[idx, feature_idx]\n",
    "            updated_value = ops.add(current_value, tensor.convert_to_tensor(spike_value))\n",
    "            # Update the tensor using bracket assignment\n",
    "            normal_tensor[idx, feature_idx] = updated_value\n",
    "            \n",
    "        elif anomaly_type == 1:  # Correlation anomaly\n",
    "            random_values = tensor.random_normal((n_features,), mean=0.0, stddev=1.0)\n",
    "            # Update the tensor using bracket assignment\n",
    "            normal_tensor[idx, :] = random_values\n",
    "            \n",
    "        else:  # Collective anomaly\n",
    "            random_values = tensor.random_uniform((n_features,), minval=2.0, maxval=3.0)\n",
    "            current_row = normal_tensor[idx, :]\n",
    "            updated_row = ops.add(current_row, random_values)\n",
    "            # Update the tensor using bracket assignment\n",
    "            normal_tensor[idx, :] = updated_row\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    data = tensor.to_numpy(normal_tensor)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    columns = [f\"feature_{i+1}\" for i in range(n_features)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Add anomaly label\n",
    "    df['anomaly'] = 0\n",
    "    df.loc[anomaly_indices, 'anomaly'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Make sure we're using the right data types when generating data\n",
    "df = generate_data(n_samples=1000, n_features=10, anomaly_fraction=0.05)\n",
    "# Plot a few features over time\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get normal and anomaly indices\n",
    "normal_indices = df['anomaly'] == 0\n",
    "anomaly_indices = df['anomaly'] == 1\n",
    "\n",
    "# Plot the first 3 features\n",
    "for i in range(3):\n",
    "    feature_col = f\"feature_{i+1}\"\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    \n",
    "    # Plot normal data\n",
    "    plt.plot(df.index[normal_indices], \n",
    "             df.loc[normal_indices, feature_col], \n",
    "             'b-', alpha=0.7, label='Normal')\n",
    "    \n",
    "    # Plot anomalies\n",
    "    plt.scatter(df.index[anomaly_indices],\n",
    "                df.loc[anomaly_indices, feature_col],\n",
    "                color='red', marker='o', label='Anomaly')\n",
    "    \n",
    "    plt.title(f'{feature_col} Over Time')\n",
    "    plt.ylabel('Value')\n",
    "    if i == 2:  # Only show x-label for the bottom plot\n",
    "        plt.xlabel('Time Index')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "## 3. Prepare Data for Anomaly Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and split data\n",
    "features = df.drop('anomaly', axis=1).values\n",
    "labels = df['anomaly'].values\n",
    "\n",
    "# Split data into normal and anomalous\n",
    "normal_indices = labels == 0\n",
    "anomaly_indices = labels == 1\n",
    "\n",
    "normal_features = features[normal_indices]\n",
    "anomaly_features = features[anomaly_indices]\n",
    "\n",
    "# Split normal data into training and validation sets (80/20)\n",
    "n_normal = len(normal_features)\n",
    "n_train = int(n_normal * 0.8)\n",
    "\n",
    "train_features = tensor.convert_to_tensor(normal_features[:n_train])\n",
    "val_features = tensor.convert_to_tensor(normal_features[n_train:])\n",
    "\n",
    "print(f\"Training set: {train_features.shape} samples\")\n",
    "print(f\"Validation set: {val_features.shape} samples\")\n",
    "print(f\"Anomaly set: {anomaly_features.shape} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the RBM-based Anomaly Detector\n",
    "\n",
    "Now we'll train an RBM-based anomaly detector using Ember ML. The detector will learn the normal patterns in the data and identify deviations as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RBM-based anomaly detector\n",
    "print(\"Initializing RBM-based anomaly detector...\")\n",
    "detector = RBMBasedAnomalyDetector(\n",
    "    n_hidden=5,\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.5,\n",
    "    weight_decay=0.0001,\n",
    "    batch_size=10,\n",
    "    anomaly_threshold_percentile=95.0,\n",
    "    anomaly_score_method='reconstruction',\n",
    "    track_states=True\n",
    ")\n",
    "\n",
    "# Train anomaly detector\n",
    "print(\"\\nTraining anomaly detector...\")\n",
    "start_time = time.time()\n",
    "# Ensure train_features is in NumPy format\n",
    "if hasattr(train_features, 'numpy'):\n",
    "    validation_data=val_features if isinstance(val_features, TensorLike) else (val_features.numpy() if hasattr(val_features, 'numpy') else\n",
    "                     ops.to_numpy(val_features) if hasattr(ops, 'to_numpy') else val_features),\n",
    "elif hasattr(ops, 'to_numpy'):\n",
    "    train_features_np = ops.to_numpy(train_features)\n",
    "else:\n",
    "    train_features_np = train_features  # Assume it's already NumPy\n",
    "\n",
    "detector.fit(\n",
    "    X=train_features_np,\n",
    "    validation_data=val_features,\n",
    "    epochs=30,\n",
    "    k=1,\n",
    "    early_stopping_patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Print detector summary\n",
    "print(\"\\nAnomaly Detector Summary:\")\n",
    "print(detector.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detect Anomalies and Evaluate Performance\n",
    "\n",
    "Let's use the trained detector to identify anomalies in our test data and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine validation and anomaly data for testing\n",
    "test_features = ops.vstack([val_features, anomaly_features])\n",
    "test_labels = ops.hstack([tensor.zeros(len(val_features)), tensor.ones(len(anomaly_features))])\n",
    "\n",
    "# Predict anomalies\n",
    "print(\"Detecting anomalies...\")\n",
    "predicted_anomalies = detector.predict(test_features)\n",
    "anomaly_scores = detector.anomaly_score(test_features)\n",
    "\n",
    "# Compute metrics\n",
    "true_positives = ops.logical_and(ops.equal(predicted_anomalies, 1), ops.equal(test_labels, 1))\n",
    "false_positives = ops.logical_and(ops.equal(predicted_anomalies, 1), ops.equal(test_labels, 0))\n",
    "true_negatives = ops.logical_and(ops.equal(predicted_anomalies, 0), ops.equal(test_labels, 0))\n",
    "false_negatives = ops.logical_and(ops.equal(predicted_anomalies, 0), ops.equal(test_labels, 1))\n",
    "\n",
    "# Convert boolean tensors to count\n",
    "tp_sum = stats.sum(tensor.cast(true_positives, tensor.int32))\n",
    "fp_sum = stats.sum(tensor.cast(false_positives, tensor.int32))\n",
    "tn_sum = stats.sum(tensor.cast(true_negatives, tensor.int32))\n",
    "fn_sum = stats.sum(tensor.cast(false_negatives, tensor.int32))\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = tp_sum / (tp_sum + fp_sum) if (tp_sum + fp_sum) > 0 else 0.0\n",
    "recall = tp_sum / (tp_sum + fn_sum) if (tp_sum + fn_sum) > 0 else 0.0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_matrix = tensor.convert_to_tensor([\n",
    "    [true_negatives, false_positives],\n",
    "    [false_negatives, true_positives]\n",
    "])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Normal', 'Anomaly'])\n",
    "plt.yticks([0, 1], ['Normal', 'Anomaly'])\n",
    "\n",
    "# Add text annotations\n",
    "thresh = confusion_matrix.max() / 2.\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Anomaly Detection Results\n",
    "\n",
    "Let's visualize the anomaly scores and how well they separate normal from anomalous data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = RBMVisualizer()\n",
    "\n",
    "# Plot anomaly scores\n",
    "print(\"Plotting anomaly scores...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Split scores by label\n",
    "normal_scores = anomaly_scores[test_labels == 0]\n",
    "anomaly_scores_filtered = anomaly_scores[test_labels == 1]\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(normal_scores, bins=30, alpha=0.7, label='Normal')\n",
    "plt.hist(anomaly_scores_filtered, bins=30, alpha=0.7, label='Anomaly')\n",
    "plt.axvline(detector.anomaly_threshold, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Anomaly Score Distribution')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize RBM Internals\n",
    "\n",
    "Let's explore the internal representations learned by the RBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "print(\"Plotting training curve...\")\n",
    "visualizer.plot_training_curve(detector.rbm, show=True)\n",
    "\n",
    "# Plot weight matrix\n",
    "print(\"\\nPlotting weight matrix...\")\n",
    "visualizer.plot_weight_matrix(detector.rbm, show=True)\n",
    "\n",
    "# Plot reconstructions\n",
    "print(\"\\nPlotting reconstructions...\")\n",
    "visualizer.plot_reconstructions(detector.rbm, test_features[:5], show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use Ember ML's RBM-based anomaly detection capabilities to identify anomalies in multivariate data. The key advantages of this approach include:\n",
    "\n",
    "1. **Unsupervised learning**: No labeled anomaly data required for training\n",
    "2. **Capturing complex patterns**: RBMs can learn non-linear relationships in the data\n",
    "3. **Backend agnosticism**: The same code works across different computational backends\n",
    "4. **Interpretable results**: Visualizations help understand the model's behavior\n",
    "\n",
    "This approach can be extended to real-world anomaly detection tasks in various domains, including cybersecurity, industrial monitoring, and financial fraud detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
