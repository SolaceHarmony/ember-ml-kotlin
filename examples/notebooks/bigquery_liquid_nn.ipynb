{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Data Preparation and Feature Extraction for Liquid Neural Networks\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Extract and prepare data from BigQuery tables using BigFrames\n",
    "2. Process features through Restricted Boltzmann Machines (RBMs)\n",
    "3. Feed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\n",
    "4. Implement a motor neuron that outputs a value to trigger deeper exploration\n",
    "\n",
    "The pipeline is designed to handle terabyte-sized tables efficiently through chunked processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install google-cloud-bigquery bigframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bigframes.bigquery as bf\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any, Generator\n",
    "\n",
    "# Import ember_ml instead of NumPy and TensorFlow\n",
    "from ember_ml import ops\n",
    "from ember_ml.nn import tensor\n",
    "from ember_ml.ops import get_backend\n",
    "\n",
    "# Print the current backend\n",
    "current_backend = get_backend()\n",
    "print(f\"Using {current_backend} backend\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('bigquery_pipeline')\n",
    "\n",
    "# Import our components\n",
    "from ember_ml.nn.features.terabyte_feature_extractor_bigframes import TerabyteFeatureExtractor, TerabyteTemporalStrideProcessor\n",
    "from ember_ml.models.optimized_rbm import OptimizedRBM\n",
    "from ember_ml.models.stride_aware_cfc import (\n",
    "    create_liquid_network_with_motor_neuron,\n",
    "    create_lstm_gated_liquid_network,\n",
    "    create_multi_stride_liquid_network\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Connection Setup\n",
    "\n",
    "Set up the connection to BigQuery. You can use service account credentials or application default credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your GCP project ID\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your project ID\n",
    "\n",
    "# Path to service account credentials (optional)\n",
    "CREDENTIALS_PATH = 'path/to/your/credentials.json'  # Replace with path to credentials.json if needed\n",
    "\n",
    "# BigQuery location\n",
    "LOCATION = \"US\"\n",
    "\n",
    "# Initialize the feature extractor\n",
    "feature_extractor = TerabyteFeatureExtractor(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    chunk_size=100000,\n",
    "    max_memory_gb=16.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Set up BigQuery connection\n",
    "feature_extractor.setup_bigquery_connection(CREDENTIALS_PATH)\n",
    "\n",
    "print(f\"Connected to BigQuery project: {PROJECT_ID}\")\n",
    "print(f\"Using location: {LOCATION}\")\n",
    "print(f\"Feature extractor initialized with BigFrames support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Available Tables\n",
    "\n",
    "Let's explore the available tables in your BigQuery project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BigQuery client\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# List datasets\n",
    "datasets = list(client.list_datasets())\n",
    "print(f\"Datasets in project {PROJECT_ID}:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"- {dataset.dataset_id}\")\n",
    "\n",
    "# Choose a dataset to explore\n",
    "if datasets:\n",
    "    dataset_id = datasets[0].dataset_id\n",
    "    print(f\"\\nTables in dataset {dataset_id}:\")\n",
    "    tables = list(client.list_tables(dataset_id))\n",
    "    for table in tables:\n",
    "        print(f\"- {table.table_id}\")\n",
    "        \n",
    "    # Get more details about the first table\n",
    "    if tables:\n",
    "        first_table = tables[0]\n",
    "        table_ref = f\"{dataset_id}.{first_table.table_id}\"\n",
    "        table = client.get_table(table_ref)\n",
    "        \n",
    "        print(f\"\\nDetails for table {table_ref}:\")\n",
    "        print(f\"Description: {table.description}\")\n",
    "        print(f\"Row count: {table.num_rows}\")\n",
    "        print(f\"Created: {table.created}\")\n",
    "        print(f\"Last modified: {table.modified}\")\n",
    "        \n",
    "        print(\"\\nSchema:\")\n",
    "        for field in table.schema:\n",
    "            print(f\"- {field.name} ({field.field_type})\")\n",
    "            \n",
    "        # Preview data\n",
    "        print(\"\\nPreview data:\")\n",
    "        query = f\"SELECT * FROM `{PROJECT_ID}.{table_ref}` LIMIT 5\"\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        \n",
    "        for row in results:\n",
    "            print(row)\n",
    "else:\n",
    "    print(\"No datasets found in this project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from BigQuery\n",
    "\n",
    "Now let's extract features from a BigQuery table. Replace `TABLE_ID` with the table you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the table ID\n",
    "TABLE_ID = \"your-dataset.your-table\"  # Replace with your table ID\n",
    "\n",
    "# Set the target column (optional)\n",
    "TARGET_COLUMN = 'target_column'  # Replace with your target column if needed\n",
    "\n",
    "# Set a limit for testing (remove for full dataset)\n",
    "LIMIT = 10000\n",
    "\n",
    "# Extract features\n",
    "result = feature_extractor.prepare_data(\n",
    "    table_id=TABLE_ID,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    limit=LIMIT,\n",
    "    force_categorical_columns=[\n",
    "        # Add your categorical columns here\n",
    "        \"category1\", \"category2\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if result is not None:\n",
    "    train_df, val_df, test_df, train_features, val_features, test_features, scaler, imputer = result\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Validation shape: {val_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(f\"Features: {train_features}\")\n",
    "    \n",
    "    # Now we can use these DataFrames directly with BigFrames operations\n",
    "    # For example, to get the first few rows of the training data:\n",
    "    print(\"\\nFirst few rows of training data:\")\n",
    "    print(train_df.head())\n",
    "    \n",
    "    # Or to get summary statistics:\n",
    "    print(\"\\nSummary statistics for training data:\")\n",
    "    print(train_df[train_features].describe())\n",
    "    \n",
    "    # Define a function to convert DataFrames to ember_ml tensors\n",
    "    def df_to_tensor(df, columns):\n",
    "        \"\"\"\n",
    "        Convert DataFrame to ember_ml tensor.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame\n",
    "            columns: Columns to include\n",
    "            \n",
    "        Returns:\n",
    "            ember_ml tensor\n",
    "        \"\"\"\n",
    "        # We need to convert to numpy array first, then to ember_ml tensor\n",
    "        array_data = df[columns].to_numpy()\n",
    "        return tensor.convert_to_tensor(array_data)\n",
    "    \n",
    "    # Convert DataFrames to ember_ml tensors\n",
    "    print(\"\\nConverting DataFrames to ember_ml tensors...\")\n",
    "    train_tensor = df_to_tensor(train_df, train_features)\n",
    "    val_tensor = df_to_tensor(val_df, val_features)\n",
    "    test_tensor = df_to_tensor(test_df, test_features)\n",
    "    \n",
    "    print(\"\\nConverted to ember_ml tensors for acceleration\")\n",
    "    print(f\"Train tensor shape: {tensor.shape(train_tensor)}\")\n",
    "    print(f\"Validation tensor shape: {tensor.shape(val_tensor)}\")\n",
    "    print(f\"Test tensor shape: {tensor.shape(test_tensor)}\")\n",
    "    \n",
    "    # Perform some basic operations with ember_ml ops\n",
    "    print(\"\\nBasic statistics using ember_ml ops:\")\n",
    "    print(f\"Mean of train features: {ops.stats.mean(train_tensor, axis=0)[:5]}...\")  # Show first 5 means\n",
    "    print(f\"Standard deviation of train features: {ops.sqrt(ops.stats.mean(ops.square(train_tensor - ops.stats.mean(train_tensor, axis=0)), axis=0))[:5]}...\")  # Show first 5 stds\n",
    "    print(f\"Min of train features: {stats.min(train_tensor, axis=0)[:5]}...\")  # Show first 5 mins\n",
    "    print(f\"Max of train features: {stats.max(train_tensor, axis=0)[:5]}...\")  # Show first 5 maxs\n",
    "\n",
    "else:\n",
    "    print(\"Feature extraction failed\")\n",
    "    # Create empty variables to avoid NameError in subsequent cells\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    train_features = []\n",
    "    val_features = []\n",
    "    test_features = []\n",
    "    train_tensor = tensor.zeros((0, 0))\n",
    "    val_tensor = tensor.zeros((0, 0))\n",
    "    test_tensor = tensor.zeros((0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Temporal Stride Processing\n",
    "\n",
    "Now let's apply temporal stride processing to the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal processor\n",
    "temporal_processor = TerabyteTemporalStrideProcessor(\n",
    "    window_size=10,\n",
    "    stride_perspectives=[1, 3, 5],\n",
    "    pca_components=32,\n",
    "    batch_size=10000,\n",
    "    use_incremental_pca=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define a generator to yield data in batches\n",
    "def data_generator(df, features, batch_size=10000):\n",
    "    # Convert DataFrame to ember_ml tensors in batches\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Get a batch of data\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Convert directly to ember_ml tensor using our helper function\n",
    "        yield df_to_tensor(batch, features)\n",
    "\n",
    "# Process data - make sure train_df and train_features are defined\n",
    "if len(train_df) > 0 and len(train_features) > 0:\n",
    "    # Process the data through the temporal stride processor\n",
    "    stride_perspectives = temporal_processor.process_large_dataset(\n",
    "        data_generator(train_df, train_features, batch_size=10000)\n",
    "    )\n",
    "    \n",
    "    # Print stride perspective shapes\n",
    "    for stride, data in stride_perspectives.items():\n",
    "        # Use tensor.shape instead of .shape\n",
    "        print(f\"Stride {stride}: shape {tensor.shape(data)}\")\n",
    "    \n",
    "    # Visualize explained variance for each stride\n",
    "    explained_variances = [temporal_processor.get_explained_variance(stride) for stride in stride_perspectives.keys()]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(stride_perspectives.keys(), explained_variances)\n",
    "    plt.xlabel('Stride Length')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Stride Length')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize feature importance for the first stride\n",
    "    first_stride = list(stride_perspectives.keys())[0]\n",
    "    feature_importance = temporal_processor.get_feature_importance(first_stride)\n",
    "    \n",
    "    if feature_importance is not None:\n",
    "        # Convert to numpy for matplotlib\n",
    "        feature_importance_np = tensor.to_numpy(feature_importance)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(range(len(feature_importance_np)), feature_importance_np)\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.title(f'Feature Importance (Stride {first_stride})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Apply temporal processing to create multi-stride features\n",
    "    print(\"\\nApplying temporal processing to create multi-stride features...\")\n",
    "    \n",
    "    # Process through each stride perspective\n",
    "    multi_stride_features = {}\n",
    "    for stride, data in stride_perspectives.items():\n",
    "        # Store the transformed features\n",
    "        multi_stride_features[stride] = data\n",
    "        print(f\"Stride {stride} features shape: {tensor.shape(multi_stride_features[stride])}\")\n",
    "    \n",
    "    # Demonstrate how to combine multi-stride features\n",
    "    print(\"\\nCombining multi-stride features...\")\n",
    "    \n",
    "    # Get a list of all stride features\n",
    "    stride_features_list = [multi_stride_features[stride] for stride in sorted(multi_stride_features.keys())]\n",
    "    \n",
    "    # Concatenate along feature dimension (axis 1)\n",
    "    combined_features = ops.concatenate(stride_features_list, axis=1)\n",
    "    \n",
    "    print(f\"Combined multi-stride features shape: {tensor.shape(combined_features)}\")\n",
    "    \n",
    "    # Calculate correlation between strides using ember_ml ops\n",
    "    print(\"\\nCalculating correlation between stride features...\")\n",
    "    \n",
    "    # Calculate correlation matrix using ember_ml ops\n",
    "    # First center the data\n",
    "    centered_features = combined_features - ops.stats.mean(combined_features, axis=0)\n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix = ops.matmul(\n",
    "        ops.transpose(centered_features),\n",
    "        centered_features\n",
    "    ) / tensor.shape(combined_features)[0]\n",
    "    \n",
    "    # Convert to numpy only for visualization with matplotlib\n",
    "    cov_matrix_np = tensor.to_numpy(cov_matrix)\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cov_matrix_np, cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.title('Multi-stride Feature Covariance')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMulti-stride temporal processing complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot process data: train_df or train_features is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Restricted Boltzmann Machine\n",
    "\n",
    "Now let's train an RBM on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RBM\n",
    "if len(train_features) > 0:\n",
    "    # Initialize RBM\n",
    "    rbm = OptimizedRBM(\n",
    "        n_visible=len(train_features),\n",
    "        n_hidden=64,\n",
    "        learning_rate=0.01,\n",
    "        momentum=0.5,\n",
    "        weight_decay=0.0001,\n",
    "        batch_size=100,\n",
    "        use_binary_states=False,\n",
    "        use_gpu=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Define a generator to yield data in batches directly from DataFrames\n",
    "    def rbm_data_generator(df, features, batch_size=100):\n",
    "        # Get total size\n",
    "        total_size = len(df)\n",
    "        \n",
    "        # Create random indices for shuffling\n",
    "        indices = tensor.to_numpy(tensor.argsort(tensor.random_uniform((total_size,))))\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            end_idx = min(i + batch_size, total_size)\n",
    "            batch_indices = indices[i:end_idx]\n",
    "            \n",
    "            # Get batch from DataFrame\n",
    "            batch = df.iloc[batch_indices]\n",
    "            \n",
    "            # Convert directly to numpy array for RBM\n",
    "            yield batch[features].to_numpy()\n",
    "    \n",
    "    # Train RBM\n",
    "    training_errors = rbm.train_in_chunks(\n",
    "        rbm_data_generator(train_df, train_features, batch_size=100),\n",
    "        epochs=10,\n",
    "        k=1\n",
    "    )\n",
    "    \n",
    "    # Plot training errors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_errors)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title('RBM Training Error')\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract RBM features\n",
    "    def feature_generator(df, features, batch_size=1000):\n",
    "        # Get total size\n",
    "        total_size = len(df)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            end_idx = min(i + batch_size, total_size)\n",
    "            \n",
    "            # Get batch from DataFrame\n",
    "            batch = df.iloc[i:end_idx]\n",
    "            \n",
    "            # Convert directly to numpy array for RBM\n",
    "            yield batch[features].to_numpy()\n",
    "    \n",
    "    # Extract features from RBM\n",
    "    train_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(train_df, train_features, batch_size=1000)\n",
    "    )\n",
    "    \n",
    "    val_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(val_df, val_features, batch_size=1000)\n",
    "    )\n",
    "    \n",
    "    test_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(test_df, test_features, batch_size=1000)\n",
    "    )\n",
    "    \n",
    "    # Convert to ember_ml tensors\n",
    "    train_rbm_tensor = tensor.convert_to_tensor(train_rbm_features)\n",
    "    val_rbm_tensor = tensor.convert_to_tensor(val_rbm_features)\n",
    "    test_rbm_tensor = tensor.convert_to_tensor(test_rbm_features)\n",
    "    \n",
    "    print(f\"Train RBM features shape: {tensor.shape(train_rbm_tensor)}\")\n",
    "    print(f\"Validation RBM features shape: {tensor.shape(val_rbm_tensor)}\")\n",
    "    print(f\"Test RBM features shape: {tensor.shape(test_rbm_tensor)}\")\n",
    "    \n",
    "    # Visualize RBM feature distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot histograms for first 16 RBM features\n",
    "    for i in range(min(16, tensor.shape(train_rbm_tensor)[1])):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        # Convert to numpy for matplotlib\n",
    "        feature_np = tensor.to_numpy(train_rbm_tensor[:, i])\n",
    "        plt.hist(feature_np, bins=30, alpha=0.7)\n",
    "        plt.title(f'Feature {i+1}')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle('RBM Feature Distributions', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize feature correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Use ember_ml ops for correlation\n",
    "    centered_features = train_rbm_tensor - ops.stats.means.mean(train_rbm_tensor, axis=0)\n",
    "    corr_matrix = ops.matmul(\n",
    "        ops.transpose(centered_features),\n",
    "        centered_features\n",
    "    ) / tensor.shape(train_rbm_tensor)[0]\n",
    "    \n",
    "    # Convert to numpy only for visualization with matplotlib\n",
    "    corr_matrix_np = tensor.to_numpy(corr_matrix)\n",
    "    \n",
    "    plt.imshow(corr_matrix_np, cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.title('RBM Feature Correlations')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"RBM feature extraction complete!\")\n",
    "else:\n",
    "    print(\"Cannot train RBM: train_features is empty\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
