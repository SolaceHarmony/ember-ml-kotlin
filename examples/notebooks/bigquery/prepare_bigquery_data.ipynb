{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Data Preparation and Feature Extraction for Liquid Neural Networks\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Extract and prepare data from BigQuery tables using BigFrames\n",
    "2. Process features through Restricted Boltzmann Machines (RBMs)\n",
    "3. Feed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\n",
    "4. Implement a motor neuron that outputs a value to trigger deeper exploration\n",
    "\n",
    "The pipeline is designed to handle terabyte-sized tables efficiently through chunked processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install google-cloud-bigquery bigframes tensorflow ncps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: get_stats_module - Backend base module name: ember_ml.backend.mlx\n",
      "DEBUG: get_stats_module - Constructed module name: ember_ml.backend.mlx.stats\n",
      "DEBUG: get_stats_module - Successfully imported module: ember_ml.backend.mlx.stats\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any, Generator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('bigquery_pipeline')\n",
    "\n",
    "# Import our components\n",
    "from ember_ml.nn.features.terabyte_feature_extractor import TerabyteFeatureExtractor, TerabyteTemporalStrideProcessor\n",
    "from ember_ml.models.optimized_rbm import OptimizedRBM\n",
    "\n",
    "\n",
    "# Import the integrated pipeline\n",
    "from pipeline_demo import IntegratedPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery Connection Setup\n",
    "\n",
    "Set up the connection to BigQuery. You can use service account credentials or application default credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:29:37,151 - terabyte_feature_extractor - INFO - Using mlx backend for computation\n",
      "2025-04-22 12:29:37,151 - ember_ml.utils.backend - WARNING - Backend mlx does not support setting random seed\n",
      "2025-04-22 12:29:37,152 - terabyte_feature_extractor - INFO - Initialized TerabyteFeatureExtractor with chunk_size=100000, max_memory_gb=16.0\n",
      "2025-04-22 12:29:37,179 - terabyte_feature_extractor - INFO - BigQuery connection set up successfully\n"
     ]
    }
   ],
   "source": [
    "# Set your GCP project ID\n",
    "PROJECT_ID = \"massmkt-poc\"  # Replace with your project ID\n",
    "\n",
    "# Path to service account credentials (optional)\n",
    "CREDENTIALS_PATH = '/Users/sydneybach/sydney-bach.json'  # Replace with path to credentials.json if needed\n",
    "\n",
    "# BigQuery location\n",
    "LOCATION = \"US\"\n",
    "\n",
    "# Initialize the feature extractor\n",
    "feature_extractor = TerabyteFeatureExtractor(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    chunk_size=100000,\n",
    "    max_memory_gb=16.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Set up BigQuery connection\n",
    "feature_extractor.setup_bigquery_connection(CREDENTIALS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Available Tables\n",
    "\n",
    "Let's explore the available tables in your BigQuery project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project massmkt-poc:\n",
      "- 180601513\n",
      "- 239703222\n",
      "- 242584221\n",
      "- 267229650\n",
      "- BQML_Datasets\n",
      "- BigQuery_Google_Ads\n",
      "- Event_Data_Dictionary\n",
      "- FieldOps_Reporting_Dataset\n",
      "- MM_LP_Data\n",
      "- MM_LP_POC\n",
      "- Marketing_Cloud\n",
      "- Monitored_Zipcodes_Expanded\n",
      "- Partners\n",
      "- QF_AIOPS\n",
      "- QF_Activation_POC\n",
      "- QF_BUY_FLOW_TRANSACTIONS\n",
      "- QF_Shell_Account_CleanUp_Data\n",
      "- Service_Appointment_History\n",
      "- TEST1\n",
      "- abandoned_jobs\n",
      "- analytics_251783832\n",
      "- analytics_379694883\n",
      "- analytics_405473592\n",
      "- analytics_424581992\n",
      "- analytics_435146347\n",
      "- analytics_451204749\n",
      "- biwf_mysql_db\n",
      "- confluent_sink\n",
      "- connected_communities_dev\n",
      "- connected_communities_prod\n",
      "- contact_engine\n",
      "- datafirst_prod\n",
      "- datalake_ingestion_sandbox\n",
      "- design_repair\n",
      "- dev_sandbox\n",
      "- dispatch_events\n",
      "- dispatch_events_test\n",
      "- ds_mmdldev_bluemarble\n",
      "- ds_mmdldev_bluemarble_raw\n",
      "- ds_mmdldev_bluemarble_stg\n",
      "- ds_mmdldev_kafka_master\n",
      "- ds_mmdldev_nokiahal_raw\n",
      "- ds_mmdldev_pc360_raw\n",
      "- ds_mmpoc_dev_consmobile_silver\n",
      "- ds_mmpoc_master\n",
      "- ds_mmpoc_master_uscentral\n",
      "- dwh_staging\n",
      "- dx_ds\n",
      "- dx_ds_central\n",
      "- field_mission\n",
      "- firebase_crashlytics\n",
      "- gcp_cost_monitoring\n",
      "- gen_ai_auto\n",
      "- gen_ai_automation\n",
      "- goals_cart\n",
      "- google_adgroups\n",
      "- google_tag_manager_monitor\n",
      "- hierarchy\n",
      "- ibm_watson_discovery\n",
      "- jira_data\n",
      "- key_tables\n",
      "- key_tables_central\n",
      "- looker_scratch\n",
      "- looker_scratch_ga4\n",
      "- looker_scratch_prod\n",
      "- mdu_ros_data\n",
      "- mmdl_blueMarble\n",
      "- mmdl_pc360\n",
      "- mmdldev_gwynngroup_raw\n",
      "- mmdldev_gwynngroup_stg\n",
      "- mnet_test\n",
      "- nlp\n",
      "- nps\n",
      "- nps_survey_health\n",
      "- performics_historical\n",
      "- polygons\n",
      "- prodapt_dev\n",
      "- prodapt_prod\n",
      "- qf_activations_poc\n",
      "- qf_business_transactions\n",
      "- qf_events\n",
      "- qf_events_of_value\n",
      "- raw_comtech\n",
      "- raw_consmobile\n",
      "- raw_creditcheck\n",
      "- raw_gwynngroup\n",
      "- raw_ibmchat\n",
      "- raw_intrado\n",
      "- raw_modems\n",
      "- raw_salesforce_ctl_test\n",
      "- ros_data\n",
      "- rto\n",
      "- s3_html_email_import\n",
      "- sagar_test\n",
      "- salesforce_qf\n",
      "- salesforce_raw\n",
      "- sampletest\n",
      "- scale_serp\n",
      "- sfdc_gcp\n",
      "- splunk_dev\n",
      "- sqe\n",
      "- srv_assia_cloudcheck\n",
      "- srv_comtech\n",
      "- srv_consmobile\n",
      "- srv_creditcheck\n",
      "- srv_gwynngroup\n",
      "- srv_ibmchat\n",
      "- srv_intrado\n",
      "- srv_modems\n",
      "- srv_motive\n",
      "- srv_salesforce_qf\n",
      "- stage_dispatch_events_to_dataprod\n",
      "- stg_intrado\n",
      "- temp\n",
      "- test_datasets\n",
      "- test_sandbox\n",
      "- testingsample\n",
      "\n",
      "Tables in dataset 180601513:\n",
      "- ga_sessions_20190823\n",
      "- ga_sessions_20190824\n",
      "- ga_sessions_20190825\n",
      "- ga_sessions_20190826\n",
      "- ga_sessions_20190827\n",
      "- ga_sessions_20190828\n",
      "- ga_sessions_20190829\n",
      "- ga_sessions_20190830\n",
      "- ga_sessions_20190831\n",
      "- ga_sessions_20190901\n",
      "- ga_sessions_20190902\n",
      "- ga_sessions_20190903\n",
      "- ga_sessions_20190904\n",
      "- ga_sessions_20190905\n",
      "- ga_sessions_20190906\n",
      "- ga_sessions_20190907\n",
      "- ga_sessions_20190908\n",
      "- ga_sessions_20190909\n",
      "- ga_sessions_20190910\n",
      "- ga_sessions_20190911\n",
      "- ga_sessions_20190912\n",
      "- ga_sessions_20190913\n",
      "- ga_sessions_20190914\n",
      "- ga_sessions_20190915\n",
      "- ga_sessions_20190916\n",
      "- ga_sessions_20190917\n",
      "- ga_sessions_20190918\n",
      "- ga_sessions_20190919\n",
      "- ga_sessions_20190920\n",
      "- ga_sessions_20190921\n",
      "- ga_sessions_20190922\n",
      "- ga_sessions_20190923\n",
      "- ga_sessions_20190924\n",
      "- ga_sessions_20190925\n",
      "- ga_sessions_20190926\n",
      "- ga_sessions_20190927\n",
      "- ga_sessions_20190928\n",
      "- ga_sessions_20190929\n",
      "- ga_sessions_20190930\n",
      "- ga_sessions_20191001\n",
      "- ga_sessions_20191002\n",
      "- ga_sessions_20191003\n",
      "- ga_sessions_20191004\n",
      "- ga_sessions_20191005\n",
      "- ga_sessions_20191006\n",
      "- ga_sessions_20191007\n",
      "- ga_sessions_20191008\n",
      "- ga_sessions_20191009\n",
      "- ga_sessions_20191010\n",
      "- ga_sessions_20191011\n",
      "- ga_sessions_20191012\n",
      "- ga_sessions_20191013\n",
      "- ga_sessions_20191014\n",
      "- ga_sessions_20191015\n",
      "- ga_sessions_20191016\n",
      "- ga_sessions_20191017\n",
      "- ga_sessions_20191018\n",
      "- ga_sessions_20191019\n",
      "- ga_sessions_20191020\n",
      "- ga_sessions_20191021\n",
      "- ga_sessions_20191022\n",
      "- ga_sessions_20191023\n",
      "- ga_sessions_20191024\n",
      "- ga_sessions_20191025\n",
      "- ga_sessions_20191026\n",
      "- ga_sessions_20191027\n",
      "- ga_sessions_20191028\n",
      "- ga_sessions_20191029\n",
      "- ga_sessions_20191030\n",
      "- ga_sessions_20191031\n",
      "- ga_sessions_20191101\n",
      "- ga_sessions_20191102\n",
      "- ga_sessions_20191103\n",
      "- ga_sessions_20191104\n",
      "- ga_sessions_20191105\n",
      "- ga_sessions_20191106\n",
      "- ga_sessions_20191107\n",
      "- ga_sessions_20191108\n",
      "- ga_sessions_20191109\n",
      "- ga_sessions_20191110\n",
      "- ga_sessions_20191111\n",
      "- ga_sessions_20191112\n",
      "- ga_sessions_20191113\n",
      "- ga_sessions_20191114\n",
      "- ga_sessions_20191115\n",
      "- ga_sessions_20191116\n",
      "- ga_sessions_20191117\n",
      "- ga_sessions_20191118\n",
      "- ga_sessions_20191119\n",
      "- ga_sessions_20191120\n",
      "- ga_sessions_20191121\n",
      "- ga_sessions_20191122\n",
      "- ga_sessions_20191123\n",
      "- ga_sessions_20191124\n",
      "- ga_sessions_20191125\n",
      "- ga_sessions_20191126\n",
      "- ga_sessions_20191127\n",
      "- ga_sessions_20191128\n",
      "- ga_sessions_20191129\n",
      "- ga_sessions_20191130\n",
      "- ga_sessions_20191201\n",
      "- ga_sessions_20191202\n",
      "- ga_sessions_20191203\n",
      "- ga_sessions_20191204\n",
      "- ga_sessions_20191205\n",
      "- ga_sessions_20191206\n",
      "- ga_sessions_20191207\n",
      "- ga_sessions_20191208\n",
      "- ga_sessions_20191209\n",
      "- ga_sessions_20191210\n",
      "- ga_sessions_20191211\n",
      "- ga_sessions_20191212\n",
      "- ga_sessions_20191213\n",
      "- ga_sessions_20191214\n",
      "- ga_sessions_20191215\n",
      "- ga_sessions_20191216\n",
      "- ga_sessions_20191217\n",
      "- ga_sessions_20191218\n",
      "- ga_sessions_20191219\n",
      "- ga_sessions_20191220\n",
      "- ga_sessions_20191221\n",
      "- ga_sessions_20191222\n",
      "- ga_sessions_20191223\n",
      "- ga_sessions_20191224\n",
      "- ga_sessions_20191225\n",
      "- ga_sessions_20191226\n",
      "- ga_sessions_20191227\n",
      "- ga_sessions_20191228\n",
      "- ga_sessions_20191229\n",
      "- ga_sessions_20191230\n",
      "- ga_sessions_20191231\n",
      "- ga_sessions_20200101\n",
      "- ga_sessions_20200102\n",
      "- ga_sessions_20200103\n",
      "- ga_sessions_20200104\n",
      "- ga_sessions_20200105\n",
      "- ga_sessions_20200106\n",
      "- ga_sessions_20200107\n",
      "- ga_sessions_20200108\n",
      "- ga_sessions_20200109\n",
      "- ga_sessions_20200110\n",
      "- ga_sessions_20200111\n",
      "- ga_sessions_20200112\n",
      "- ga_sessions_20200113\n",
      "- ga_sessions_20200114\n",
      "- ga_sessions_20200115\n",
      "- ga_sessions_20200116\n",
      "- ga_sessions_20200117\n",
      "- ga_sessions_20200118\n",
      "- ga_sessions_20200119\n",
      "- ga_sessions_20200120\n",
      "- ga_sessions_20200121\n",
      "- ga_sessions_20200122\n",
      "- ga_sessions_20200123\n",
      "- ga_sessions_20200124\n",
      "- ga_sessions_20200125\n",
      "- ga_sessions_20200126\n",
      "- ga_sessions_20200127\n",
      "- ga_sessions_20200128\n",
      "- ga_sessions_20200129\n",
      "- ga_sessions_20200130\n",
      "- ga_sessions_20200131\n",
      "- ga_sessions_20200201\n",
      "- ga_sessions_20200202\n",
      "- ga_sessions_20200203\n",
      "- ga_sessions_20200204\n",
      "- ga_sessions_20200205\n",
      "- ga_sessions_20200206\n",
      "- ga_sessions_20200207\n",
      "- ga_sessions_20200208\n",
      "- ga_sessions_20200209\n",
      "- ga_sessions_20200210\n",
      "- ga_sessions_20200211\n",
      "- ga_sessions_20200212\n",
      "- ga_sessions_20200213\n",
      "- ga_sessions_20200214\n",
      "- ga_sessions_20200215\n",
      "- ga_sessions_20200216\n",
      "- ga_sessions_20200217\n",
      "- ga_sessions_20200218\n",
      "- ga_sessions_20200219\n",
      "- ga_sessions_20200220\n",
      "- ga_sessions_20200221\n",
      "- ga_sessions_20200222\n",
      "- ga_sessions_20200223\n",
      "- ga_sessions_20200224\n",
      "- ga_sessions_20200225\n",
      "- ga_sessions_20200226\n",
      "- ga_sessions_20200227\n",
      "- ga_sessions_20200228\n",
      "- ga_sessions_20200229\n",
      "- ga_sessions_20200301\n",
      "- ga_sessions_20200302\n",
      "- ga_sessions_20200303\n",
      "- ga_sessions_20200304\n",
      "- ga_sessions_20200305\n",
      "- ga_sessions_20200306\n",
      "- ga_sessions_20200307\n",
      "- ga_sessions_20200308\n",
      "- ga_sessions_20200309\n",
      "- ga_sessions_20200310\n",
      "- ga_sessions_20200311\n",
      "- ga_sessions_20200312\n",
      "- ga_sessions_20200313\n",
      "- ga_sessions_20200314\n",
      "- ga_sessions_20200315\n",
      "- ga_sessions_20200316\n",
      "- ga_sessions_20200317\n",
      "- ga_sessions_20200318\n",
      "- ga_sessions_20200319\n",
      "- ga_sessions_20200320\n",
      "- ga_sessions_20200321\n",
      "- ga_sessions_20200322\n",
      "- ga_sessions_20200323\n",
      "- ga_sessions_20200324\n",
      "- ga_sessions_20200325\n",
      "- ga_sessions_20200326\n",
      "- ga_sessions_20200327\n",
      "- ga_sessions_20200328\n",
      "- ga_sessions_20200329\n",
      "- ga_sessions_20200330\n",
      "- ga_sessions_20200331\n",
      "- ga_sessions_20200401\n",
      "- ga_sessions_20200402\n",
      "- ga_sessions_20200403\n",
      "- ga_sessions_20200404\n",
      "- ga_sessions_20200405\n",
      "- ga_sessions_20200406\n",
      "- ga_sessions_20200407\n",
      "- ga_sessions_20200408\n",
      "- ga_sessions_20200409\n",
      "- ga_sessions_20200410\n",
      "- ga_sessions_20200411\n",
      "- ga_sessions_20200412\n",
      "- ga_sessions_20200413\n",
      "- ga_sessions_20200414\n",
      "- ga_sessions_20200415\n",
      "- ga_sessions_20200416\n",
      "- ga_sessions_20200417\n",
      "- ga_sessions_20200418\n",
      "- ga_sessions_20200419\n",
      "- ga_sessions_20200420\n",
      "- ga_sessions_20200421\n",
      "- ga_sessions_20200422\n",
      "- ga_sessions_20200423\n",
      "- ga_sessions_20200424\n",
      "- ga_sessions_20200425\n",
      "- ga_sessions_20200426\n",
      "- ga_sessions_20200427\n",
      "- ga_sessions_20200428\n",
      "- ga_sessions_20200429\n",
      "- ga_sessions_20200430\n",
      "- ga_sessions_20200501\n",
      "- ga_sessions_20200502\n",
      "- ga_sessions_20200503\n",
      "- ga_sessions_20200504\n",
      "- ga_sessions_20200505\n",
      "- ga_sessions_20200506\n",
      "- ga_sessions_20200507\n",
      "- ga_sessions_20200508\n",
      "- ga_sessions_20200509\n",
      "- ga_sessions_20200510\n",
      "- ga_sessions_20200511\n",
      "- ga_sessions_20200512\n",
      "- ga_sessions_20200513\n",
      "- ga_sessions_20200514\n",
      "- ga_sessions_20200515\n",
      "- ga_sessions_20200516\n",
      "- ga_sessions_20200517\n",
      "- ga_sessions_20200518\n",
      "- ga_sessions_20200519\n",
      "- ga_sessions_20200520\n",
      "- ga_sessions_20200521\n",
      "- ga_sessions_20200522\n",
      "- ga_sessions_20200523\n",
      "- ga_sessions_20200524\n",
      "- ga_sessions_20200525\n",
      "- ga_sessions_20200526\n",
      "- ga_sessions_20200527\n",
      "- ga_sessions_20200528\n",
      "- ga_sessions_20200529\n",
      "- ga_sessions_20200530\n",
      "- ga_sessions_20200531\n",
      "- ga_sessions_20200601\n",
      "- ga_sessions_20200602\n",
      "- ga_sessions_20200603\n",
      "- ga_sessions_20200604\n",
      "- ga_sessions_20200605\n",
      "- ga_sessions_20200606\n",
      "- ga_sessions_20200607\n",
      "- ga_sessions_20200608\n",
      "- ga_sessions_20200609\n",
      "- ga_sessions_20200610\n",
      "- ga_sessions_20200611\n",
      "- ga_sessions_20200612\n",
      "- ga_sessions_20200613\n",
      "- ga_sessions_20200614\n",
      "- ga_sessions_20200615\n",
      "- ga_sessions_20200616\n",
      "- ga_sessions_20200617\n",
      "- ga_sessions_20200618\n",
      "- ga_sessions_20200619\n",
      "- ga_sessions_20200620\n",
      "- ga_sessions_20200621\n",
      "- ga_sessions_20200622\n",
      "- ga_sessions_20200623\n",
      "- ga_sessions_20200624\n",
      "- ga_sessions_20200625\n",
      "- ga_sessions_20200626\n",
      "- ga_sessions_20200627\n",
      "- ga_sessions_20200628\n",
      "- ga_sessions_20200629\n",
      "- ga_sessions_20200630\n",
      "- ga_sessions_20200701\n",
      "- ga_sessions_20200702\n",
      "- ga_sessions_20200703\n",
      "- ga_sessions_20200704\n",
      "- ga_sessions_20200705\n",
      "- ga_sessions_20200706\n",
      "- ga_sessions_20200707\n",
      "- ga_sessions_20200708\n",
      "- ga_sessions_20200709\n",
      "- ga_sessions_20200710\n",
      "- ga_sessions_20200711\n",
      "- ga_sessions_20200712\n",
      "- ga_sessions_20200713\n",
      "- ga_sessions_20200714\n",
      "- ga_sessions_20200715\n",
      "- ga_sessions_20200716\n",
      "- ga_sessions_20200717\n",
      "- ga_sessions_20200718\n",
      "- ga_sessions_20200719\n",
      "- ga_sessions_20200720\n",
      "- ga_sessions_20200721\n",
      "- ga_sessions_20200722\n",
      "- ga_sessions_20200723\n",
      "- ga_sessions_20200724\n",
      "- ga_sessions_20200725\n",
      "- ga_sessions_20200726\n",
      "- ga_sessions_20200727\n",
      "- ga_sessions_20200728\n",
      "- ga_sessions_20200729\n",
      "- ga_sessions_20200730\n",
      "- ga_sessions_20200731\n",
      "- ga_sessions_20200801\n",
      "- ga_sessions_20200802\n",
      "- ga_sessions_20200803\n",
      "- ga_sessions_20200804\n",
      "- ga_sessions_20200805\n",
      "- ga_sessions_20200806\n",
      "- ga_sessions_20200807\n",
      "- ga_sessions_20200808\n",
      "- ga_sessions_20200809\n",
      "- ga_sessions_20200810\n",
      "- ga_sessions_20200811\n",
      "- ga_sessions_20200812\n",
      "- ga_sessions_20200813\n",
      "- ga_sessions_20200814\n",
      "- ga_sessions_20200815\n",
      "- ga_sessions_20200816\n",
      "- ga_sessions_20200817\n",
      "- ga_sessions_20200818\n",
      "- ga_sessions_20200819\n",
      "- ga_sessions_20200820\n",
      "- ga_sessions_20200821\n",
      "- ga_sessions_20200822\n",
      "- ga_sessions_20200823\n",
      "- ga_sessions_20200824\n",
      "- ga_sessions_20200825\n",
      "- ga_sessions_20200826\n",
      "- ga_sessions_20200827\n",
      "- ga_sessions_20200828\n",
      "- ga_sessions_20200829\n",
      "- ga_sessions_20200830\n",
      "- ga_sessions_20200831\n",
      "- ga_sessions_20200901\n",
      "- ga_sessions_20200902\n",
      "- ga_sessions_20200903\n",
      "- ga_sessions_20200904\n",
      "- ga_sessions_20200905\n",
      "- ga_sessions_20200906\n",
      "- ga_sessions_20200907\n",
      "- ga_sessions_20200908\n",
      "- ga_sessions_20200909\n",
      "- ga_sessions_20200910\n",
      "- ga_sessions_20200911\n",
      "- ga_sessions_20200912\n",
      "- ga_sessions_20200913\n",
      "- ga_sessions_20200914\n",
      "- ga_sessions_20200915\n",
      "- ga_sessions_20200916\n",
      "- ga_sessions_20200917\n",
      "- ga_sessions_20200918\n",
      "- ga_sessions_20200919\n",
      "- ga_sessions_20200920\n",
      "- ga_sessions_20200921\n",
      "- ga_sessions_20200922\n",
      "- ga_sessions_20200923\n",
      "- ga_sessions_20200924\n",
      "- ga_sessions_20200925\n",
      "- ga_sessions_20200926\n",
      "- ga_sessions_20200927\n",
      "- ga_sessions_20200928\n",
      "- ga_sessions_20200929\n",
      "- ga_sessions_20200930\n",
      "- ga_sessions_20201001\n",
      "- ga_sessions_20201002\n",
      "- ga_sessions_20201003\n",
      "- ga_sessions_20201004\n",
      "- ga_sessions_20201005\n",
      "- ga_sessions_20201006\n",
      "- ga_sessions_20201007\n",
      "- ga_sessions_20201008\n",
      "- ga_sessions_20201009\n",
      "- ga_sessions_20201010\n",
      "- ga_sessions_20201011\n",
      "- ga_sessions_20201012\n",
      "- ga_sessions_20201013\n",
      "- ga_sessions_20201014\n",
      "- ga_sessions_20201015\n",
      "- ga_sessions_20201016\n",
      "- ga_sessions_20201017\n",
      "- ga_sessions_20201018\n",
      "- ga_sessions_20201019\n",
      "- ga_sessions_20201020\n",
      "- ga_sessions_20201021\n",
      "- ga_sessions_20201022\n",
      "- ga_sessions_20201023\n",
      "- ga_sessions_20201024\n",
      "- ga_sessions_20201025\n",
      "- ga_sessions_20201026\n",
      "- ga_sessions_20201027\n",
      "- ga_sessions_20201028\n",
      "- ga_sessions_20201029\n",
      "- ga_sessions_20201030\n",
      "- ga_sessions_20201031\n",
      "- ga_sessions_20201101\n",
      "- ga_sessions_20201102\n",
      "- ga_sessions_20201103\n",
      "- ga_sessions_20201104\n",
      "- ga_sessions_20201105\n",
      "- ga_sessions_20201106\n",
      "- ga_sessions_20201107\n",
      "- ga_sessions_20201108\n",
      "- ga_sessions_20201109\n",
      "- ga_sessions_20201110\n",
      "- ga_sessions_20201111\n",
      "- ga_sessions_20201112\n",
      "- ga_sessions_20201113\n",
      "- ga_sessions_20201114\n",
      "- ga_sessions_20201115\n",
      "- ga_sessions_20201116\n",
      "- ga_sessions_20201117\n",
      "- ga_sessions_20201118\n",
      "- ga_sessions_20201119\n",
      "- ga_sessions_20201120\n",
      "- ga_sessions_20201121\n",
      "- ga_sessions_20201122\n",
      "- ga_sessions_20201123\n",
      "- ga_sessions_20201124\n",
      "- ga_sessions_20201125\n",
      "- ga_sessions_20201126\n",
      "- ga_sessions_20201127\n",
      "- ga_sessions_20201128\n",
      "- ga_sessions_20201129\n",
      "- ga_sessions_20201130\n",
      "- ga_sessions_20201201\n",
      "- ga_sessions_20201202\n",
      "- ga_sessions_20201203\n",
      "- ga_sessions_20201204\n",
      "- ga_sessions_20201205\n",
      "- ga_sessions_20201206\n",
      "- ga_sessions_20201207\n",
      "- ga_sessions_20201208\n",
      "- ga_sessions_20201209\n",
      "- ga_sessions_20201210\n",
      "- ga_sessions_20201211\n",
      "- ga_sessions_20201212\n",
      "- ga_sessions_20201213\n",
      "- ga_sessions_20201214\n",
      "- ga_sessions_20201215\n",
      "- ga_sessions_20201216\n",
      "- ga_sessions_20201217\n",
      "- ga_sessions_20201218\n",
      "- ga_sessions_20201219\n",
      "- ga_sessions_20201220\n",
      "- ga_sessions_20201221\n",
      "- ga_sessions_20201222\n",
      "- ga_sessions_20201223\n",
      "- ga_sessions_20201224\n",
      "- ga_sessions_20201225\n",
      "- ga_sessions_20201226\n",
      "- ga_sessions_20201227\n",
      "- ga_sessions_20201228\n",
      "- ga_sessions_20201229\n",
      "- ga_sessions_20201230\n",
      "- ga_sessions_20201231\n",
      "- ga_sessions_20210101\n",
      "- ga_sessions_20210102\n",
      "- ga_sessions_20210103\n",
      "- ga_sessions_20210104\n",
      "- ga_sessions_20210105\n",
      "- ga_sessions_20210106\n",
      "- ga_sessions_20210107\n",
      "- ga_sessions_20210108\n",
      "- ga_sessions_20210109\n",
      "- ga_sessions_20210110\n",
      "- ga_sessions_20210111\n",
      "- ga_sessions_20210112\n",
      "- ga_sessions_20210113\n",
      "- ga_sessions_20210114\n",
      "- ga_sessions_20210115\n",
      "- ga_sessions_20210116\n",
      "- ga_sessions_20210117\n",
      "- ga_sessions_20210118\n",
      "- ga_sessions_20210119\n",
      "- ga_sessions_20210120\n",
      "- ga_sessions_20210121\n",
      "- ga_sessions_20210122\n",
      "- ga_sessions_20210123\n",
      "- ga_sessions_20210124\n",
      "- ga_sessions_20210125\n",
      "- ga_sessions_20210126\n",
      "- ga_sessions_20210127\n",
      "- ga_sessions_20210128\n",
      "- ga_sessions_20210129\n",
      "- ga_sessions_20210130\n",
      "- ga_sessions_20210131\n",
      "- ga_sessions_20210201\n",
      "- ga_sessions_20210202\n",
      "- ga_sessions_20210203\n",
      "- ga_sessions_20210204\n",
      "- ga_sessions_20210205\n",
      "- ga_sessions_20210206\n",
      "- ga_sessions_20210207\n",
      "- ga_sessions_20210208\n",
      "- ga_sessions_20210209\n",
      "- ga_sessions_20210210\n",
      "- ga_sessions_20210211\n",
      "- ga_sessions_20210212\n",
      "- ga_sessions_20210213\n",
      "- ga_sessions_20210214\n",
      "- ga_sessions_20210215\n",
      "- ga_sessions_20210216\n",
      "- ga_sessions_20210217\n",
      "- ga_sessions_20210218\n",
      "- ga_sessions_20210219\n",
      "- ga_sessions_20210220\n",
      "- ga_sessions_20210221\n",
      "- ga_sessions_20210222\n",
      "- ga_sessions_20210223\n",
      "- ga_sessions_20210224\n",
      "- ga_sessions_20210225\n",
      "- ga_sessions_20210226\n",
      "- ga_sessions_20210227\n",
      "- ga_sessions_20210228\n",
      "- ga_sessions_20210301\n",
      "- ga_sessions_20210302\n",
      "- ga_sessions_20210303\n",
      "- ga_sessions_20210304\n",
      "- ga_sessions_20210305\n",
      "- ga_sessions_20210306\n",
      "- ga_sessions_20210307\n",
      "- ga_sessions_20210308\n",
      "- ga_sessions_20210309\n",
      "- ga_sessions_20210310\n",
      "- ga_sessions_20210311\n",
      "- ga_sessions_20210312\n",
      "- ga_sessions_20210313\n",
      "- ga_sessions_20210314\n",
      "- ga_sessions_20210315\n",
      "- ga_sessions_20210316\n",
      "- ga_sessions_20210317\n",
      "- ga_sessions_20210318\n",
      "- ga_sessions_20210319\n",
      "- ga_sessions_20210320\n",
      "- ga_sessions_20210321\n",
      "- ga_sessions_20210322\n",
      "- ga_sessions_20210323\n",
      "- ga_sessions_20210324\n",
      "- ga_sessions_20210325\n",
      "- ga_sessions_20210326\n",
      "- ga_sessions_20210327\n",
      "- ga_sessions_20210328\n",
      "- ga_sessions_20210329\n",
      "- ga_sessions_20210330\n",
      "- ga_sessions_20210331\n",
      "- ga_sessions_20210401\n",
      "- ga_sessions_20210402\n",
      "- ga_sessions_20210403\n",
      "- ga_sessions_20210404\n",
      "- ga_sessions_20210405\n",
      "- ga_sessions_20210406\n",
      "- ga_sessions_20210407\n",
      "- ga_sessions_20210408\n",
      "- ga_sessions_20210409\n",
      "- ga_sessions_20210410\n",
      "- ga_sessions_20210411\n",
      "- ga_sessions_20210412\n",
      "- ga_sessions_20210413\n",
      "- ga_sessions_20210414\n",
      "- ga_sessions_20210415\n",
      "- ga_sessions_20210416\n",
      "- ga_sessions_20210417\n",
      "- ga_sessions_20210418\n",
      "- ga_sessions_20210419\n",
      "- ga_sessions_20210420\n",
      "- ga_sessions_20210421\n",
      "- ga_sessions_20210422\n",
      "- ga_sessions_20210423\n",
      "- ga_sessions_20210424\n",
      "- ga_sessions_20210425\n",
      "- ga_sessions_20210426\n",
      "- ga_sessions_20210427\n",
      "- ga_sessions_20210428\n",
      "- ga_sessions_20210429\n",
      "- ga_sessions_20210430\n",
      "- ga_sessions_20210501\n",
      "- ga_sessions_20210502\n",
      "- ga_sessions_20210503\n",
      "- ga_sessions_20210504\n",
      "- ga_sessions_20210505\n",
      "- ga_sessions_20210506\n",
      "- ga_sessions_intraday_20210507\n",
      "- google_aggregate_historical\n"
     ]
    }
   ],
   "source": [
    "# Import BigQuery client\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# List datasets\n",
    "datasets = list(client.list_datasets())\n",
    "print(f\"Datasets in project {PROJECT_ID}:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"- {dataset.dataset_id}\")\n",
    "\n",
    "# Choose a dataset to explore\n",
    "if datasets:\n",
    "    dataset_id = datasets[0].dataset_id\n",
    "    print(f\"\\nTables in dataset {dataset_id}:\")\n",
    "    tables = list(client.list_tables(dataset_id))\n",
    "    for table in tables:\n",
    "        print(f\"- {table.table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from BigQuery\n",
    "\n",
    "Now let's extract features from a BigQuery table. Replace `TABLE_ID` with the table you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:29:41,258 - terabyte_feature_extractor - INFO - Starting data preparation for table TEST1.ctl_modem_speedtest_event\n",
      "2025-04-22 12:29:41,259 - terabyte_feature_extractor - INFO - Limiting to 10000 rows (1 chunks)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 915563ac-1d74-46bf-ae84-eb36163a1b61 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:915563ac-1d74-46bf-ae84-eb36163a1b61&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 4f5c9c4c-747a-414a-bab9-50669a33ec84 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:4f5c9c4c-747a-414a-bab9-50669a33ec84&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 47d57860-63c0-41d6-a83c-9b7c5a9210fe is DONE. 8 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:47d57860-63c0-41d6-a83c-9b7c5a9210fe&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:29:50,660 - terabyte_feature_extractor - INFO - Table TEST1.ctl_modem_speedtest_event has approximately 241 rows\n",
      "2025-04-22 12:29:50,661 - terabyte_feature_extractor - INFO - Processing approximately 241 rows in 1 chunks of 100000\n",
      "2025-04-22 12:29:50,661 - terabyte_feature_extractor - INFO - Processing chunk 1/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 0bbae6ff-ee11-41b2-a47e-67bb0718b3d2 is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:0bbae6ff-ee11-41b2-a47e-67bb0718b3d2&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:29:54,590 - terabyte_feature_extractor - INFO - Chunk 1/1 processed in 3.93s. Progress: 100.0%. Estimated time remaining: 0.00s\n",
      "2025-04-22 12:29:54,591 - terabyte_feature_extractor - INFO - Current memory usage: 0.91 GB\n",
      "2025-04-22 12:29:54,742 - terabyte_feature_extractor - INFO - Completed processing 1 chunks in 4.08s\n",
      "2025-04-22 12:29:54,742 - terabyte_feature_extractor - ERROR - Failed to process data: result is not a DataFrame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction failed\n"
     ]
    }
   ],
   "source": [
    "# Set the table ID\n",
    "TABLE_ID = \"TEST1.ctl_modem_speedtest_event\"  # Replace with your table ID\n",
    "\n",
    "# Set the target column (optional)\n",
    "TARGET_COLUMN = 'downloadLatency'  # Replace with your target column if needed\n",
    "\n",
    "# Set a limit for testing (remove for full dataset)\n",
    "LIMIT = 10000\n",
    "\n",
    "# Define a function to process each chunk\n",
    "def process_chunk(chunk_df):\n",
    "    # If it's a BigFrames DataFrame, convert it to pandas\n",
    "    if hasattr(chunk_df, 'to_pandas') and callable(getattr(chunk_df, 'to_pandas')):\n",
    "        # Convert BigFrames DataFrame to pandas DataFrame\n",
    "        return chunk_df.to_pandas()\n",
    "    \n",
    "    # If it's already a pandas DataFrame, return it\n",
    "    if isinstance(chunk_df, pd.DataFrame):\n",
    "        return chunk_df\n",
    "    \n",
    "    # Otherwise, try to convert to pandas DataFrame\n",
    "    try:\n",
    "        return pd.DataFrame(chunk_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to pandas DataFrame: {e}\")\n",
    "        # Return empty DataFrame as fallback\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Extract features\n",
    "result = feature_extractor.prepare_data(\n",
    "    table_id=TABLE_ID,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    limit=LIMIT,\n",
    "    force_categorical_columns=[\n",
    "        \"eventType\", \"eventSource\", \"eventCategory\", \"eventPublisherId\",\n",
    "        \"productClass\", \"downloadTestStatus\", \"uploadState\", \"uploadTestStatus\",\n",
    "        \"wtn\", \"serialNumber\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if result is not None:\n",
    "    train_df, val_df, test_df, train_features, val_features, test_features, scaler, imputer = result\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Validation shape: {val_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(f\"Features: {train_features}\")\n",
    "else:\n",
    "    print(\"Feature extraction failed\")\n",
    "    # Create empty variables to avoid NameError in subsequent cells\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    train_features = []\n",
    "    val_features = []\n",
    "    test_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Query job a0e50eca-629a-4c24-87f3-3b5ea0758749 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:a0e50eca-629a-4c24-87f3-3b5ea0758749&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema (bf_df.dtypes): eventId               string[pyarrow]\n",
      "eventType             string[pyarrow]\n",
      "eventSource           string[pyarrow]\n",
      "eventCategory         string[pyarrow]\n",
      "eventPublisherId      string[pyarrow]\n",
      "wtn                   string[pyarrow]\n",
      "serialNumber          string[pyarrow]\n",
      "productClass          string[pyarrow]\n",
      "downloadThroughput            Float64\n",
      "downloadLatency               Float64\n",
      "downloadTestStatus    string[pyarrow]\n",
      "uploadState           string[pyarrow]\n",
      "uploadThroughput              Float64\n",
      "uploadTestStatus      string[pyarrow]\n",
      "dtype: object\n",
      "Performing random split...\n",
      "Random split ratios: 80/20 (train/temp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job a4eab183-5348-484d-bca2-11cd64b945cb is DONE. 6.0 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:a4eab183-5348-484d-bca2-11cd64b945cb&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/test split ratios: 50/50 from temp\n",
      "Preparing dataframe (is_train=True)...\n",
      "Columns to drop (after filtering): []\n",
      "WARNING: No target specified. Using 'uploadThroughput' as target.\n",
      "Initial features: ['downloadThroughput', 'downloadLatency']\n",
      "Encoding categorical columns: ['eventType', 'eventSource', 'eventCategory', 'eventPublisherId', 'wtn', 'serialNumber', 'productClass', 'downloadTestStatus', 'uploadState', 'uploadTestStatus']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 77ff1da2-2168-4050-83af-136fd32210bb is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:77ff1da2-2168-4050-83af-136fd32210bb&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 5e917bd5-4164-490f-8fef-a1bf07bbb85d is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:5e917bd5-4164-490f-8fef-a1bf07bbb85d&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After datetime feature engineering, columns: ['eventId', 'downloadThroughput', 'downloadLatency', 'uploadThroughput', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "Final features: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job cec19b9d-2d5a-43c3-9ac5-3cc40e5676f6 is DONE. 26.7 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:cec19b9d-2d5a-43c3-9ac5-3cc40e5676f6&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 8e8f5e85-de80-451b-9dae-d5a33578ecfd is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:8e8f5e85-de80-451b-9dae-d5a33578ecfd&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 767f84b2-5d02-46ca-8dcd-0c1e8133cc97 is DONE. 135.0 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:767f84b2-5d02-46ca-8dcd-0c1e8133cc97&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job e1782db2-6ce9-40f6-a698-5b6d46a98983 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:e1782db2-6ce9-40f6-a698-5b6d46a98983&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 38de1d3d-4db8-4470-a804-cb4bf2c57cf5 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:38de1d3d-4db8-4470-a804-cb4bf2c57cf5&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 465046b8-94f2-4a7f-b472-fcb61d626a38 is DONE. 138.3 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:465046b8-94f2-4a7f-b472-fcb61d626a38&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job ee34eb59-1831-4320-ac7c-4a3668dae9bf is DONE. 138.3 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:ee34eb59-1831-4320-ac7c-4a3668dae9bf&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 5ad38590-757a-408a-845d-412608d6399b is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:5ad38590-757a-408a-845d-412608d6399b&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler and imputer fitted and applied on training data.\n",
      "Columns to drop due to high nulls: []\n",
      "Features after dropping high null columns: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "Dataframe prepared successfully. Returning split.\n",
      "Preparing dataframe (is_train=False)...\n",
      "Columns to drop (after filtering): []\n",
      "WARNING: No target specified. Using 'uploadThroughput' as target.\n",
      "Initial features: ['downloadThroughput', 'downloadLatency']\n",
      "Encoding categorical columns: ['eventType', 'eventSource', 'eventCategory', 'eventPublisherId', 'wtn', 'serialNumber', 'productClass', 'downloadTestStatus', 'uploadState', 'uploadTestStatus']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 5b49664f-91a9-4e6a-bc5a-9cf305c76e66 is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:5b49664f-91a9-4e6a-bc5a-9cf305c76e66&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job cf4a9d06-d688-4614-8e27-f856bd4babdd is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:cf4a9d06-d688-4614-8e27-f856bd4babdd&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After datetime feature engineering, columns: ['eventId', 'downloadThroughput', 'downloadLatency', 'uploadThroughput', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "Final features: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 11c139c9-f9cc-4b4d-bfff-ffa6a0b5e685 is DONE. 8.4 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:11c139c9-f9cc-4b4d-bfff-ffa6a0b5e685&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 218ea50b-ade2-47cc-a248-e9057a868626 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:218ea50b-ade2-47cc-a248-e9057a868626&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 186ec372-61e3-420f-8426-063ef96e7ae5 is DONE. 11.4 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:186ec372-61e3-420f-8426-063ef96e7ae5&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 573ea844-e0e3-4f1c-85a6-57c5712af25b is DONE. 11.4 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:573ea844-e0e3-4f1c-85a6-57c5712af25b&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 65bf7a7d-b288-41fb-bb16-a04033ecf84c is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:65bf7a7d-b288-41fb-bb16-a04033ecf84c&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler and imputer applied on validation/test data.\n",
      "Columns to drop due to high nulls: []\n",
      "Features after dropping high null columns: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "Dataframe prepared successfully. Returning split.\n",
      "Preparing dataframe (is_train=False)...\n",
      "Columns to drop (after filtering): []\n",
      "WARNING: No target specified. Using 'uploadThroughput' as target.\n",
      "Initial features: ['downloadThroughput', 'downloadLatency']\n",
      "Encoding categorical columns: ['eventType', 'eventSource', 'eventCategory', 'eventPublisherId', 'wtn', 'serialNumber', 'productClass', 'downloadTestStatus', 'uploadState', 'uploadTestStatus']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 633f2ed2-f094-4495-a597-ef40d4996bd8 is DONE. 44.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:633f2ed2-f094-4495-a597-ef40d4996bd8&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 42c8f595-b7ec-4972-968d-52b07eb0cbd7 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:42c8f595-b7ec-4972-968d-52b07eb0cbd7&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After datetime feature engineering, columns: ['eventId', 'downloadThroughput', 'downloadLatency', 'uploadThroughput', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "Final features: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 02f51194-e9a0-447b-b6dc-db9c3cfebf4e is DONE. 15.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:02f51194-e9a0-447b-b6dc-db9c3cfebf4e&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 188bc659-eef4-43e5-8270-56e7dad43417 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:188bc659-eef4-43e5-8270-56e7dad43417&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 47822893-4d2b-4e4e-a20d-70a888e08154 is DONE. 22.1 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:47822893-4d2b-4e4e-a20d-70a888e08154&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job d06222ac-9e9b-412c-ac63-0f4493e75b8e is DONE. 22.1 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:d06222ac-9e9b-412c-ac63-0f4493e75b8e&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load job 162cac26-debe-434b-b2b5-2f7a26facd04 is DONE. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:162cac26-debe-434b-b2b5-2f7a26facd04&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler and imputer applied on validation/test data.\n",
      "Columns to drop due to high nulls: []\n",
      "Features after dropping high null columns: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "Dataframe prepared successfully. Returning split.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job dff8557f-605a-4da0-85f3-1a090965f4e9 is DONE. 155.1 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:dff8557f-605a-4da0-85f3-1a090965f4e9&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 47eb6ee1-5284-4eff-b291-63e38813c867 is DONE. 12.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:47eb6ee1-5284-4eff-b291-63e38813c867&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 3fa198b8-602c-4a83-bdfc-2ca193c21d94 is DONE. 24.8 kB processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=massmkt-poc&j=bq:US:3fa198b8-602c-4a83-bdfc-2ca193c21d94&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF Shape: (194, 88)\n",
      "Training Features: ['downloadThroughput', 'downloadLatency', 'eventType_Create', 'eventSource_PC360', 'eventCategory_PC360_Event', 'eventPublisherId_PC360', 'wtn_1000247195', 'wtn_1000303624', 'wtn_1000332141', 'wtn_1100005591', 'wtn_1100020488', 'wtn_1100020668', 'wtn_1100020705', 'wtn_1100020834', 'wtn_1100021198', 'wtn_1500002566', 'wtn_1500017644', 'wtn_1500022989', 'wtn_1500035626', 'wtn_1500036835', 'wtn_1500040944', 'wtn_1500042692', 'wtn_1500056528', 'wtn_1500063674', 'wtn_1500067899', 'wtn_1500069551', 'wtn_1500069582', 'wtn_1500071356', 'wtn_1500071635', 'wtn_1500081777', 'wtn_1500102184', 'wtn_1500118760', 'wtn_1500119464', 'wtn_1500158163', 'wtn_1500160533', 'wtn_1500191274', 'wtn_1500198761', 'wtn_1500241380', 'wtn_1500280766', 'wtn_1500281624', 'wtn_1500316464', 'wtn_1500339563', 'wtn_6238761000', 'serialNumber_C4000BZS210Z48005293', 'serialNumber_C4000BZS220Z37010089', 'serialNumber_C4000XG2007055551', 'serialNumber_C4000XG2044189633', 'serialNumber_C4000XG2112228810', 'serialNumber_C4000XG2134319195', 'serialNumber_C4000XG2152423452', 'serialNumber_C5500XK2139001380', 'serialNumber_C5500XK2139001949', 'serialNumber_C5500XK2139008951', 'serialNumber_C5500XK2140000541', 'serialNumber_C5500XK2143002375', 'serialNumber_C5500XK2144002788', 'serialNumber_C5500XK2148005097', 'serialNumber_C5500XK2150006116', 'serialNumber_C5500XK2150006665', 'serialNumber_C5500XK2152006531', 'serialNumber_C5500XK2207008417', 'serialNumber_C5500XK2216004759', 'serialNumber_C5500XK2226007139', 'serialNumber_C5500XK2229002102', 'serialNumber_C5500XK2231000609', 'serialNumber_C5500XK2231001385', 'serialNumber_C5500XK2239006644', 'serialNumber_C5500XK2241007228', 'serialNumber_C6500XK2216000221', 'serialNumber_C6500XK2216000824', 'serialNumber_C6500XK2249004013', 'serialNumber_C6500XK2250003149', 'productClass_C4000BZ', 'productClass_C4000LZ', 'productClass_C4000XG', 'productClass_C5500XK', 'productClass_C6500XK', 'downloadTestStatus_Download_Test_Failed', 'downloadTestStatus_Success', 'downloadTestStatus_TCP_errors__TCP_retransmits__or_high_cross_traffic_exists_on_the_line_Download_side', 'uploadState_Completed', 'uploadState_Error_Internal', 'uploadState_Error_NoTransferComplete', 'uploadState_Error_Timeout', 'uploadTestStatus_Success', 'uploadTestStatus_Upload_Test_Failed']\n",
      "BigFrames version: 1.38.0\n",
      "Pandas version: 2.2.3\n",
      "Scaler type: <class 'bigframes.ml.preprocessing.MaxAbsScaler'>\n"
     ]
    }
   ],
   "source": [
    "import bigframes.pandas as bf\n",
    "from sklearn.preprocessing import StandardScaler  # Will be replaced with BigFrames scaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import pandas as pd  # For struct flattening and one-hot encoding\n",
    "import warnings\n",
    "import numpy as np  # For random number generation\n",
    "from bigframes.ml.preprocessing import MaxAbsScaler  # BigFrames scaler\n",
    "import gc  # For memory management\n",
    "\n",
    "# --- Imports for corrected type checking ---\n",
    "import pyarrow.types as pat  # Explicit PyArrow types import\n",
    "# --- End corrected type checking imports ---\n",
    "PROJECT_ID = \"massmkt-poc\"  # @param {type:\"string\"}\n",
    "REGION = \"US\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set BigQuery DataFrames options\n",
    "# Note: The project option is not required in all environments.\n",
    "# On BigQuery Studio, the project ID is automatically detected.\n",
    "bf.options.bigquery.project = PROJECT_ID\n",
    "\n",
    "# Note: The location option is not required.\n",
    "# It defaults to the location of the first table or query\n",
    "# passed to read_gbq(). For APIs where a location can't be\n",
    "# auto-detected, the location defaults to the \"US\" location.\n",
    "bf.options.bigquery.location = REGION\n",
    "def prepare_bigquery_data_bf(\n",
    "    project_id: str,\n",
    "    table_id: str,\n",
    "    target_column: Optional[str] = None,\n",
    "    force_categorical_columns: List[str] = None,\n",
    "    drop_columns: List[str] = None,\n",
    "    high_null_threshold: float = 0.9,\n",
    "    limit: Optional[int] = None,\n",
    "    index_col: Optional[str] = None,\n",
    ") -> Tuple[bf.DataFrame, bf.DataFrame, bf.DataFrame, List[str], List[str], List[str], MaxAbsScaler, SimpleImputer]:\n",
    "    \"\"\"\n",
    "    Prepares data from a BigQuery table using BigFrames.\n",
    "\n",
    "    Args:\n",
    "        project_id: GCP project ID.\n",
    "        table_id: BigQuery table ID (dataset.table).\n",
    "        target_column: Target variable name. Heuristics if None.\n",
    "        force_categorical_columns: Always treat as categorical.\n",
    "        drop_columns: Columns to drop.\n",
    "        high_null_threshold: Drop columns with > this % nulls (after encoding).\n",
    "        limit: Optional row limit for testing.\n",
    "        index_col: Optional index column.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (train_bf_df, val_bf_df, test_bf_df, train_features, val_features, test_features, scaler, imputer)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Session and Memory Cleanup ---\n",
    "    bf.close_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # Set fixed random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    bf.options.bigquery.project = project_id\n",
    "\n",
    "    if force_categorical_columns is None:\n",
    "        force_categorical_columns = []\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "\n",
    "    bf_df = bf.read_gbq(table_id, index_col=index_col)\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Data Type Conversion ---\n",
    "    numeric_columns = ['downloadThroughput', 'downloadLatency', 'uploadThroughput']\n",
    "    for col in numeric_columns:\n",
    "        if col in bf_df.columns:\n",
    "            bf_df[col] = bf_df[col].astype('Float64')\n",
    "    if 'eventTimestamp' in bf_df.columns:\n",
    "        bf_df['eventTimestamp'] = bf_df['eventTimestamp'].astype('datetime64[ns]')\n",
    "    # --- End data type conversion ---\n",
    "\n",
    "    if limit:\n",
    "        bf_df = bf_df.head(limit)\n",
    "    print(\"Schema (bf_df.dtypes):\", bf_df.dtypes)\n",
    "\n",
    "    # --- Robust Numeric and Datetime Type Detection ---\n",
    "    def is_numeric_type(col_type) -> bool:\n",
    "        try:\n",
    "            if pd.api.types.is_numeric_dtype(col_type):\n",
    "                return True\n",
    "            if hasattr(col_type, \"id\"):\n",
    "                return pat.is_integer(col_type) or pat.is_floating(col_type)\n",
    "            return False\n",
    "        except TypeError as e:\n",
    "            print(f\"Error checking numeric type for {col_type}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_datetime_type(col_type) -> bool:\n",
    "        try:\n",
    "            if pd.api.types.is_datetime64_any_dtype(col_type):\n",
    "                return True\n",
    "            if hasattr(col_type, \"id\"):\n",
    "                return pat.is_timestamp(col_type) or pat.is_date(col_type)\n",
    "            return False\n",
    "        except TypeError as e:\n",
    "            print(f\"Error checking datetime type for {col_type}: {e}\")\n",
    "            return False\n",
    "\n",
    "    schema = bf_df.dtypes\n",
    "    numeric_cols: List[str] = []\n",
    "    datetime_cols: List[str] = []\n",
    "    categorical_cols: List[str] = []\n",
    "    struct_cols: List[str] = []\n",
    "    boolean_cols: List[str] = []\n",
    "\n",
    "    for col_name, col_type in schema.items():\n",
    "        if is_numeric_type(col_type):\n",
    "            numeric_cols.append(col_name)\n",
    "        elif is_datetime_type(col_type):\n",
    "            datetime_cols.append(col_name)\n",
    "        elif hasattr(bf, 'BooleanDtype') and col_type == bf.BooleanDtype():\n",
    "            boolean_cols.append(col_name)\n",
    "        elif hasattr(bf, 'StringDtype') and (col_type == bf.StringDtype() or col_name in force_categorical_columns):\n",
    "            categorical_cols.append(col_name)\n",
    "        elif \"STRUCT\" in str(col_type).upper():\n",
    "            struct_cols.append(col_name)\n",
    "\n",
    "    # Modify prepare_dataframe to accept fixed_dummy_columns\n",
    "    def prepare_dataframe(bf_df_split, is_train=False, scaler=None, imputer=None,\n",
    "                          high_null_cols_train=None,\n",
    "                          fixed_dummy_columns: Optional[List[str]] = None\n",
    "                         ) -> Optional[Tuple[bf.DataFrame, List[str], MaxAbsScaler, SimpleImputer, List[str], Optional[List[str]]]]:\n",
    "        try:\n",
    "            print(f\"Preparing dataframe (is_train={is_train})...\")\n",
    "\n",
    "            # Filter drop_columns if any\n",
    "            cols_to_drop = [col for col in drop_columns if col in bf_df_split.columns]\n",
    "            print(f\"Columns to drop (after filtering): {cols_to_drop}\")\n",
    "            if cols_to_drop:\n",
    "                bf_df_split = bf_df_split.drop(columns=cols_to_drop)\n",
    "\n",
    "            # Flatten STRUCT columns\n",
    "            for struct_col in struct_cols:\n",
    "                if struct_col in bf_df_split.columns:\n",
    "                    print(f\"Flattening struct column: {struct_col}\")\n",
    "                    bf_df_split = bf_df_split.ml.flatten(struct_col)\n",
    "\n",
    "            # Convert datetime columns\n",
    "            for col in datetime_cols:\n",
    "                if col in bf_df_split.columns:\n",
    "                    print(f\"Converting to timestamp: {col}\")\n",
    "                    bf_df_split[col] = bf_df_split[col].to_timestamp()\n",
    "\n",
    "            # Determine target column\n",
    "            _target_column = target_column\n",
    "            if _target_column is None:\n",
    "                eligible = [col for col in numeric_cols if col in bf_df_split.columns and col not in categorical_cols]\n",
    "                if eligible:\n",
    "                    _target_column = eligible[-1]\n",
    "                    print(f\"WARNING: No target specified. Using '{_target_column}' as target.\")\n",
    "                else:\n",
    "                    raise ValueError(\"No suitable target column found.\")\n",
    "\n",
    "            # Initial features: numeric (excluding target) plus boolean columns\n",
    "            features = [col for col in numeric_cols if col != _target_column and col in bf_df_split.columns]\n",
    "            features.extend(boolean_cols)\n",
    "            print(f\"Initial features: {features}\")\n",
    "\n",
    "            # One-hot encode categorical columns\n",
    "            if categorical_cols:\n",
    "                print(f\"Encoding categorical columns: {categorical_cols}\")\n",
    "                df_pd = bf_df_split.to_pandas()\n",
    "                df_dummy = pd.get_dummies(df_pd, columns=categorical_cols, drop_first=False)\n",
    "                df_dummy.columns = df_dummy.columns.str.replace(r'[^0-9a-zA-Z_]', '_', regex=True)\n",
    "                if is_train:\n",
    "                    fixed_dummy = list(df_dummy.columns)\n",
    "                else:\n",
    "                    if fixed_dummy_columns is not None:\n",
    "                        df_dummy = df_dummy.reindex(columns=fixed_dummy_columns, fill_value=0)\n",
    "                    fixed_dummy = fixed_dummy_columns\n",
    "                bf_df_split = bf.DataFrame(df_dummy)\n",
    "            else:\n",
    "                print(\"No categorical columns to encode.\")\n",
    "                fixed_dummy = fixed_dummy_columns\n",
    "\n",
    "            # Add cyclical features for datetime columns\n",
    "            for col in datetime_cols:\n",
    "                if col in bf_df_split.columns:\n",
    "                    print(f\"Adding cyclical features for: {col}\")\n",
    "                    bf_df_split[col + '_sin_hour'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.hour / 23.0)\n",
    "                    bf_df_split[col + '_cos_hour'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.hour / 23.0)\n",
    "                    bf_df_split[col + '_sin_dayofweek'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.dayofweek / 6.0)\n",
    "                    bf_df_split[col + '_cos_dayofweek'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.dayofweek / 6.0)\n",
    "                    bf_df_split[col + '_sin_dayofmonth'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.day / 30.0)\n",
    "                    bf_df_split[col + '_cos_dayofmonth'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.day / 30.0)\n",
    "                    bf_df_split[col + '_sin_month'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.month / 11.0)\n",
    "                    bf_df_split[col + '_cos_month'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.month / 11.0)\n",
    "\n",
    "            print(f\"After datetime feature engineering, columns: {bf_df_split.columns.tolist()}\")\n",
    "\n",
    "            # Re-assess numeric columns\n",
    "            current_numeric = [col for col, dtype in bf_df_split.dtypes.items() if is_numeric_type(dtype)]\n",
    "            features = [col for col in current_numeric if col != _target_column and col in bf_df_split.columns]\n",
    "            print(f\"Final features: {features}\")\n",
    "\n",
    "            # Drop high-null columns (if any)\n",
    "            high_null_cols_to_drop_list = []\n",
    "            if is_train:\n",
    "                train_pd = bf_df_split[features + [_target_column]].to_pandas()\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                train_pd[features] = imputer.fit_transform(train_pd[features])\n",
    "                scaler = MaxAbsScaler()\n",
    "                train_pd[features] = scaler.fit_transform(train_pd[features])\n",
    "                # Update numeric features: drop then join updated ones.\n",
    "                updated_numeric = bf.DataFrame(train_pd[features], index=train_pd.index)\n",
    "                bf_df_split = bf_df_split.drop(columns=features).join(updated_numeric)\n",
    "                print(\"Scaler and imputer fitted and applied on training data.\")\n",
    "            else:\n",
    "                if high_null_cols_train is None:\n",
    "                    raise ValueError(\"High null columns from training must be provided for validation/test sets.\")\n",
    "                high_null_cols_to_drop_list = high_null_cols_train\n",
    "                temp_pd = bf_df_split[features + [_target_column]].to_pandas()\n",
    "                temp_pd[features] = imputer.transform(temp_pd[features])\n",
    "                temp_pd[features] = scaler.transform(temp_pd[features])\n",
    "                updated_numeric = bf.DataFrame(temp_pd[features], index=temp_pd.index)\n",
    "                bf_df_split = bf_df_split.drop(columns=features).join(updated_numeric)\n",
    "                print(\"Scaler and imputer applied on validation/test data.\")\n",
    "\n",
    "            # (Optional) Drop any high-null columns\n",
    "            high_null_cols_to_drop_final = [col for col in high_null_cols_to_drop_list if col in bf_df_split.columns]\n",
    "            print(f\"Columns to drop due to high nulls: {high_null_cols_to_drop_final}\")\n",
    "            if high_null_cols_to_drop_final:\n",
    "                bf_df_split = bf_df_split.drop(columns=high_null_cols_to_drop_final)\n",
    "            features = [col for col in features if col not in high_null_cols_to_drop_final]\n",
    "            print(f\"Features after dropping high null columns: {features}\")\n",
    "\n",
    "            gc.collect()\n",
    "            print(\"Dataframe prepared successfully. Returning split.\")\n",
    "            return bf_df_split, features, scaler, imputer, high_null_cols_to_drop_final, fixed_dummy\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in prepare_dataframe (is_train={is_train}): {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    # --- Split Data ---\n",
    "    if index_col and pd.api.types.is_datetime64_any_dtype(bf_df.index):\n",
    "        print(\"Performing temporal split...\")\n",
    "        split_date = bf_df.index.quantile(0.8)\n",
    "        train_bf_df = bf_df[bf_df.index <= split_date]\n",
    "        temp_bf_df = bf_df[bf_df.index > split_date]\n",
    "        print(f\"Temporal split date: {split_date}\")\n",
    "    else:\n",
    "        print(\"Performing random split...\")\n",
    "        bf_df['__split_rand'] = bf.Series(np.random.rand(len(bf_df.index)).tolist(), index=bf_df.index)\n",
    "        train_bf_df = bf_df[bf_df['__split_rand'] <= 0.8].drop(columns=['__split_rand'])\n",
    "        temp_bf_df = bf_df[(bf_df['__split_rand'] > 0.8) & (bf_df['__split_rand'] <= 1.0)]\n",
    "        print(\"Random split ratios: 80/20 (train/temp)\")\n",
    "\n",
    "    # Manually split temp_bf_df into validation and test sets.\n",
    "    temp_bf_df['__split_rand2'] = bf.Series(np.random.rand(len(temp_bf_df.index)).tolist(), index=temp_bf_df.index)\n",
    "    val_bf_df = temp_bf_df[temp_bf_df['__split_rand2'] <= 0.5].drop(columns=['__split_rand2'])\n",
    "    test_bf_df = temp_bf_df[temp_bf_df['__split_rand2'] > 0.5].drop(columns=['__split_rand2'])\n",
    "    if '__split_rand' in temp_bf_df.columns:\n",
    "        temp_bf_df = temp_bf_df.drop(columns=['__split_rand'])\n",
    "    print(\"Validation/test split ratios: 50/50 from temp\")\n",
    "\n",
    "    # --- Process Splits ---\n",
    "    res_train = prepare_dataframe(train_bf_df, is_train=True)\n",
    "    if res_train is None:\n",
    "        return None\n",
    "    train_bf_df_processed, train_features, scaler, imputer, high_null_cols_train, fixed_dummy_columns = res_train\n",
    "\n",
    "    res_val = prepare_dataframe(val_bf_df, is_train=False, scaler=scaler, imputer=imputer, high_null_cols_train=high_null_cols_train, fixed_dummy_columns=fixed_dummy_columns)\n",
    "    if res_val is None:\n",
    "        return None\n",
    "    val_bf_df_processed, val_features, _, _, _, _ = res_val\n",
    "\n",
    "    res_test = prepare_dataframe(test_bf_df, is_train=False, scaler=scaler, imputer=imputer, high_null_cols_train=high_null_cols_train, fixed_dummy_columns=fixed_dummy_columns)\n",
    "    if res_test is None:\n",
    "        return None\n",
    "    test_bf_df_processed, test_features, _, _, _, _ = res_test\n",
    "\n",
    "    gc.collect()\n",
    "    return train_bf_df_processed, val_bf_df_processed, test_bf_df_processed, train_features, val_features, test_features, scaler, imputer\n",
    "\n",
    "\n",
    "def flatten_struct(df: pd.DataFrame, struct_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flattens a simple struct column in a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    struct_df = pd.json_normalize(df[struct_col_name])\n",
    "    struct_df = struct_df.add_prefix(f\"{struct_col_name}_\")\n",
    "    df = pd.concat([df.drop(columns=[struct_col_name]), struct_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "project_id = \"massmkt-poc\"\n",
    "table_id = \"TEST1.ctl_modem_speedtest_event\"\n",
    "result = prepare_bigquery_data_bf(\n",
    "    project_id,\n",
    "    table_id,\n",
    "    # target_column=\"downloadThroughput\",  # Optional\n",
    "    force_categorical_columns=[\n",
    "        \"eventType\", \"eventSource\", \"eventCategory\", \"eventPublisherId\",\n",
    "        \"productClass\", \"downloadTestStatus\", \"uploadState\", \"uploadTestStatus\",\n",
    "        \"wtn\", \"serialNumber\"\n",
    "    ],\n",
    "    high_null_threshold=0.95,\n",
    "    limit=1000,  # For testing\n",
    "    index_col=\"eventTimestamp\"  # Use datetime index for temporal split\n",
    ")\n",
    "\n",
    "if result is not None:\n",
    "    train_bf_df, val_bf_df, test_bf_df, train_features, val_features, test_features, scaler, imputer = result\n",
    "\n",
    "    # Convert to pandas DataFrames only when necessary\n",
    "    train_df = train_bf_df.to_pandas()\n",
    "    val_df = val_bf_df.to_pandas()\n",
    "    test_df = test_bf_df.to_pandas()\n",
    "\n",
    "    print(\"Train DF Shape:\", train_df.shape)\n",
    "    print(\"Training Features:\", train_features)\n",
    "    print(f\"BigFrames version: {bf.__version__}\")\n",
    "    print(f\"Pandas version: {pd.__version__}\")\n",
    "    print(f\"Scaler type: {type(scaler)}\")\n",
    "else:\n",
    "    print(\"Error occurred during data preparation. Check traceback above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Temporal Stride Processing\n",
    "\n",
    "Now let's apply temporal stride processing to the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:31:59,506 - terabyte_feature_extractor - INFO - Using mlx backend for computation\n",
      "2025-04-22 12:31:59,506 - terabyte_feature_extractor - INFO - Initialized TerabyteTemporalStrideProcessor with window_size=10, stride_perspectives=[1, 3, 5], batch_size=10000\n",
      "2025-04-22 12:31:59,511 - terabyte_feature_extractor - INFO - Processing batch 1 with 194 rows\n",
      "2025-04-22 12:31:59,515 - ember_ml.utils.backend - WARNING - Error converting to tensor: Invalid type  ndarray received in array initialization.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid type  ndarray received in array initialization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Process data - make sure train_df and train_features are defined\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_features) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     stride_perspectives \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_large_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Print stride perspective shapes\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stride, data \u001b[38;5;129;01min\u001b[39;00m stride_perspectives\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/nn/features/terabyte_feature_extractor.py:894\u001b[0m, in \u001b[0;36mTerabyteTemporalStrideProcessor.process_large_dataset\u001b[0;34m(self, data_generator, maintain_state)\u001b[0m\n\u001b[1;32m    891\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# Convert batch data to tensor\u001b[39;00m\n\u001b[0;32m--> 894\u001b[0m batch_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;66;03m# If state buffer exists and maintain_state is True, prepend to current batch\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maintain_state \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/utils/backend_utils.py:101\u001b[0m, in \u001b[0;36mconvert_to_tensor_safe\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    103\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError converting to tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/nn/tensor/__init__.py:81\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(data, dtype, device, requires_grad)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Create and return an EmberTensor instance.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# The EmberTensor.__init__ method is responsible for handling the backend conversion,\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# dtype setting, device placement, and storing the backend tensor and EmberDType.\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEmberTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/nn/tensor/common/ember_tensor.py:87\u001b[0m, in \u001b[0;36mEmberTensor.__init__\u001b[0;34m(self, data, dtype, device, requires_grad)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# Otherwise, use it as is (assuming it's a DType or compatible)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         processed_dtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_backend_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Use the device_ops to get the default device\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Import get_backend_module directly for reliable access during init\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01member_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_backend_module, get_backend\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/nn/tensor/common/__init__.py:75\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m slice_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _get_backend_tensor_ops_module()\u001b[38;5;241m.\u001b[39mslice_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Rename the current function to indicate it's internal\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Dynamically get the correct convert_to_<backend>_tensor function\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m _convert_to_backend_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Dynamically import the backend's utility module\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43member_ml.backend.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.tensor.ops.utility\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Call the standardized internal function name\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_convert_to_tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# _convert_to_backend_tensor = lambda *args, **kwargs: _get_backend_tensor_ops_module().convert_to_tensor(*args, **kwargs) # Placeholder comment\u001b[39;00m\n\u001b[1;32m     82\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _get_backend_tensor_ops_module()\u001b[38;5;241m.\u001b[39mshape(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/backend/mlx/tensor/ops/utility.py:245\u001b[0m, in \u001b[0;36m_convert_to_tensor\u001b[0;34m(data, dtype, device)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mConvert input to MLX array with specific dtype handling.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    MLX array\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Initial conversion using the refined _convert_input\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m current_mlx_dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Validate and get the target MLX dtype\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/stuff/Projects/ember-ml/ember_ml/backend/mlx/tensor/ops/utility.py:164\u001b[0m, in \u001b[0;36m_convert_input\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mx\u001b[38;5;241m.\u001b[39marray(x, dtype\u001b[38;5;241m=\u001b[39mmx\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# Let MLX handle other numpy dtype conversions\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Handle NumPy scalar types using hasattr\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;66;03m# Check for item method common to numpy scalars\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__class__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(x\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     x\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid type  ndarray received in array initialization."
     ]
    }
   ],
   "source": [
    "from ember_ml.models.stride_aware_cfc import (\n",
    "    create_liquid_network_with_motor_neuron,\n",
    "    create_lstm_gated_liquid_network,\n",
    "    create_multi_stride_liquid_network\n",
    ")\n",
    "# Create temporal processor\n",
    "temporal_processor = TerabyteTemporalStrideProcessor(\n",
    "    window_size=10,\n",
    "    stride_perspectives=[1, 3, 5],\n",
    "    pca_components=32,\n",
    "    batch_size=10000,\n",
    "    use_incremental_pca=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define a generator to yield data in batches\n",
    "def data_generator(df, features, batch_size=10000):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size][features].values\n",
    "\n",
    "# Process data - make sure train_df and train_features are defined\n",
    "if len(train_df) > 0 and len(train_features) > 0:\n",
    "    stride_perspectives = temporal_processor.process_large_dataset(\n",
    "        data_generator(train_df, train_features, batch_size=10000)\n",
    "    )\n",
    "\n",
    "    # Print stride perspective shapes\n",
    "    for stride, data in stride_perspectives.items():\n",
    "        print(f\"Stride {stride}: shape {data.shape}\")\n",
    "    \n",
    "    # Visualize explained variance for each stride\n",
    "    explained_variances = [temporal_processor.get_explained_variance(stride) for stride in stride_perspectives.keys()]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(stride_perspectives.keys(), explained_variances)\n",
    "    plt.xlabel('Stride Length')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Stride Length')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot process data: train_df or train_features is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigframes.pandas as bf\n",
    "from sklearn.preprocessing import StandardScaler  # Will be replaced with BigFrames scaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import pandas as pd  # For struct flattening and one-hot encoding\n",
    "import warnings\n",
    "import numpy as np  # For random number generation\n",
    "from bigframes.ml.preprocessing import MaxAbsScaler  # BigFrames scaler\n",
    "import gc  # For memory management\n",
    "\n",
    "# --- Imports for corrected type checking ---\n",
    "import pyarrow.types as pat  # Explicit PyArrow types import\n",
    "# --- End corrected type checking imports ---\n",
    "\n",
    "PROJECT_ID = \"massmkt-poc\"  # @param {type:\"string\"}\n",
    "REGION = \"US\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set BigQuery DataFrames options\n",
    "# Note: The project option is not required in all environments.\n",
    "# On BigQuery Studio, the project ID is automatically detected.\n",
    "bf.options.bigquery.project = PROJECT_ID\n",
    "\n",
    "# Note: The location option is not required.\n",
    "# It defaults to the location of the first table or query\n",
    "# passed to read_gbq(). For APIs where a location can't be\n",
    "# auto-detected, the location defaults to the \"US\" location.\n",
    "bf.options.bigquery.location = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bigquery_data_bf(\n",
    "    project_id: str,\n",
    "    table_id: str,\n",
    "    target_column: Optional[str] = None,\n",
    "    force_categorical_columns: List[str] = None,\n",
    "    drop_columns: List[str] = None,\n",
    "    high_null_threshold: float = 0.9,\n",
    "    limit: Optional[int] = None,\n",
    "    index_col: Optional[str] = None,\n",
    ") -> Tuple[bf.DataFrame, bf.DataFrame, bf.DataFrame, List[str], List[str], List[str], MaxAbsScaler, SimpleImputer]:\n",
    "    \"\"\"\n",
    "    Prepares data from a BigQuery table using BigFrames.\n",
    "\n",
    "    Args:\n",
    "        project_id: GCP project ID.\n",
    "        table_id: BigQuery table ID (dataset.table).\n",
    "        target_column: Target variable name. Heuristics if None.\n",
    "        force_categorical_columns: Always treat as categorical.\n",
    "        drop_columns: Columns to drop.\n",
    "        high_null_threshold: Drop columns with > this % nulls (after encoding).\n",
    "        limit: Optional row limit for testing.\n",
    "        index_col: Optional index column.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (train_bf_df, val_bf_df, test_bf_df, train_features, val_features, test_features, scaler, imputer)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Session and Memory Cleanup ---\n",
    "    bf.close_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # Set fixed random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    bf.options.bigquery.project = project_id\n",
    "\n",
    "    if force_categorical_columns is None:\n",
    "        force_categorical_columns = []\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "\n",
    "    bf_df = bf.read_gbq(table_id, index_col=index_col)\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Data Type Conversion ---\n",
    "    numeric_columns = ['downloadThroughput', 'downloadLatency', 'uploadThroughput']\n",
    "    for col in numeric_columns:\n",
    "        if col in bf_df.columns:\n",
    "            bf_df[col] = bf_df[col].astype('Float64')\n",
    "    if 'eventTimestamp' in bf_df.columns:\n",
    "        bf_df['eventTimestamp'] = bf_df['eventTimestamp'].astype('datetime64[ns]')\n",
    "    # --- End data type conversion ---\n",
    "\n",
    "    if limit:\n",
    "        bf_df = bf_df.head(limit)\n",
    "    print(\"Schema (bf_df.dtypes):\", bf_df.dtypes)\n",
    "\n",
    "    # --- Robust Numeric and Datetime Type Detection ---\n",
    "    def is_numeric_type(col_type) -> bool:\n",
    "        try:\n",
    "            if pd.api.types.is_numeric_dtype(col_type):\n",
    "                return True\n",
    "            if hasattr(col_type, \"id\"):\n",
    "                return pat.is_integer(col_type) or pat.is_floating(col_type)\n",
    "            return False\n",
    "        except TypeError as e:\n",
    "            print(f\"Error checking numeric type for {col_type}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def is_datetime_type(col_type) -> bool:\n",
    "        try:\n",
    "            if pd.api.types.is_datetime64_any_dtype(col_type):\n",
    "                return True\n",
    "            if hasattr(col_type, \"id\"):\n",
    "                return pat.is_timestamp(col_type) or pat.is_date(col_type)\n",
    "            return False\n",
    "        except TypeError as e:\n",
    "            print(f\"Error checking datetime type for {col_type}: {e}\")\n",
    "            return False\n",
    "\n",
    "    schema = bf_df.dtypes\n",
    "    numeric_cols: List[str] = []\n",
    "    datetime_cols: List[str] = []\n",
    "    categorical_cols: List[str] = []\n",
    "    struct_cols: List[str] = []\n",
    "    boolean_cols: List[str] = []\n",
    "\n",
    "    for col_name, col_type in schema.items():\n",
    "        if is_numeric_type(col_type):\n",
    "            numeric_cols.append(col_name)\n",
    "        elif is_datetime_type(col_type):\n",
    "            datetime_cols.append(col_name)\n",
    "        elif hasattr(bf, 'BooleanDtype') and col_type == bf.BooleanDtype():\n",
    "            boolean_cols.append(col_name)\n",
    "        elif hasattr(bf, 'StringDtype') and (col_type == bf.StringDtype() or col_name in force_categorical_columns):\n",
    "            categorical_cols.append(col_name)\n",
    "        elif \"STRUCT\" in str(col_type).upper():\n",
    "            struct_cols.append(col_name)\n",
    "\n",
    "    # Modify prepare_dataframe to accept fixed_dummy_columns\n",
    "    def prepare_dataframe(bf_df_split, is_train=False, scaler=None, imputer=None,\n",
    "                          high_null_cols_train=None,\n",
    "                          fixed_dummy_columns: Optional[List[str]] = None\n",
    "                         ) -> Optional[Tuple[bf.DataFrame, List[str], MaxAbsScaler, SimpleImputer, List[str], Optional[List[str]]]]:\n",
    "        try:\n",
    "            print(f\"Preparing dataframe (is_train={is_train})...\")\n",
    "\n",
    "            # Filter drop_columns if any\n",
    "            cols_to_drop = [col for col in drop_columns if col in bf_df_split.columns]\n",
    "            print(f\"Columns to drop (after filtering): {cols_to_drop}\")\n",
    "            if cols_to_drop:\n",
    "                bf_df_split = bf_df_split.drop(columns=cols_to_drop)\n",
    "\n",
    "            # Flatten STRUCT columns\n",
    "            for struct_col in struct_cols:\n",
    "                if struct_col in bf_df_split.columns:\n",
    "                    print(f\"Flattening struct column: {struct_col}\")\n",
    "                    bf_df_split = bf_df_split.ml.flatten(struct_col)\n",
    "\n",
    "            # Convert datetime columns\n",
    "            for col in datetime_cols:\n",
    "                if col in bf_df_split.columns:\n",
    "                    print(f\"Converting to timestamp: {col}\")\n",
    "                    bf_df_split[col] = bf_df_split[col].to_timestamp()\n",
    "\n",
    "            # Determine target column\n",
    "            _target_column = target_column\n",
    "            if _target_column is None:\n",
    "                eligible = [col for col in numeric_cols if col in bf_df_split.columns and col not in categorical_cols]\n",
    "                if eligible:\n",
    "                    _target_column = eligible[-1]\n",
    "                    print(f\"WARNING: No target specified. Using '{_target_column}' as target.\")\n",
    "                else:\n",
    "                    raise ValueError(\"No suitable target column found.\")\n",
    "\n",
    "            # Initial features: numeric (excluding target) plus boolean columns\n",
    "            features = [col for col in numeric_cols if col != _target_column and col in bf_df_split.columns]\n",
    "            features.extend(boolean_cols)\n",
    "            print(f\"Initial features: {features}\")\n",
    "\n",
    "            # One-hot encode categorical columns\n",
    "            if categorical_cols:\n",
    "                print(f\"Encoding categorical columns: {categorical_cols}\")\n",
    "                df_pd = bf_df_split.to_pandas()\n",
    "                df_dummy = pd.get_dummies(df_pd, columns=categorical_cols, drop_first=False)\n",
    "                df_dummy.columns = df_dummy.columns.str.replace(r'[^0-9a-zA-Z_]', '_', regex=True)\n",
    "                if is_train:\n",
    "                    fixed_dummy = list(df_dummy.columns)\n",
    "                else:\n",
    "                    if fixed_dummy_columns is not None:\n",
    "                        df_dummy = df_dummy.reindex(columns=fixed_dummy_columns, fill_value=0)\n",
    "                    fixed_dummy = fixed_dummy_columns\n",
    "                bf_df_split = bf.DataFrame(df_dummy)\n",
    "            else:\n",
    "                print(\"No categorical columns to encode.\")\n",
    "                fixed_dummy = fixed_dummy_columns\n",
    "\n",
    "            # Add cyclical features for datetime columns\n",
    "            for col in datetime_cols:\n",
    "                if col in bf_df_split.columns:\n",
    "                    print(f\"Adding cyclical features for: {col}\")\n",
    "                    bf_df_split[col + '_sin_hour'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.hour / 23.0)\n",
    "                    bf_df_split[col + '_cos_hour'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.hour / 23.0)\n",
    "                    bf_df_split[col + '_sin_dayofweek'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.dayofweek / 6.0)\n",
    "                    bf_df_split[col + '_cos_dayofweek'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.dayofweek / 6.0)\n",
    "                    bf_df_split[col + '_sin_dayofmonth'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.day / 30.0)\n",
    "                    bf_df_split[col + '_cos_dayofmonth'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.day / 30.0)\n",
    "                    bf_df_split[col + '_sin_month'] = ops.sin(2 * ops.pi * bf_df_split[col].dt.month / 11.0)\n",
    "                    bf_df_split[col + '_cos_month'] = ops.cos(2 * ops.pi * bf_df_split[col].dt.month / 11.0)\n",
    "\n",
    "            print(f\"After datetime feature engineering, columns: {bf_df_split.columns.tolist()}\")\n",
    "\n",
    "            # Re-assess numeric columns\n",
    "            current_numeric = [col for col, dtype in bf_df_split.dtypes.items() if is_numeric_type(dtype)]\n",
    "            features = [col for col in current_numeric if col != _target_column and col in bf_df_split.columns]\n",
    "            print(f\"Final features: {features}\")\n",
    "\n",
    "            # Drop high-null columns (if any)\n",
    "            high_null_cols_to_drop_list = []\n",
    "            if is_train:\n",
    "                train_pd = bf_df_split[features + [_target_column]].to_pandas()\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                train_pd[features] = imputer.fit_transform(train_pd[features])\n",
    "                scaler = MaxAbsScaler()\n",
    "                train_pd[features] = scaler.fit_transform(train_pd[features])\n",
    "                # Update numeric features: drop then join updated ones.\n",
    "                updated_numeric = bf.DataFrame(train_pd[features], index=train_pd.index)\n",
    "                bf_df_split = bf_df_split.drop(columns=features).join(updated_numeric)\n",
    "                print(\"Scaler and imputer fitted and applied on training data.\")\n",
    "            else:\n",
    "                if high_null_cols_train is None:\n",
    "                    raise ValueError(\"High null columns from training must be provided for validation/test sets.\")\n",
    "                high_null_cols_to_drop_list = high_null_cols_train\n",
    "                temp_pd = bf_df_split[features + [_target_column]].to_pandas()\n",
    "                temp_pd[features] = imputer.transform(temp_pd[features])\n",
    "                temp_pd[features] = scaler.transform(temp_pd[features])\n",
    "                updated_numeric = bf.DataFrame(temp_pd[features], index=temp_pd.index)\n",
    "                bf_df_split = bf_df_split.drop(columns=features).join(updated_numeric)\n",
    "                print(\"Scaler and imputer applied on validation/test data.\")\n",
    "\n",
    "            # (Optional) Drop any high-null columns\n",
    "            high_null_cols_to_drop_final = [col for col in high_null_cols_to_drop_list if col in bf_df_split.columns]\n",
    "            print(f\"Columns to drop due to high nulls: {high_null_cols_to_drop_final}\")\n",
    "            if high_null_cols_to_drop_final:\n",
    "                bf_df_split = bf_df_split.drop(columns=high_null_cols_to_drop_final)\n",
    "            features = [col for col in features if col not in high_null_cols_to_drop_final]\n",
    "            print(f\"Features after dropping high null columns: {features}\")\n",
    "\n",
    "            gc.collect()\n",
    "            print(\"Dataframe prepared successfully. Returning split.\")\n",
    "            return bf_df_split, features, scaler, imputer, high_null_cols_to_drop_final, fixed_dummy\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in prepare_dataframe (is_train={is_train}): {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    # --- Split Data ---\n",
    "    if index_col and pd.api.types.is_datetime64_any_dtype(bf_df.index):\n",
    "        print(\"Performing temporal split...\")\n",
    "        split_date = bf_df.index.quantile(0.8)\n",
    "        train_bf_df = bf_df[bf_df.index <= split_date]\n",
    "        temp_bf_df = bf_df[bf_df.index > split_date]\n",
    "        print(f\"Temporal split date: {split_date}\")\n",
    "    else:\n",
    "        print(\"Performing random split...\")\n",
    "        bf_df['__split_rand'] = bf.Series(np.random.rand(len(bf_df.index)).tolist(), index=bf_df.index)\n",
    "        train_bf_df = bf_df[bf_df['__split_rand'] <= 0.8].drop(columns=['__split_rand'])\n",
    "        temp_bf_df = bf_df[(bf_df['__split_rand'] > 0.8) & (bf_df['__split_rand'] <= 1.0)]\n",
    "        print(\"Random split ratios: 80/20 (train/temp)\")\n",
    "\n",
    "    # Manually split temp_bf_df into validation and test sets.\n",
    "    temp_bf_df['__split_rand2'] = bf.Series(np.random.rand(len(temp_bf_df.index)).tolist(), index=temp_bf_df.index)\n",
    "    val_bf_df = temp_bf_df[temp_bf_df['__split_rand2'] <= 0.5].drop(columns=['__split_rand2'])\n",
    "    test_bf_df = temp_bf_df[temp_bf_df['__split_rand2'] > 0.5].drop(columns=['__split_rand2'])\n",
    "    if '__split_rand' in temp_bf_df.columns:\n",
    "        temp_bf_df = temp_bf_df.drop(columns=['__split_rand'])\n",
    "    print(\"Validation/test split ratios: 50/50 from temp\")\n",
    "\n",
    "    # --- Process Splits ---\n",
    "    res_train = prepare_dataframe(train_bf_df, is_train=True)\n",
    "    if res_train is None:\n",
    "        return None\n",
    "    train_bf_df_processed, train_features, scaler, imputer, high_null_cols_train, fixed_dummy_columns = res_train\n",
    "\n",
    "    res_val = prepare_dataframe(val_bf_df, is_train=False, scaler=scaler, imputer=imputer, high_null_cols_train=high_null_cols_train, fixed_dummy_columns=fixed_dummy_columns)\n",
    "    if res_val is None:\n",
    "        return None\n",
    "    val_bf_df_processed, val_features, _, _, _, _ = res_val\n",
    "\n",
    "    res_test = prepare_dataframe(test_bf_df, is_train=False, scaler=scaler, imputer=imputer, high_null_cols_train=high_null_cols_train, fixed_dummy_columns=fixed_dummy_columns)\n",
    "    if res_test is None:\n",
    "        return None\n",
    "    test_bf_df_processed, test_features, _, _, _, _ = res_test\n",
    "\n",
    "    gc.collect()\n",
    "    return train_bf_df_processed, val_bf_df_processed, test_bf_df_processed, train_features, val_features, test_features, scaler, imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_struct(df: pd.DataFrame, struct_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flattens a simple struct column in a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    struct_df = pd.json_normalize(df[struct_col_name])\n",
    "    struct_df = struct_df.add_prefix(f\"{struct_col_name}_\")\n",
    "    df = pd.concat([df.drop(columns=[struct_col_name]), struct_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# --- Example Usage ---\n",
    "project_id = \"massmkt-poc\"\n",
    "table_id = \"TEST1.ctl_modem_speedtest_event\"\n",
    "result = prepare_bigquery_data_bf(\n",
    "    project_id,\n",
    "    table_id,\n",
    "    target_column=\"downloadLatency\",  # Set target column to match the one in the feature extractor\n",
    "    force_categorical_columns=[\n",
    "        \"eventType\", \"eventSource\", \"eventCategory\", \"eventPublisherId\",\n",
    "        \"productClass\", \"downloadTestStatus\", \"uploadState\", \"uploadTestStatus\",\n",
    "        \"wtn\", \"serialNumber\"\n",
    "    ],\n",
    "    high_null_threshold=0.95,\n",
    "    limit=1000,  # For testing\n",
    "    index_col=\"eventTimestamp\"  # Use datetime index for temporal split\n",
    ")\n",
    "\n",
    "if result is not None:\n",
    "    train_bf_df, val_bf_df, test_bf_df, train_features, val_features, test_features, scaler, imputer = result\n",
    "\n",
    "    # Convert to pandas DataFrames only when necessary\n",
    "    train_df = train_bf_df.to_pandas()\n",
    "    val_df = val_bf_df.to_pandas()\n",
    "    test_df = test_bf_df.to_pandas()\n",
    "\n",
    "    print(\"Train DF Shape:\", train_df.shape)\n",
    "    print(\"Training Features:\", train_features)\n",
    "    print(f\"BigFrames version: {bf.__version__}\")\n",
    "    print(f\"Pandas version: {pd.__version__}\")\n",
    "    print(f\"Scaler type: {type(scaler)}\")\n",
    "else:\n",
    "    print(\"Error occurred during data preparation. Check traceback above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Restricted Boltzmann Machine\n",
    "\n",
    "Now let's train an RBM on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RBM\n",
    "if len(train_features) > 0:\n",
    "    rbm = OptimizedRBM(\n",
    "        n_visible=len(train_features),\n",
    "        n_hidden=64,\n",
    "        learning_rate=0.01,\n",
    "        momentum=0.5,\n",
    "        weight_decay=0.0001,\n",
    "        batch_size=100,\n",
    "        use_binary_states=False,\n",
    "        use_gpu=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Define a generator to yield data in batches\n",
    "    def rbm_data_generator(data, features, batch_size=100):\n",
    "        # Shuffle data\n",
    "        indices = tensor.random_permutation(len(data))\n",
    "        data = data.iloc[indices]\n",
    "        \n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield data.iloc[i:i+batch_size][features].values\n",
    "\n",
    "    # Train RBM\n",
    "    training_errors = rbm.train_in_chunks(\n",
    "        rbm_data_generator(train_df, train_features, batch_size=100),\n",
    "        epochs=10,\n",
    "        k=1\n",
    "    )\n",
    "\n",
    "    # Plot training errors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_errors)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title('RBM Training Error')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot train RBM: train_features is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract RBM Features\n",
    "\n",
    "Now let's extract features from the trained RBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator to yield data in batches\n",
    "def feature_generator(data, features, batch_size=1000):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data.iloc[i:i+batch_size][features].values\n",
    "\n",
    "# Extract features if RBM is trained and train_features is not empty\n",
    "if 'rbm' in locals() and len(train_features) > 0:\n",
    "    # Extract features\n",
    "    train_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(train_df, train_features, batch_size=1000)\n",
    "    )\n",
    "\n",
    "    val_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(val_df, val_features, batch_size=1000)\n",
    "    )\n",
    "\n",
    "    test_rbm_features = rbm.transform_in_chunks(\n",
    "        feature_generator(test_df, test_features, batch_size=1000)\n",
    "    )\n",
    "\n",
    "    print(f\"Train RBM features shape: {train_rbm_features.shape}\")\n",
    "    print(f\"Validation RBM features shape: {val_rbm_features.shape}\")\n",
    "    print(f\"Test RBM features shape: {test_rbm_features.shape}\")\n",
    "else:\n",
    "    print(\"Cannot extract RBM features: RBM not trained or train_features is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize RBM Features\n",
    "\n",
    "Let's visualize the RBM features to understand what patterns it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RBM features if they exist\n",
    "if 'train_rbm_features' in locals():\n",
    "    # Visualize RBM feature distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot histograms for first 16 RBM features\n",
    "    for i in range(min(16, train_rbm_features.shape[1])):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.hist(train_rbm_features[:, i], bins=30, alpha=0.7)\n",
    "        plt.title(f'Feature {i+1}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.suptitle('RBM Feature Distributions', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize feature correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr_matrix = np.corrcoef(train_rbm_features, rowvar=False)\n",
    "    plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('RBM Feature Correlations')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot visualize RBM features: train_rbm_features not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Liquid Neural Network\n",
    "\n",
    "Now let's create and train a liquid neural network with the RBM features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train liquid neural network if RBM features exist\n",
    "if 'train_rbm_features' in locals():\n",
    "    # For demonstration, we'll create dummy targets\n",
    "    # In a real application, you would use actual targets from your data\n",
    "    train_targets = np.random.rand(len(train_rbm_features), 1)\n",
    "    val_targets = np.random.rand(len(val_rbm_features), 1)\n",
    "    test_targets = np.random.rand(len(test_rbm_features), 1)\n",
    "\n",
    "    # Reshape RBM features for sequence input\n",
    "    train_rbm_seq = train_rbm_features.reshape(train_rbm_features.shape[0], 1, train_rbm_features.shape[1])\n",
    "    val_rbm_seq = val_rbm_features.reshape(val_rbm_features.shape[0], 1, val_rbm_features.shape[1])\n",
    "    test_rbm_seq = test_rbm_features.reshape(test_rbm_features.shape[0], 1, test_rbm_features.shape[1])\n",
    "\n",
    "    # Create liquid neural network\n",
    "    liquid_network = create_liquid_network_with_motor_neuron(\n",
    "        input_dim=train_rbm_features.shape[1],\n",
    "        units=128,\n",
    "        output_dim=1,\n",
    "        sparsity_level=0.5,\n",
    "        stride_length=1,\n",
    "        time_scale_factor=1.0,\n",
    "        threshold=0.5,\n",
    "        adaptive_threshold=True,\n",
    "        mixed_memory=True\n",
    "    )\n",
    "\n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train liquid network\n",
    "    history = liquid_network.fit(\n",
    "        train_rbm_seq,\n",
    "        train_targets,\n",
    "        validation_data=(val_rbm_seq, val_targets),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train')\n",
    "    plt.plot(history.history['val_mae'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot train liquid neural network: RBM features not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Test Data and Analyze Motor Neuron Output\n",
    "\n",
    "Now let's process the test data through the complete pipeline and analyze the motor neuron output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data if liquid network is trained\n",
    "if 'liquid_network' in locals() and 'test_rbm_seq' in locals():\n",
    "    # Process test data\n",
    "    outputs = liquid_network.predict(test_rbm_seq)\n",
    "\n",
    "    # Extract motor neuron outputs and trigger signals\n",
    "    if isinstance(outputs, list):\n",
    "        motor_outputs = outputs[0]\n",
    "        trigger_signals = outputs[1][0]  # First element is trigger\n",
    "        threshold_values = outputs[1][1]  # Second element is threshold\n",
    "    else:\n",
    "        motor_outputs = outputs\n",
    "        trigger_signals = (motor_outputs > 0.5).astype(float)\n",
    "        threshold_values = np.full_like(trigger_signals, 0.5)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Motor neuron output range: {motor_outputs.min():.4f} to {motor_outputs.max():.4f}\")\n",
    "    print(f\"Trigger rate: {trigger_signals.mean():.4f}\")\n",
    "\n",
    "    # Plot motor neuron outputs and triggers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(motor_outputs[:100], label='Motor Neuron Output')\n",
    "    plt.plot(threshold_values[:100], 'r--', label='Threshold')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Output Value')\n",
    "    plt.title('Motor Neuron Output and Threshold')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(trigger_signals[:100], 'g', label='Trigger Signal')\n",
    "    plt.axhline(y=trigger_signals.mean(), color='r', linestyle='--', \n",
    "               label=f'Trigger Rate: {trigger_signals.mean():.2f}')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Trigger (0/1)')\n",
    "    plt.title('Exploration Trigger Signals')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot process test data: liquid network not trained or test_rbm_seq not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Triggered Samples\n",
    "\n",
    "Let's analyze the samples that triggered deeper exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze triggered samples if motor outputs and test RBM features exist\n",
    "if 'motor_outputs' in locals() and 'test_rbm_features' in locals() and 'trigger_signals' in locals():\n",
    "    # Get indices of triggered samples\n",
    "    triggered_indices = ops.where(trigger_signals == 1)[0]\n",
    "    non_triggered_indices = ops.where(trigger_signals == 0)[0]\n",
    "\n",
    "    print(f\"Number of triggered samples: {len(triggered_indices)}\")\n",
    "    print(f\"Number of non-triggered samples: {len(non_triggered_indices)}\")\n",
    "\n",
    "    # Compare RBM features for triggered vs. non-triggered samples\n",
    "    if len(triggered_indices) > 0 and len(non_triggered_indices) > 0:\n",
    "        # Calculate mean features\n",
    "        triggered_mean = stats.mean(test_rbm_features[triggered_indices], axis=0)\n",
    "        non_triggered_mean = stats.mean(test_rbm_features[non_triggered_indices], axis=0)\n",
    "        \n",
    "        # Calculate feature difference\n",
    "        feature_diff = triggered_mean - non_triggered_mean\n",
    "        \n",
    "        # Plot feature difference\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(range(len(feature_diff)), feature_diff)\n",
    "        plt.xlabel('RBM Feature')\n",
    "        plt.ylabel('Difference (Triggered - Non-triggered)')\n",
    "        plt.title('Feature Difference Between Triggered and Non-triggered Samples')\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot feature distributions for top 3 differentiating features\n",
    "        top_features = np.argsort(ops.abs(feature_diff))[-3:]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, feature_idx in enumerate(top_features):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.hist(test_rbm_features[triggered_indices, feature_idx], bins=20, alpha=0.5, label='Triggered')\n",
    "            plt.hist(test_rbm_features[non_triggered_indices, feature_idx], bins=20, alpha=0.5, label='Non-triggered')\n",
    "            plt.xlabel(f'Feature {feature_idx}')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title(f'Feature {feature_idx} Distribution')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Cannot analyze triggered samples: motor_outputs, test_rbm_features, or trigger_signals not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models\n",
    "\n",
    "Let's save the trained models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for models\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "# Save RBM if it exists\n",
    "if 'rbm' in locals():\n",
    "    rbm.save('./models/rbm.npy')\n",
    "    print(\"RBM saved to ./models/rbm.npy\")\n",
    "else:\n",
    "    print(\"Cannot save RBM: not trained\")\n",
    "\n",
    "# Save liquid network if it exists\n",
    "if 'liquid_network' in locals():\n",
    "    liquid_network.save('./models/liquid_network')\n",
    "    print(\"Liquid network saved to ./models/liquid_network\")\n",
    "else:\n",
    "    print(\"Cannot save liquid network: not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Integrated Pipeline\n",
    "\n",
    "Now let's demonstrate how to use the integrated pipeline for a more streamlined workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create integrated pipeline\n",
    "pipeline = IntegratedPipeline(\n",
    "    project_id=PROJECT_ID,\n",
    "    rbm_hidden_units=64,\n",
    "    cfc_units=128,\n",
    "    lstm_units=32,\n",
    "    stride_perspectives=[1, 3, 5],\n",
    "    sparsity_level=0.5,\n",
    "    threshold=0.5,\n",
    "    use_gpu=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize feature extractor\n",
    "pipeline.initialize_feature_extractor(CREDENTIALS_PATH)\n",
    "\n",
    "# Extract features\n",
    "train_features_pipeline, val_features_pipeline, test_features_pipeline = pipeline.extract_features(\n",
    "    table_id=TABLE_ID,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "# Check if features were extracted successfully\n",
    "if train_features_pipeline is not None:\n",
    "    # Apply temporal processing\n",
    "    train_temporal = pipeline.apply_temporal_processing(train_features_pipeline)\n",
    "\n",
    "    # Train RBM\n",
    "    pipeline.train_rbm(train_features_pipeline, epochs=10)\n",
    "\n",
    "    # Extract RBM features\n",
    "    train_rbm_features_pipeline = pipeline.extract_rbm_features(train_features_pipeline)\n",
    "    val_rbm_features_pipeline = pipeline.extract_rbm_features(val_features_pipeline)\n",
    "    test_rbm_features_pipeline = pipeline.extract_rbm_features(test_features_pipeline)\n",
    "\n",
    "    # Create dummy targets for demonstration\n",
    "    train_targets_pipeline = np.random.rand(len(train_rbm_features_pipeline), 1)\n",
    "    val_targets_pipeline = np.random.rand(len(val_rbm_features_pipeline), 1)\n",
    "\n",
    "    # Train liquid network\n",
    "    pipeline.train_liquid_network(\n",
    "        features=train_rbm_features_pipeline,\n",
    "        targets=train_targets_pipeline,\n",
    "        validation_data=(val_rbm_features_pipeline, val_targets_pipeline),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        network_type='lstm_gated'\n",
    "    )\n",
    "\n",
    "    # Process test data\n",
    "    motor_outputs_pipeline, trigger_signals_pipeline = pipeline.process_data(test_rbm_features_pipeline)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Processed {len(test_rbm_features_pipeline)} test samples\")\n",
    "    print(f\"Motor neuron output range: {motor_outputs_pipeline.min():.4f} to {motor_outputs_pipeline.max():.4f}\")\n",
    "    print(f\"Trigger rate: {trigger_signals_pipeline.mean():.4f}\")\n",
    "\n",
    "    # Save models\n",
    "    pipeline.save_model('./models')\n",
    "\n",
    "    # Print pipeline summary\n",
    "    print(pipeline.summary())\n",
    "else:\n",
    "    print(\"Pipeline feature extraction failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Extract and prepare data from BigQuery tables using our terabyte-scale feature extractor\n",
    "2. Apply temporal stride processing to capture patterns at different time scales\n",
    "3. Train a Restricted Boltzmann Machine to learn latent representations\n",
    "4. Feed the RBM output into a CfC-based liquid neural network with LSTM neurons for gating\n",
    "5. Implement a motor neuron that outputs a value to trigger deeper exploration\n",
    "6. Analyze the results to understand which samples trigger deeper exploration\n",
    "\n",
    "This pipeline can be used for processing terabyte-sized tables efficiently through chunked processing, making it suitable for large-scale data analysis and exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
