"""
Closed-form Continuous-time (CfC) Neural Network

This module provides an implementation of CfC layers,
which are a type of recurrent neural network that operates in continuous time.
This implementation directly uses NeuronMap for both structure and dynamics.
"""

# (Removed unused typing imports)

from ember_ml import ops
from ember_ml.nn import tensor
from ember_ml.nn.modules.wiring import NeuronMap, NCPMap
from ember_ml.nn.modules import Module, Parameter
from ember_ml.nn.initializers import glorot_uniform, orthogonal
from ember_ml.nn.modules.activations import get_activation

class CfC(Module):
    """
    Closed-form Continuous-time (CfC) RNN layer.
    
    This layer directly uses NeuronMap for both structure and dynamics,
    without relying on separate cell objects.
    # Migration Notes from CfCCell and ModuleWiredCell:
    # - time_scale_factor: float parameter for time constant (CfCCell)
    # - mixed_memory: bool to enable mixed memory (CfCCell)
    # - recurrent_activation: string name for recurrent activation (CfCCell)
    # - kernel_initializer, recurrent_initializer, bias_initializer: initializers for weights and bias (CfCCell)
    # - time_scale: learnable Parameter for time constants (CfCCell)
    # - input_mask, recurrent_mask, output_mask: masks generated by NeuronMap in ModuleWiredCell
    # - synapse_count and sensory_synapse_count: statistics via stats.sum (ModuleWiredCell)
    # - consider integrating regularization hooks (get_regularization_loss) as in ModuleWiredCell or seCfC
    # TODO: Review and migrate these patterns into CfC implementation for spatial-seRNN support

    """
    
    def __init__(
        self,
        neuron_map: NCPMap,
        return_sequences: bool = False,
        return_state: bool = False,
        go_backwards: bool = False,
        mode: str = "default",
        time_scale_factor: float = 1.0,
        activation: str = "tanh",
        recurrent_activation: str = "sigmoid",
        use_bias: bool = True,
        kernel_initializer: str = "glorot_uniform",
        recurrent_initializer: str = "orthogonal",
        bias_initializer: str = "zeros",
        mixed_memory: bool = False,
        **kwargs
    ):
        """
        Initialize the CfC layer.
        
        Args:
            neuron_map: NCPMap instance defining both structure and dynamics
            return_sequences: Whether to return the full sequence or just the last output
            return_state: Whether to return the final state
            go_backwards: Whether to process the sequence backwards
            **kwargs: Additional keyword arguments
        """
        super().__init__(**kwargs)
        
        # Validate neuron_map type
        if not isinstance(neuron_map, NeuronMap):
            raise TypeError("neuron_map must be a NeuronMap instance")
        
        # Store the neuron map
        self.neuron_map = neuron_map
        
        # Store layer-specific parameters
        self.return_sequences = return_sequences
        self.return_state = return_state
        self.go_backwards = go_backwards

        # CfC layer parameters from CfCCell
        self.mode = mode
        self.time_scale_factor = time_scale_factor
        self.activation = activation
        self.recurrent_activation = recurrent_activation
        self.use_bias = use_bias
        self.kernel_initializer = kernel_initializer
        self.recurrent_initializer = recurrent_initializer
        self.bias_initializer = bias_initializer
        self.mixed_memory = mixed_memory
        
        # Initialize parameters
        self.kernel = None
        self.recurrent_kernel = None
        self.bias = None
        self.built = False
    
    def build(self, input_shape):
        """Build the CfC layer."""
        # Skip if already built
        if self.built:
            return
            
        # Get input dimension
        input_dim = input_shape[-1]
        
        # Build the neuron map if not already built
        if not self.neuron_map.is_built():
            # Store the masks returned by neuron_map.build()
            self.input_mask, self.recurrent_mask, self.output_mask = self.neuron_map.build(input_dim)
        else:
            # If already built, retrieve the masks
            # This assumes neuron_map retains its masks after building
            # If not, we might need to rebuild or find another way to access it
            # For now, assume it retains the masks
            self.input_mask = self.neuron_map.get_input_mask()
            self.recurrent_mask = self.neuron_map.get_recurrent_mask()
            self.output_mask = self.neuron_map.get_output_mask()
        
        # Get dimensions from neuron map
        units = self.neuron_map.units
        
        # Initialize parameters
        kernel_shape = (input_dim, ops.multiply(units, 4))
        recurrent_shape = (units, ops.multiply(units, 4))
        self.kernel = Parameter(tensor.zeros(kernel_shape))
        self.recurrent_kernel = Parameter(tensor.zeros(recurrent_shape))
        
        # Initialize bias if needed
        if self.use_bias:
            bias_shape = (ops.multiply(units, 4),)
            self.bias = Parameter(tensor.zeros(bias_shape))
        
        # Initialize weights
        if self.kernel_initializer == "glorot_uniform":
            self.kernel.data = glorot_uniform(kernel_shape)

        if self.recurrent_initializer == "orthogonal":
            self.recurrent_kernel.data = orthogonal(recurrent_shape)

        # Initialize learnable time_scale parameter
        ones = tensor.ones((units,))  # shape: (units,)
        # Handle different backends that may not have device attribute
        try:
            scale_val = tensor.convert_to_tensor(self.time_scale_factor, dtype=ones.dtype, device=ones.device)
        except (AttributeError, TypeError):
            # For backends without device attribute (numpy, mlx)
            scale_val = tensor.convert_to_tensor(self.time_scale_factor, dtype=ones.dtype)
        
        self.time_scale = Parameter(ops.multiply(ones, scale_val))
        
        # Mark as built
        self.built = True
    
    def forward(self, inputs, initial_state=None, time_deltas=None):
        """
        Forward pass through the layer.
        
        Args:
            inputs: Input tensor (batch, time, features)
            initial_state: Initial state(s) for the cell
            time_deltas: Time deltas between inputs (optional)
            
        Returns:
            Layer output(s)
        """
        # Build if not already built
        if not self.built:
            self.build(tensor.shape(inputs))
        
        # Get input shape
        input_shape = tensor.shape(inputs)
        if len(input_shape) != 3:
             raise ValueError("Input tensor must be 3D (batch, time, features)")
        batch_size, time_steps, _ = input_shape
        
        # Create initial state if not provided
        if initial_state is None:
            h0 = tensor.zeros((batch_size, self.neuron_map.units))
            t0 = tensor.zeros((batch_size, self.neuron_map.units))
            initial_state = [h0, t0]
        
        # Process sequence
        outputs = []
        states = initial_state
        
        # Get CfC-specific layer parameters
        time_scale_factor = self.time_scale_factor
        activation_fn = get_activation(self.activation)
        rec_activation_fn = get_activation(self.recurrent_activation)
        
        # Process sequence in reverse if go_backwards is True
        if self.go_backwards:
            time_indices = list(range(time_steps))
            time_indices.reverse()
        else:
            time_indices = list(range(time_steps))
        
        # Process each time step
        for t in time_indices:
                # Get current input
                x_t = inputs[:, t]
                
                # Get time delta for this step if provided
                ts = 1.0
                if time_deltas is not None:
                    ts = time_deltas[:, t]
                
                # Project input
                # In the original ncps implementation, the mask is applied to the weights
                # during the linear transformation, not to the inputs
                # Create a sparsity mask for the kernel weights based on the input_mask
                # The input_mask is a 1D tensor of shape (input_dim,) with all ones
                # We need to expand it to match the kernel shape (input_dim, units*4)
                input_dim = tensor.shape(self.input_mask)[0]
                units = self.neuron_map.units
                
                # Reshape input_mask for broadcasting with kernel
                # From (input_dim,) to (input_dim, 1)
                reshaped_input_mask = tensor.reshape(self.input_mask, (input_dim, 1))
                
                # Apply the mask to the kernel
                # This will broadcast the mask across all gates for each input feature
                masked_kernel = ops.multiply(self.kernel.data, reshaped_input_mask)
                z = ops.matmul(x_t, masked_kernel)
                
                # Apply recurrent mask to the recurrent kernel
                # The recurrent_mask is a 2D tensor of shape (units, units)
                # We need to expand it to match the recurrent_kernel shape (units, units*4)
                
                # Create a mask for the recurrent kernel
                # For each source neuron (row in recurrent_mask),
                # apply its connectivity pattern to all 4 gates of each target neuron
                recurrent_units = tensor.shape(self.recurrent_mask)[0]
                
                # Reshape for broadcasting: (units, units, 1)
                reshaped_recurrent_mask = tensor.reshape(self.recurrent_mask, (recurrent_units, recurrent_units, 1))
                
                # Reshape recurrent kernel for applying mask: (units, units, 4)
                reshaped_recurrent_kernel = tensor.reshape(self.recurrent_kernel.data, (recurrent_units, recurrent_units, 4))
                
                # Apply mask and reshape back
                masked_recurrent_kernel_3d = ops.multiply(reshaped_recurrent_kernel, reshaped_recurrent_mask)
                masked_recurrent_kernel = tensor.reshape(masked_recurrent_kernel_3d, (recurrent_units, recurrent_units * 4))
                
                # Use the masked recurrent kernel
                z = ops.add(z, ops.matmul(states[0], masked_recurrent_kernel))
                
                if self.use_bias:
                    z = ops.add(z, self.bias)
                
                # Split into gates
                z_chunks = tensor.split_tensor(z, 4, axis=-1)
                z_i, z_f, z_o, z_c = z_chunks
                
                # Apply activations
                i = rec_activation_fn(z_i)  # Input gate
                f = rec_activation_fn(z_f)  # Forget gate
                o = rec_activation_fn(z_o)  # Output gate
                c = activation_fn(z_c)      # Cell input
                
                # Apply time scaling
                decay = ops.exp(ops.divide(-ts, time_scale_factor))
                
                # Update state
                t_next = ops.add(ops.multiply(f, states[1]), ops.multiply(i, c))
                h_next = ops.multiply(o, activation_fn(ops.add(
                    ops.multiply(decay, states[0]),
                    ops.multiply(ops.subtract(tensor.ones_like(decay), decay), t_next)
                )))
                
                # Store output and update state
                outputs.append(h_next)
                states = [h_next, t_next]
        
        # If processing backwards, reverse the outputs sequence
        if self.go_backwards:
            outputs.reverse()
        
        # Stack outputs
        if self.return_sequences:
            outputs_tensor = tensor.stack(outputs, axis=1)
        else:
            outputs_tensor = outputs[-1]
            
        # If the neuron map is an NCPMap, extract only the motor neurons for the output
        if isinstance(self.neuron_map, NCPMap):
            # Apply the stored output mask to the outputs
            if self.return_sequences:
                # For sequence output, apply the mask to each time step
                # Reshape to apply mask across the units dimension for all batch and time steps
                original_shape = tensor.shape(outputs_tensor)
                reshaped_outputs = tensor.reshape(outputs_tensor, (-1, self.neuron_map.units))
                
                # Apply mask
                masked_output = ops.multiply(reshaped_outputs, self.output_mask)
                
                # Extract only the motor neurons by slicing the last dimension
                motor_neurons_int = tensor.item(self.neuron_map.motor_neurons) # Convert to int
                motor_output = masked_output[:, :motor_neurons_int]

                # Reshape back to include batch and time dimensions
                outputs_tensor = tensor.reshape(motor_output, (original_shape[0], original_shape[1], motor_neurons_int))
            else:
                # For single output, apply the mask directly
                masked_output = ops.multiply(outputs_tensor, self.output_mask)
                
                # Extract only the motor neurons by slicing the last dimension
                motor_neurons_int = tensor.item(self.neuron_map.motor_neurons) # Convert to int
                outputs_tensor = masked_output[:, :motor_neurons_int]

        # Return outputs and states if requested
        if self.return_state:
            return outputs_tensor, states
        else:
            return outputs_tensor
    
    def reset_state(self, batch_size=1):
        """
        Reset the layer state.
        
        Args:
            batch_size: Batch size
            
        Returns:
            Initial state
        """
        h0 = tensor.zeros((batch_size, self.neuron_map.units))
        t0 = tensor.zeros((batch_size, self.neuron_map.units))
        return [h0, t0]
    
    def get_config(self):
        """Returns the configuration of the CfC layer."""
        config = super().get_config()
        config.update({
            "neuron_map": self.neuron_map.get_config(),
            "return_sequences": self.return_sequences,
            "return_state": self.return_state,
            "go_backwards": self.go_backwards,
            "mode": self.mode,
            "time_scale_factor": self.time_scale_factor,
            "activation": self.activation,
            "recurrent_activation": self.recurrent_activation,
            "use_bias": self.use_bias,
            "kernel_initializer": self.kernel_initializer,
            "recurrent_initializer": self.recurrent_initializer,
            "bias_initializer": self.bias_initializer,
            "mixed_memory": self.mixed_memory
        })
        return config
    
    @classmethod
    def from_config(cls, config):
        """Creates a CfC layer from its configuration."""
        # Extract neuron_map config
        neuron_map_config = config.pop("neuron_map", {})
        
        # Create neuron_map
        from ember_ml.nn.modules.wiring import NCPMap
        neuron_map = NCPMap.from_config(neuron_map_config)
        
        # Create layer
        return cls(neuron_map=neuron_map, **config)
