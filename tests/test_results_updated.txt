============================= test session starts ==============================
platform darwin -- Python 3.12.2, pytest-8.3.4, pluggy-1.5.0 -- /Users/sydneybach/miniconda3/bin/python
cachedir: .pytest_cache
rootdir: /Volumes/stuff/Projects/LNNDemo
plugins: cov-6.0.0, langsmith-0.3.4, anyio-4.8.0
collecting ... collected 21 items

test_backend.py::TestBackendSelection::test_default_backend PASSED       [  4%]
test_backend.py::TestBackendSelection::test_backend_switching[numpy] PASSED [  9%]
test_backend.py::TestBackendSelection::test_backend_switching[torch] PASSED [ 14%]
test_backend.py::TestBackendSelection::test_backend_switching[mlx] PASSED [ 19%]
test_backend.py::TestBackendSelection::test_invalid_backend PASSED       [ 23%]
test_backend.py::TestBackendPersistence::test_backend_persistence[numpy] FAILED [ 28%]
test_backend.py::TestBackendPersistence::test_backend_persistence[torch] PASSED [ 33%]
test_backend.py::TestBackendPersistence::test_backend_persistence[mlx] FAILED [ 38%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[numpy-numpy] SKIPPED [ 42%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[numpy-torch] PASSED [ 47%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[numpy-mlx] PASSED [ 52%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[torch-numpy] PASSED [ 57%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[torch-torch] SKIPPED [ 61%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[torch-mlx] FAILED [ 66%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[mlx-numpy] PASSED [ 71%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[mlx-torch] PASSED [ 76%]
test_backend.py::TestBackendCompatibility::test_tensor_conversion[mlx-mlx] SKIPPED [ 80%]
test_backend.py::TestBackendCompatibility::test_operation_consistency[numpy] PASSED [ 85%]
test_backend.py::TestBackendCompatibility::test_operation_consistency[torch] PASSED [ 90%]
test_backend.py::TestBackendCompatibility::test_operation_consistency[mlx] PASSED [ 95%]
test_backend.py::TestBackendAutoDetection::test_backend_auto_detection PASSED [100%]

=================================== FAILURES ===================================
____________ TestBackendPersistence.test_backend_persistence[numpy] ____________

self = <test_backend.TestBackendPersistence object at 0x34c26f650>
backend_name = 'numpy'

    @pytest.mark.parametrize('backend_name', BACKENDS)
    def test_backend_persistence(self, backend_name):
        """Test that backend setting persists across module reloads."""
        # Get the current backend
        current_backend = get_backend()
    
        try:
            # Switch to the specified backend
            set_backend(backend_name)
            ops.set_ops(backend_name)
    
            # Verify that the backend is set correctly
            assert get_backend() == backend_name
            assert ops.get_ops() == backend_name
    
            # Reload the backend module
            import importlib
            importlib.reload(importlib.import_module('ember_ml.backend'))
    
            # Verify that the backend is still set correctly
>           assert get_backend() == backend_name
E           AssertionError: assert 'torch' == 'numpy'
E             
E             - numpy
E             + torch

test_backend.py:107: AssertionError
_____________ TestBackendPersistence.test_backend_persistence[mlx] _____________

self = <test_backend.TestBackendPersistence object at 0x34c26fcb0>
backend_name = 'mlx'

    @pytest.mark.parametrize('backend_name', BACKENDS)
    def test_backend_persistence(self, backend_name):
        """Test that backend setting persists across module reloads."""
        # Get the current backend
        current_backend = get_backend()
    
        try:
            # Switch to the specified backend
            set_backend(backend_name)
            ops.set_ops(backend_name)
    
            # Verify that the backend is set correctly
            assert get_backend() == backend_name
            assert ops.get_ops() == backend_name
    
            # Reload the backend module
            import importlib
            importlib.reload(importlib.import_module('ember_ml.backend'))
    
            # Verify that the backend is still set correctly
>           assert get_backend() == backend_name
E           AssertionError: assert 'torch' == 'mlx'
E             
E             - mlx
E             + torch

test_backend.py:107: AssertionError
__________ TestBackendCompatibility.test_tensor_conversion[torch-mlx] __________

self = <test_backend.TestBackendCompatibility object at 0x34c2984a0>
backend1 = 'mlx', backend2 = 'torch'

    @pytest.mark.parametrize('backend1', BACKENDS)
    @pytest.mark.parametrize('backend2', BACKENDS)
    def test_tensor_conversion(self, backend1, backend2):
        """Test tensor conversion between backends."""
        # Skip if the backends are the same
        if backend1 == backend2:
            pytest.skip(f"Skipping conversion from {backend1} to {backend2}")
    
        # Get the current backend
        current_backend = get_backend()
    
        try:
            # Switch to the first backend
            set_backend(backend1)
            ops.set_ops(backend1)
    
            # Create a tensor
            x1 = ops.ones((3, 4))
    
            # Switch to the second backend
            set_backend(backend2)
            ops.set_ops(backend2)
    
            # Convert the tensor to the second backend
>           x2 = tensor.convert_to_tensor(ops.to_numpy(x1))

test_backend.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../ember_ml/ops/__init__.py:192: in <lambda>
    to_numpy = lambda x: tensor_ops().to_numpy(x)
../ember_ml/ops/torch/tensor_ops.py:391: in to_numpy
    x_tensor = self.convert_to_tensor(x)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ember_ml.ops.torch.tensor_ops.TorchTensorOps object at 0x34c2b6060>
x = array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=float32)
dtype = None, device = None

    def convert_to_tensor(self, x: Any, dtype: Optional[DType] = None, device: Optional[str] = None) -> torch.Tensor:
        """
        Convert input to a PyTorch tensor.
    
        Args:
            x: Input data (array, tensor, scalar)
            dtype: Optional data type
            device: Optional device
    
        Returns:
            PyTorch tensor representation of the input
        """
        if isinstance(x, torch.Tensor):
            if dtype is not None:
                x = x.to(dtype=dtype)
            if device is not None:
                x = x.to(device=device)
            return x
        if isinstance(x, np.ndarray):
            tensor = torch.from_numpy(x)
            if dtype is not None:
                tensor = tensor.to(dtype=dtype)
            if device is not None:
                tensor = tensor.to(device=device)
            return tensor
>       return torch.tensor(x, dtype=dtype, device=device)
E       TypeError: len() 0-dimensional array.

../ember_ml/ops/torch/tensor_ops.py:326: TypeError
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: Type google._upb._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED test_backend.py::TestBackendPersistence::test_backend_persistence[numpy]
FAILED test_backend.py::TestBackendPersistence::test_backend_persistence[mlx]
FAILED test_backend.py::TestBackendCompatibility::test_tensor_conversion[torch-mlx]
============= 3 failed, 15 passed, 3 skipped, 2 warnings in 5.12s ==============
